{%- set env =  grains['yandex-environment'] -%}
{%- set prod =  ['prestable', 'production'] -%}
{%- if env in prod -%}
{%- set zk_hosts = ['zk01man.mds.yandex.net:2181', 'zk01myt.mds.yandex.net:2181', 'zk01sas.mds.yandex.net:2181'] -%}
{%- set discovery_prefix = 'karl_discovery' -%}
{%- set elliptics_remotes = ['[2a02:6b8:c03:72b:0:41af:4715:f6f]:1025', '[2a02:6b8:c0e:11c:0:41af:1786:344d]:1025', '[2a02:6b8:c01:718:0:41af:6278:bd92]:1025', '[2a02:6b8:c04:29b:0:41af:f276:3c46]:1025', '[2a02:6b8:c03:703:0:41af:81ab:3e26]:1025', '[2a02:6b8:c02:5de:0:41af:ea99:4c7b]:1025', '[2a02:6b8:c02:5c2:0:41af:b86b:d000]:1025'] -%}
{%- set mdb_federation = 'mds-1' -%}
{%- set mdb_hosts = ['man-wqe22p195w0fonw9.db.yandex.net', 'sas-oxo8ojezy5djvhwo.db.yandex.net', 'vla-csesxmr83pri0pka.db.yandex.net'] -%}
{%- set mdb_db = 'karl' -%}
{%- set mdb_pass = pillar['tls_karl']['karl_db_password'] -%}
{%- set mdb_sslrootcert = 'allCAs.pem' -%}
{%- set nscfg_host = 'nscfg.mds.yandex.net' -%}
{%- set is_proxy = 'elliptics-proxy' in grains['conductor']['groups'] -%}
{%- set is_control = 'elliptics-cloud' in grains['conductor']['groups'] -%}
{%- set tls = True -%}
{%- set meta_grpc_host = "karl-control.mds.yandex.net:17896" -%}
{%- set mm_hosts = salt['conductor']['groups2hosts']('elliptics-cloud') -%}
{%- else -%}
{%- set zk_hosts = ['zk01man.mdst.yandex.net:2181', 'zk01myt.mdst.yandex.net:2181', 'zk01sas.mdst.yandex.net:2181'] -%}
{%- set discovery_prefix = 'karl_test_discovery' -%}
{%- set elliptics_remotes = ['[2a02:6b8:c02:5d1:0:4345:776b:93ed]:1025', '[2a02:6b8:c0e:11c:0:4345:3497:e76b]:1025', '[2a02:6b8:c01:26:0:4345:8ec6:46cb]:1025'] -%}
{%- set mdb_federation = 'mds-test-1' -%}
{%- set mdb_hosts = ['man-i0u1lnj8cj20h5ej.db.yandex.net', 'sas-93ixtxukmhftpk2p.db.yandex.net', 'vla-kqexpbl7q4eyp9zo.db.yandex.net'] -%}
{%- set mdb_db = 'karl-sharding-testing' -%}
{%- set mdb_pass = pillar['tls_karl']['karl_db_password_testing'] -%}
{%- set mdb_sslrootcert = 'allCAs.pem' -%}
{%- set nscfg_host = 'nscfg.mdst.yandex.net' -%}
{%- set is_proxy = 'elliptics-test-proxies' in grains['conductor']['groups'] -%}
{%- set is_control = 'elliptics-test-cloud' in grains['conductor']['groups'] -%}
{%- set tls = True -%}
{%- set mm_hosts = salt['conductor']['groups2hosts']('elliptics-test-cloud') -%}
{%- set meta_grpc_host = "karl-control.mdst.yandex.net:17896" -%}
{%- endif -%}

{%- set notification_hosts = [] -%}
{%- for host in mm_hosts -%}
    {%- do notification_hosts.append(host + ':9010') -%}
{%- endfor -%}

{%- set is_disk = grains['conductor']['project'] == 'disk' -%}
{%- if is_disk -%}
{%- set discovery_prefix = discovery_prefix + 'disk' -%}
{%- endif -%}

{%- if is_proxy or is_disk -%}
{%- set mdb_update_timeout = '10s' -%}
{%- set mdb_max_idle_conns = 20 -%}
{%- else -%}
{%- set mdb_update_timeout = '5m' -%}
{%- set mdb_max_idle_conns = 2 -%}
{%- endif -%}
# https://a.yandex-team.ru/arc/trunk/arcadia/mds/karl/cmd/karl/server.yaml
# app server config. App curently only responses on Ping messages, checking status of all the modules.
app:
  grpc_endpoint: ":17895"
  secure_grpc: true
# modules to enable in karl.
# all of them have the same fields:
# * grpc_endpoint to bind onto. For discovery instead empty hostname gethostname() will be called
# * secure_grpc. whether to enable tls.
# * http_endpoint. Endpoint to serve http if the module have such ability.
# * secure_http. Whether to use https or not. Doesn't work right now.
# * http_prefix. Routing prefix. On this prefix the module will serve it's handlers.
# NB: if some modules have same grpc_endpoint or http_endpoint they will be serving on the same endpoint, but
# security option (secure_grpc or secure_http) should be equal.
modules:
  {% if is_proxy or is_disk -%}
  # module, which serves storage operations (write, remove, read).
  karl:
    grpc_endpoint: ":17895"
    secure_grpc: true
  # module responsible for putting daemon in discovery and serving requests in discovery service
  discovery:
    grpc_endpoint: ":17895"
    secure_grpc: true
  {%- elif is_control -%}
   # module, which serves storage operations (write, remove, read).
  karl:
    grpc_endpoint: ":17895"
    secure_grpc: true
  # module, which provides DB controlling operations, such as assigning couple to shard, etc.
  control:
    grpc_endpoint: ":17896"
    secure_grpc: true
  # module for http pprof endpoint
  {%- else -%}
  # job module, which is responsible to run periodic jobs
  job:
    grpc_endpoint: ":17898"
    secure_grpc: true
  {%- endif %}
  pprof:
    http_endpoint: "[::1]:17899"
    secure_http: false
    http_prefix: "/debug/pprof/"
  # unistat module. Serves unistat handle on http
  unistat:
    http_endpoint: ":17897"
    secure_http: false

# server logging parameters
log:
  # common logging
  common:
    # minimum verbosity of the logger, one of "trace", "debug", "info", "warn", "error", "fatal"
    level: debug
    # encoding of log message, one of "tskv", "json", "console", "cli", "kv"
    encoding: tskv
    # an array of "sinks" (output facilities), several can be specified - in this case everything will be duplicated
    sinks:
      - logrotate+async:///var/log/karl/karl.log

  # access logger is configured the same way as common logger, server writes single log record
  # into access log for every handled remote call with extra fields like status, timings, etc
  access:
    level: info
    encoding: tskv
    sinks:
      - logrotate+async:///var/log/karl/access.log

# datacenter which Karl is deployed in
# Possible values: VLA, MYT, IVA, MAN, SAS, UNKNOWN
dc: "{{ grains['conductor']['root_datacenter'] | upper }}"

# Section defines TLS config for GRPC server
tls:
  # Enable or disable TLS
  enabled: {{ tls|lower }}
  # Path to certificate
  cert_path: "/etc/karl/ssl/karl.crt"
  # Path to private key
  key_path: "/etc/karl/ssl/karl.key"
  # Paths to CA certificates
  ca_paths: ["/etc/karl/ssl/ca.crt"]
  # Verify client's certificates (default=true)
  verify: true

# Karl discovery settings. Karl uses zookeeper to be discovered by Karl clients.
discovery:
  # Zookeeper session timeout - sets ping frequency (and ephemeral node lifetime)
  timeout: 3s
  # Zookeeper hosts to connect to
  hosts:
    {%- for host in zk_hosts %}
    - "{{ host }}"
    {%- endfor %}
  # Discovery prefix. Can be used for namespacing several different group of Karls
  prefix: "{{ discovery_prefix }}"

# Initial elliptics remotes to start connection to elliptics cloud.
# They act as seed to gather info about all storage nodes and establish connection to each one.
# TODO: Right now those addresses are being resolved and go returns ip4 addresses for hostnames.
elliptics:
  remotes:
    {%- for remote in elliptics_remotes %}
    - "{{ remote }}"
    {%- endfor %}
  die_limit: 4
  node:
    # Logger defines config for logger which will be used by elliptics node
    # Field "level" defines minimum verbosity of the logger. If map contains "path",
    # the value will be used to configure simple logging as path to log file,
    # otherwise "core" and "access" sections will be used as blackhole configs
    logger:
      # Path where logger should write its logs (default: logging disabled)
      # path: /var/log/karl/elliptics.log
      # Level defines level of logger verbosity, one of the "debug", "notice", "info", "warning", "error" (default=error)
      level: error
      access:
          [{
              "sinks": [{
                  "type": "asynchronous",
                  "sink": {
                      "type": "file",
                      "path": "/var/log/karl/elliptics.log",
                      "flush": 100
                  },
                  "factor": 16,
                  "overflow": "wait"
              }],
              "formatter": {
                  "mutate" : {
                     "timestamp" : {
                        "gmtime" : false,
                        "strftime" : "%Y-%m-%dT%H:%M:%S.%f"
                     }
                  },
                  "type": "tskv",
                  "remove": [
                      "severity",
                      "message"
                  ]
              }
      }]
      core:
          [{
              "formatter": {
                  "type": "string",
                  "sevmap": ["DEBUG", "INFO", "WARN", "ERROR"],
                  "pattern": "{timestamp:l} {trace_id:{0:default}0>16} {thread:d}/{process:d} {severity}: {message}, attrs: [{...}]"
              },
              "sinks": [
                  {
                      "type": "asynchronous",
                      "factor": 16,
                      "overflow": "wait",
                      "sink": {
                          "type": "file",
                          "path": "/var/log/karl/node-1.log",
                          "flush": 100
                      }
                  }
              ]
      }]
    # TLS defines config for TLS
    tls:
      # Support defines level of TLS support: "disabled", "allowed", "required" (default=disabled)
      support: allowed
      # CertificatePath defines path to certificate
      cert_path: "/etc/elliptics/ssl/storage.crt"
      # PrivateKeyPath defines path to private key
      key_path: "/etc/elliptics/ssl/storage.key"
      # CAPath defines path ot ca directory
      ca_path: "/etc/elliptics/ssl/"
      # VerifyCertificates defines whether certificates should be verified (default=true)
      verify: true

    # IOThreads defines number of IO threads in elliptics node (default=1)
    io_thread_num: 4
    # NonBlockingIOThreads defines number of nonblocking IO threads in elliptics node (default=1)
    nonblocking_io_thread_num: 6
    # NetThreads defines number of net threads in elliptics node (default=1)
    net_thread_num: 4
    # WaitTimeout defines default timeout in seconds for further operations (default=5)
    wait_timeout: 20
    # CheckTimeout defines timeout in seconds with with check thread should be waked up (default=60)
    check_timeout: 60
    # Flags defines elliptics config flags
    flags: 0
    # StallCount defines number of timed-out operations in a row after which
    # connection to problem node will be checked (default=3)
    stall_count: 3
    # EllipticsFeatures defines which experimental features should be enabled
    features:
      # Forcing to enable the feature_a
      independent_protocol: true

# This section controls settings of the sharding metabase, which stores info about federations, shards, couple mapping etc
mdb:
  # federation to assign Karl to.
  federation: "{{ mdb_federation }}"
  # Sharding DB hosts and ports

  meta_addresses:
    {%- for mdb_host in mdb_hosts %}
    - "{{ mdb_host }}:6432"
    {%- endfor %}

  # Timeout for updating the list of couples
  update_timeout: "{{ mdb_update_timeout }}"

  # Shard DB settings
  shard:
    # Credentials and database to connect
    user: "karl"
    password: "{{ mdb_pass }}"
    dbname: "karl"
    # SSL mode choices=[disable, allow, prefer, require, verify-ca, verify-full] (default="allow")
    ssl_mode: "verify-full"
    # The cert is used to connect to shards stored in shard DB (default="")
    ssl_root_cert: "/etc/karl/{{ mdb_sslrootcert }}"
    # Maximum number of idle connections to DB.
    # if number of idle connections is greater than this setting driver will lazily close them.
    # Greater amount can help to avoid vzdrizhne
    max_idle_conns: {{ mdb_max_idle_conns }}
    # Maximum number of open connections to DB.
    # Can help to avoid db connection overwhelming.
    # Default is 0, which means no limit
    max_open_conns: 0
    # Maximum lifetime of a connection.
    # 0 means that a connection is reused forever, which seems ok
    max_conn_lifetime: 0s
    # Timeout for waiting of master
    # If the timeout ends, the connection will be closed
    wait_master_timeout: 5s
    # log level for all db operations.
    # NB: it doesn't support text unmarshalling as it is driver provided log level
    # 6 - trace, 5 - debug, 4 - info, 3 - warn, 2 - error, 1 - none
    # default is 3 (warn)
    log_level: 3
    connection_gc_period: 60s
    # interval to check for master in PG cluster
    update_interval: 5s
    # timeout to check for master. If the node do not respond during this time it is considered dead.
    update_timeout: 1s
    # log all cluster events under the hood
    log_cluster_events: false

  # provider for cache of couple mapping either "mdb" or "grpc"
  # "mdb" uses direct access to postgres, using "meta" section parameters.
  # This is quite heavy for DB if we have many karl nodes
  # "grpc" accesses couple mapping via caching grpc server (control module)
  {% if is_control -%}
  meta_cache_provider: mdb
  {%- else -%}
  meta_cache_provider: grpc
  {%- endif %}
  # config section for "grpc" cache provider
  meta_grpc:
    upstreams:
      - host: "{{ meta_grpc_host }}"
        secure: true
    connect_timeout: 5s

  # Meta DB settings. Settings are the same as in shard section below
  meta:
    user: "karl"
    password: "{{ mdb_pass }}"
    dbname: "{{ mdb_db }}"
    ssl_mode: "verify-full"
    ssl_root_cert: "/etc/karl/{{ mdb_sslrootcert }}"
    max_idle_conns: {{ mdb_max_idle_conns }}
    max_open_conns: 0
    max_conn_lifetime: 0s
    log_level: 3

# This section holds parameters to control elliptics operations fine tuning such as timeouts,
# checksumming rate, etc.
operation:
  # Chunk write timeout
  write_timeout: 10s
  # Key remove timeout
  remove_timeout: 20s
  # Heuristic timeout addition on write retry. See libmds for detailes.
  scale_group_retry_timeout_mutiplier: 2.0
  # Checksum rate. Used to calculate heuristic write timeout addition for last chunk write in order to calculate data checksum
  checksum_rate: 10000000
  # Retry count for writing chunk in case error can be retried.
  write_retry_cnt: 5
  commit_queue:
    # Size of queue per couple
    size: 100
    # Size of batch for mdb updates
    batch_size: 20
    # If batch size is not reached records will be flushed after this delay
    delay: 10s
    # Stop checking the queue after this timeout of idle time
    clear_after: 1h
    # Retry with exponential backoff from min to max timeout. Next timeout = 2 * current timeout
    retry_timeout_min: 1000ms
    retry_timeout_max: 5s

nscfg:
  endpoint: "http://{{ nscfg_host }}:9532"
  cache_path: "/var/cache/karl/nscfg-cache.json"
  update_interval: 1m

job:
  artifacts_dir: "/var/log/karl/jobs"
  log:
    level: info
    encoding: tskv
    # array of sinks, use <artifacts_dir> and <karl_job_id> as tokens for replacing
    sinks:
      - logrotate+async://<artifacts_dir>/<karl_job_id>/job.log
  # uses elliptics BulkLookup instead of multiple Lookup commands if set to true
  bulk_lookup_enabled: true
  # issues lookup_concurrency lookups simultaneously if bulk lookup is disabled
  lookup_concurrency: 8
  # job status notification settings (primary used to notify mm about job status change)
  notification:
    # whether to use tls
    secure: false
    # tls config(if applicable)
    tls: {}
    # retry policy - max amount of time to try to notify
    max_overall_time: 48h
    # retry policy - min interval to retry notification
    min_retry_interval: 1s
    # retry policy - max interval to retry notification
    max_retry_interval: 1h
    # retry policy - timeout to connect to notification service.
    notify_connect_timeout: 4s
    # retry policy - timeout to perform notification request.
    notify_timeout: 4s
    # hosts of the notification service. If no hosts are provided - NullNotifier is used
    hosts: {{ notification_hosts }}

# Path to original pgmigrate tool
pgmigrate: "/usr/bin/karl-pgmigrate"
