{%- set log_group_id = salt['pillar.get']('data:logging:group_id') -%}
{%- set logging_url = salt['pillar.get']('data:logging:url', 'api.cloud.yandex.net:443') -%}
{%- set cluster_id = salt['pillar.get']('data:agent:cid', '') -%}
{% set hostname = salt['grains.get']('dataproc:fqdn') %}
{% set folder_id = salt['grains.get']('dataproc:folder_id') %}
{% set log_tags = {
        'masternode': {
            'hadoop-hdfs-namenode': '/var/log/hadoop-hdfs/hadoop-hdfs-namenode-*.log',
            'hadoop-hdfs-secondarynamenode': '/var/log/hadoop-hdfs/hadoop-hdfs-secondarynamenode-*.log',
            'hadoop-mapreduce': '/var/log/hadoop-mapreduce/*.log',
            'hadoop-yarn-resourcemanager': '/var/log/hadoop-yarn/hadoop-yarn-resourcemanager-*.log',
            'hadoop-yarn-timelineserver': '/var/log/hadoop-yarn/hadoop-yarn-timelineserver-*.log',
            'hbase-master': '/var/log/hbase/hbase-hbase-master-*.log',
            'hbase-rest': '/var/log/hbase/hbase-hbase-rest-*.log',
            'hbase-thrift': '/var/log/hbase/hbase-hbase-thrift-*.log',
            'hive-metastore': '/var/log/hive/metastore.log',
            'hiveserver2': '/var/log/hive/hiveserver2.log',
            'livy-request': '/var/log/livy/*.log',
            'livy-out': '/var/log/livy/livy-livy-server.out',
            'oozie-audit': '/var/log/oozie/oozie-audit.log',
            'oozie-error': '/var/log/oozie/oozie-error.log',
            'oozie-instrumentation': '/var/log/oozie/oozie-instrumentation.log',
            'oozie-jetty': '/var/log/oozie/jetty.log',
            'oozie-jpa': '/var/log/oozie/oozie-jpa.log',
            'oozie-ops': '/var/log/oozie/oozie-ops.log',
            'oozie': '/var/log/oozie/oozie.log',
            'postgres': '/var/log/postgresql/postgresql-13-main.log',
            'yandex-dataproc-agent': '/var/log/yandex/dataproc-agent.log',
            'zeppelin': '/var/log/zeppelin/zeppelin-zeppelin-*.log',
            'zookeeper': '/var/log/zookeeper/zookeeper.log',
        },
        'datanode': {
             'hadoop-hdfs-datanode': '/var/log/hadoop-hdfs/hadoop-hdfs-datanode-*.log',
        },
        'computenode': {
             'hadoop-yarn-nodemanager': '/var/log/hadoop-yarn/hadoop-yarn-nodemanager-*.log',
        },
        'all_nodes': {
            'syslog': '/var/log/syslog',
            'salt-minion': '/var/log/salt/minion',
            'yandex-dataproc-start': '/var/log/yandex/dataproc-start.log',
            'yandex-dataproc-init-actions': '/var/log/yandex/dataproc-init-actions.log',
            'cloud-init': '/var/log/cloud-init.log',
        }
    }
%}
{% set log_parsers = {
        'cloud-init': 'cloud-init',
        'hadoop-hdfs-namenode': 'hadoop',
        'hadoop-hdfs-secondarynamenode': 'hadoop',
        'hadoop-mapreduce': 'hadoop',
        'hadoop-yarn-resourcemanager': 'hadoop',
        'hadoop-yarn-timelineserver': 'hadoop',
        'hadoop-yarn-timelineserver': 'hadoop',
        'hbase-master': 'hbase',
        'hbase-rest': 'hbase',
        'hbase-thrift': 'hbase',
        'hive-metastore': 'hive',
        'hiveserver2': 'hive',
        'livy-out': 'hadoop',
        'livy-request': 'livy',
        'oozie-audit': 'oozie',
        'oozie-error': 'oozie',
        'oozie-instrumentation': 'oozie',
        'oozie-jetty': 'oozie',
        'oozie-jpa': 'oozie',
        'oozie-ops': 'oozie',
        'oozie': 'oozie',
        'postgres': 'postgres',
        'salt-minion': 'salt-minion',
        'supervisor': 'supervisor',
        'syslog': 'syslog',
        'telegraf': 'telegraf',
        'yandex-dataproc-agent': 'yandex-dataproc-agent',
        'yandex-dataproc-start': 'syslog',
        'yandex-dataproc-init-actions': 'yandex-dataproc-init-actions',
        'zeppelin': 'zeppelin',
        'zookeeper': 'zookeeper',
    }
%}

{% if log_group_id %}@SET LOG_GROUP_ID={{ log_group_id }}{% endif %}
@SET ENDPOINT_URL={{ logging_url }}
@SET CLUSTER_ID={{ cluster_id }}
@SET FOLDER_ID={{ folder_id }}
@SET RESOURCE_TYPE=dataproc.cluster
@SET AUTH_TYPE=instance-service-account
@SET MESSAGE_KEY=log
@SET MESSAGE_TAG_KEY=log_type

[SERVICE]
    # Flush
    # =====
    # set an interval of seconds before to flush records to a destination
    flush        5

    # Daemon
    # ======
    # instruct Fluent Bit to run in foreground or background mode.
    daemon       Off

    # Log_Level
    # =========
    # Set the verbosity level of the service, values can be:
    #
    # - error
    # - warning
    # - info
    # - debug
    # - trace
    #
    # by default 'info' is set, that means it includes 'error' and 'warning'.
    log_level    info

    # Parsers File
    # ============
    # specify an optional 'Parsers' configuration file
    parsers_file parsers.conf

    # Plugins File
    # ============
    # specify an optional 'Plugins' configuration file to load external plugins.
    plugins_file plugins.conf

    # HTTP Server
    # ===========
    # Enable/Disable the built-in HTTP Server for metrics
    http_server  Off
    http_listen  0.0.0.0
    http_port    2020

    # Storage
    # =======
    # Fluent Bit can use memory and filesystem buffering based mechanisms
    #
    # - https://docs.fluentbit.io/manual/administration/buffering-and-storage
    #
    # storage metrics
    # ---------------
    # publish storage pipeline metrics in '/api/v1/storage'. The metrics are
    # exported only if the 'http_server' option is enabled.
    #
    storage.metrics on

    # storage.path
    # ------------
    # absolute file system path to store filesystem data buffers (chunks).
    #
    # storage.path /tmp/storage

    # storage.sync
    # ------------
    # configure the synchronization mode used to store the data into the
    # filesystem. It can take the values normal or full.
    #
    # storage.sync normal

    # storage.checksum
    # ----------------
    # enable the data integrity check when writing and reading data from the
    # filesystem. The storage layer uses the CRC32 algorithm.
    #
    # storage.checksum off

    # storage.backlog.mem_limit
    # -------------------------
    # if storage.path is set, Fluent Bit will look for data chunks that were
    # not delivered and are still in the storage layer, these are called
    # backlog data. This option configure a hint of maximum value of memory
    # to use when processing these records.
    #
    # storage.backlog.mem_limit 5M

[FILTER]
    Name modify
    Match *
    Add hostname {{ hostname }}

{% for tag, path in log_tags['all_nodes'].items() %}
{% set parser = log_parsers.get(tag) %}
[INPUT]
    {% if parser %}
    parser {{ parser }}
    {% endif %}
    name tail
    tag {{ tag }}
    path {{ path }}

[OUTPUT]
    Name yc-logging
    Match {{ tag }}
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endfor %}

{% if salt['ydputils.is_masternode']() %}

{% for tag, path in log_tags['masternode'].items() %}
{% set parser = log_parsers.get(tag) %}
[INPUT]
    {% if parser %}
    parser {{ parser }}
    {% endif %}
    name tail
    tag {{ tag }}
    path {{ path }}

[OUTPUT]
    Name yc-logging
    Match {{ tag }}
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endfor %}

[INPUT]
    Name tail
    Refresh_Interval 5
    Tag job_output
    Path /var/log/yandex/dataproc-agent/jobs/job_*.log
    Path_Key file_path

[FILTER]
    Name parser
    Key_name file_path
    Match job_output
    Parser job-output-filepath
    Parser job-output-filepath-noapp
    Reserve_Data On

[FILTER]
    Name parser
    Key_name log
    Match job_output
    Parser hadoop
    Reserve_Data On

[OUTPUT]
    Name yc-logging
    Match job_output
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endif %}

{% if salt['ydputils.is_datanode']() %}

{% for tag, path in log_tags['datanode'].items() %}
{% set parser = log_parsers.get(tag) %}
[INPUT]
    {% if parser %}
    parser {{ parser }}
    {% endif %}
    name tail
    tag {{ tag }}
    path {{ path }}

[OUTPUT]
    Name yc-logging
    Match {{ tag }}
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endfor %}

{% endif %}

{% if salt['ydputils.is_computenode']() or salt['ydputils.is_datanode']() %}

{% for tag, path in log_tags['computenode'].items() %}
{% set parser = log_parsers.get(tag) %}
[INPUT]
    {% if parser %}
    parser {{ parser }}
    {% endif %}
    name tail
    tag {{ tag }}
    path {{ path }}

[OUTPUT]
    Name yc-logging
    Match {{ tag }}
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endfor %}


[INPUT]
    Name tail
    Refresh_Interval 5
    Tag containers
    Path /var/log/hadoop-yarn/containers/*/*/*
    Path_Key file_path

[FILTER]
    Name parser
    Key_name file_path
    Match containers
    Parser yarn-container-filepath
    Reserve_Data On

[FILTER]
    Name parser
    Key_name log
    Match containers
    Parser hadoop
    Reserve_Data On

[OUTPUT]
    Name yc-logging
    Match containers
    {% if log_group_id %}group_id ${LOG_GROUP_ID}{% endif %}
    folder_id ${FOLDER_ID}
    resource_type ${RESOURCE_TYPE}
    resource_id ${CLUSTER_ID}
    message_tag_key ${MESSAGE_TAG_KEY}
    level_key level
    authorization ${AUTH_TYPE}
    message_key ${MESSAGE_KEY}
    endpoint ${ENDPOINT_URL}
{% endif %}
