{%- set masternode = salt['ydputils.get_masternodes']()[0] -%}
{%- set monitoring_hostname =  salt['pillar.get']('data:monitoring:hostname', 'monitoring.api.cloud.yandex.net') -%}
{%- set config_agent = salt['pillar.get']('data:agent', {}) -%}
{%- set config_topology = salt['pillar.get']('data:topology', {}) -%}

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

[global_tags]
  resource_type = "cluster"
  resource_id = "{{ config_agent.get('cid', '') }}"
  zone_id = "{{ config_topology.get('zone_id', '') }}"

[[inputs.disk]]
  #collect disk data from host
  ## By default stats will be gathered for all mount points.
  ## Set mount_points will restrict the stats to only the specified mount points.
  mount_points = ["/"]
  name_override = "system.disk"

{% if salt['ydputils.is_masternode']() %}
[[inputs.http]]
  #URL of YARN resource manager. Response in JSON format
  urls = ["http://{{ masternode }}:8088/ws/v1/cluster/metrics"]

  #Overwrite measurement name from default `http` to `dataproc`
  name_override = "dataproc"

  #Exclude url from tags
  tagexclude = ["url"]

  #Data from HTTP in JSON format
  data_format = "json"

  #Do not include per-partition metrics as fields
  fielddrop = ["clusterMetrics_totalUsedResourcesAcross*", "clusterMetrics_totalClusterResourcesAcross*"]

[[inputs.http]]
  #URL of hadoop namenode. Response in JSON format
  urls = ["http://{{ masternode }}:9870/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo"]

  #Overwrite measurement name from default `http` to `dfs`
  name_override = "dfs"

  #Exclude url from tags
  tagexclude = ["url"]

  #Data from HTTP in JSON format
  data_format = "json"
  # only dfs capacity info
  fieldpass = ["beans_0_Total", "beans_0_Free", "beans_0_Used", "beans_0_PercentRemaining", "beans_0_PercentUsed", "beans_0_NonDfsUsedSpace"]

[[inputs.http]]
  #URL of Dataproc Agent metrics server. Response in JSON format
  urls = ["http://localhost:8090/metrics"]

  #Overwrite measurement name from default `http` to `dataproc`
  name_override = "dataproc"

  #Exclude url from tags
  tagexclude = ["url"]

  #Data from HTTP in JSON format
  data_format = "json"

[[processors.strings]]
  # yarn cluster metrics
  [[processors.strings.replace]]
     field_key = "*"
     old = "clusterMetrics_"
     new = "yarn.cluster."

  # dfs metrics
  [[processors.strings.replace]]
     field_key = "*"
     old = "beans_0_"
     new = "dfs.cluster."
  [[processors.strings.replace]]
     field_key = "dfs.cluster.Total"
     old = "Total"
     new = "Total_bytes"
  [[processors.strings.replace]]
     field_key = "dfs.cluster.Used"
     old = "Used"
     new = "Used_bytes"
  [[processors.strings.replace]]
     field_key = "dfs.cluster.Free"
     old = "Free"
     new = "Free_bytes"
  [[processors.strings.replace]]
     field_key = "dfs.cluster.NonDfsUsedSpace"
     old = "NonDfsUsedSpace"
     new = "NonDfsUsedSpace_bytes"

{% endif %}

[[processors.strings]]
  # disk host metrics
  [[processors.strings.replace]]
     field_key = "total"
     old = "total"
     new = "system.disk.total_bytes"
  [[processors.strings.replace]]
     field_key = "used"
     old = "used"
     new = "system.disk.used_bytes"
  [[processors.strings.replace]]
     field_key = "free"
     old = "free"
     new = "system.disk.free_bytes"
  [[processors.strings.replace]]
     field_key = "*"
     old = "inodes_"
     new = "system.disk.inodes_"
  [[processors.strings.replace]]
     field_key = "used_percent"
     old = "used_percent"
     new = "system.disk.used_percent"
     # used_percent is calculated by doing used / (used + free),
     # not used / total, which is how the unix df command does it

# all values to float
[[processors.converter]]
  ## Tags to convert
  ##
  ## The table key determines the target type, and the array of key-values
  ## select the keys to convert.  The array may contain globs.
  ##   <target-type> = [<tag-key>...]
  [processors.converter.tags]
    float = []

  ## Fields to convert
  ##
  ## The table key determines the target type, and the array of key-values
  ## select the keys to convert.  The array may contain globs.
  ##   <target-type> = [<field-key>...]
  [processors.converter.fields]
    float = ["*"]

[[outputs.yandex_cloud_monitoring]]
  endpoint_url = "https://{{ monitoring_hostname }}/monitoring/v2/data/write"
  service = "data-proc"
