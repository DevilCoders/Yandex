# -*- coding: utf-8 -*-
"""
Redis module for salt
"""

from __future__ import absolute_import, print_function, unicode_literals

import logging
import os.path
import socket
import subprocess
import time
import traceback
from collections import namedtuple
from copy import deepcopy
from functools import wraps

try:
    from salt.ext import six
except ImportError as e:
    import six

    import cloud.mdb.salt_tests.common.arc_utils as arc_utils

    arc_utils.raise_if_not_arcadia(e)

try:
    from redis import StrictRedis
    from redis.exceptions import AuthenticationError, BusyLoadingError, ResponseError
    from redis.sentinel import Sentinel

    HAS_REDIS_LIB = True
except ImportError:
    HAS_REDIS_LIB = False

LOG = logging.getLogger(__name__)

_default = object()

__salt__ = {}

REBALANCE_TIMEOUT = 7200
DELETE_NODE_TIMEOUT = 90
CONNECTION_ATTEMPTS = 120
ENSURE_NOT_MASTER_TIMEOUT = 30
ADD_CLUSTER_NODE_TIMEOUT = 90
CREATE_CLUSTER_TIMEOUT = 90


def __virtual__():
    """
    We always return True here to allow using calls to module
    in plain states
    """
    return True


def _get_config_option(option, config_path, index=1):
    """
    Get full value of option from config file.
    We expect config generated by our salt component here.
    """
    string_start = '{option} '.format(option=option)
    try:
        with open(config_path) as config:
            for line in config:
                if line.lower().startswith(string_start.lower()):
                    stripped = line.split(' ')[index].strip()
                    # Workaround for quotted config options
                    # (created by config rewrite command)
                    if stripped.startswith('"') and stripped.endswith('"'):
                        return stripped[1:-1]
                    return stripped
    except Exception as exc:
        LOG.error('Unable to get %s from %s: %s', option, config_path, repr(exc))


def _get_connection(password, port=None, host='localhost', attempts=CONNECTION_ATTEMPTS, tls=False):
    """
    Get connection with redis.
    If auth fails with supplied password
    we fall back to requirepass value from config.
    This is dirty but allows password change via pillar.
    """
    if port is None:
        if tls:
            port = get_redis_tls_port()
        else:
            port = get_redis_notls_port()
    ssl_ca_certs = get_ca_cert() if tls else None
    for _ in range(attempts):
        try:
            conn = StrictRedis(host=host, port=port, db=0, password=password, ssl=tls, ssl_ca_certs=ssl_ca_certs)
            conn.ping()
            return conn
        except (AuthenticationError, ResponseError) as exc:
            LOG.warning('Connection with initially supplied password failed: %s', exc)
            # Actual password resides in /etc/redis/redis.conf
            fallback_password = _get_config_option('requirepass', '/etc/redis/redis.conf')
            conn = StrictRedis(
                host='localhost', port=port, db=0, password=fallback_password, ssl=tls, ssl_ca_certs=ssl_ca_certs
            )
            conn.ping()
            return conn
        except BusyLoadingError:
            time.sleep(5)


def _get_redis_cmd_name(command, config_path='/etc/redis/redis-main.conf'):
    """
    Get renamed Redis command from config.
    """
    return _get_config_option('rename-command {}'.format(command), config_path, index=2) or command


def _get_sentinel_cmd_name(command, config_path='/etc/redis/sentinel.conf'):
    """
    Get renamed Sentinel command from config.
    """
    return _get_config_option('sentinel rename-{}'.format(command), config_path, index=3) or command


def _get_cluster_cmd_name(command, config_path='/etc/redis/redis-main.conf'):
    """
    Get renamed Cluster subcommand from config.
    """
    return _get_config_option('rename-cluster-subcommand {}'.format(command), config_path, index=2) or command


def _get_option_from_conn(conn, option):
    config_cmd_name = get_redis_config_cmd()
    try:
        ret = conn.execute_command(config_cmd_name, 'get', option)
    except Exception as exc:
        LOG.debug('Unable to get %s option value: %s', option, repr(exc))
        ret = None
    return ret


def _set_client_output_buffer_limit(conn, config_cmd_name, test, option, value):
    """
    Set client-output-buffer-limit options.
    """
    key = 'client-output-buffer-limit'
    if not option.startswith(key):
        return
    value = value
    opts = option.split()

    ret = _get_option_from_conn(conn, key)
    if not ret:
        # Unknown config option. Skip
        return

    _, current_value = ret
    current_value = ensure_str(current_value).replace('slave', 'replica')
    new_value = '{} {}'.format(opts[1], value)

    if new_value not in current_value:
        if test:
            return value
        try:
            ret = conn.execute_command(config_cmd_name, 'set', key, new_value)
            if ensure_str(ret) == 'OK':
                return value
        except ResponseError as exc:
            LOG.warning('Changing config %s failed: %s', key, repr(exc))


def _option_differs(option, current_value, new_value):
    if option == 'notify-keyspace-events':
        return set(current_value) != set(new_value)

    return current_value != new_value


def _ensure_option(conn, config_cmd_name, test, option, value):
    """
    Ensure that option is set to value.
    """
    if value == '""' and option == 'notify-keyspace-events':
        value = ''
    if option.startswith('client-output-buffer-limit'):
        return _set_client_output_buffer_limit(conn, config_cmd_name, test, option, value)
    ret = _get_option_from_conn(conn, option)
    if not ret:
        # Unknown config option. Skip
        return
    _, current_value = ret
    str_val = str(value) if not isinstance(value, six.string_types) else value
    if _option_differs(option, ensure_str(current_value), str_val):
        if test:
            return value
        try:
            ret = conn.execute_command(config_cmd_name, 'set', option, str_val)
            if ensure_str(ret) == 'OK':
                return value
        except ResponseError as exc:
            LOG.warning('Changing config %s failed: %s', option, repr(exc))


def _is_porto():
    return __salt__['dbaas.is_porto']()


def set_options(options, password=None, port=None, test=False, config_path='/etc/redis/redis-main.conf'):
    """
    Set config options with config rewrite in the end.
    Returns dict of changed values.
    """
    if not HAS_REDIS_LIB:
        raise RuntimeError('Unable to import redis module')
    changed = {}
    config_cmd_name = _get_redis_cmd_name('config', config_path)
    conn = _get_connection(password=password, port=port)
    for option, value in options.items():
        changed[option] = _ensure_option(conn, config_cmd_name, test, option, value)
    if not test:
        conn.execute_command(config_cmd_name, 'rewrite')

    if any(changed.values()):
        return dict((k, v) for k, v in changed.items() if v)

    return dict()


class SentinelMasterData:
    def __init__(self):
        self.num_slaves = 0
        self.chosen_by_sentinels_count = 0
        self.ip = None
        self._data = None
        self.host = None

    def __str__(self):
        return "host={};data={}".format(self.host, self._data)

    def update(self, host, data):
        self._data = data
        self.num_slaves = max(self.num_slaves, data['num-slaves'])
        self.chosen_by_sentinels_count += 1
        self.ip = data['ip']
        self.host = host

    def is_better_than(self, data):
        if data is None:
            return True

        if self.num_slaves > data.num_slaves:
            return True
        elif self.num_slaves < data.num_slaves:
            return False

        if self.chosen_by_sentinels_count > data.chosen_by_sentinels_count:
            return True
        elif self.chosen_by_sentinels_count < data.chosen_by_sentinels_count:
            return False

        if self.host is None and data.host is not None:
            return False
        if self.host is not None and data.host is None:
            return True
        if self.host is not None and data.host is not None:
            if self.host > data.host:
                return True
            elif self.host < data.host:
                return False

        if self.ip is None and data.ip is not None:
            return False
        if self.ip is not None and data.ip is None:
            return True
        if self.ip is not None and data.ip is not None and self.ip > data.ip:
            return True
        return False


SentinelMasterKey = namedtuple('SentinelMasterKey', ["host", "ip"])


class SentinelMasters:
    def __init__(self):
        self.masters = dict()

    def add(self, host, data):
        key = SentinelMasterKey(host, data['ip'])
        if key not in self.masters:
            self.masters[key] = SentinelMasterData()
        self.masters[key].update(host, data)

    def get_the_best_one(self):
        if len(self.masters) == 0:
            LOG.debug('cluster sentinels know 0 masters')
            return None, None
        if len(self.masters) == 1:
            LOG.debug('cluster sentinels know 1 master')
        else:
            LOG.debug(
                'cluster sentinels know {} masters, going to choose 1 from {}'.format(len(self.masters), self.masters)
            )

        the_best_data = None
        for key, data in self.masters.items():
            if data.is_better_than(the_best_data):
                the_best_data = data
        LOG.debug('master to use in config: {}'.format(the_best_data))
        return the_best_data.host, the_best_data.ip


def _add_sentinel_masters(host, conn, masters):
    for _, sentinel in enumerate(conn.sentinels):
        try:
            cur_masters = sentinel.sentinel_masters()
        except Exception as exc:
            LOG.debug("unable to get master names: {}".format(repr(exc)))
            continue
        for _, data in cur_masters.items():
            masters.add(host, data)


def _get_sentinel_masters(hosts, port):
    masters = SentinelMasters()
    for host in hosts:
        sentinel_conn = Sentinel([(host, port)], socket_timeout=1)
        _add_sentinel_masters(host, sentinel_conn, masters)
    return masters


def redirect_redis(hosts, master):
    if master is None:
        return

    password = get_redis_password()
    slaveof_cmd = get_redis_slaveof_cmd()
    config_cmd = get_redis_config_cmd()
    replication_port = get_redis_replication_port()

    for host in hosts:
        try:
            conn = _get_connection(password, host=host)
            if host == master:
                res = conn.execute_command(slaveof_cmd, "no", "one")
            else:
                res = conn.execute_command(slaveof_cmd, master, replication_port)
            LOG.debug("setting {} master for {} result: {}".format(master, host, res))
            res = conn.execute_command(config_cmd, 'rewrite')
            LOG.debug("rewriting config for {} result: {}".format(host, res))
        except Exception as exc:
            LOG.debug("setting {} master for {} exception: {}".format(master, host, repr(exc)))


def _get_sentinel_master(hosts, port):
    """
    Get master IP from connection with sentinels.
    Unfortunately sentinels know nothing about auth atm
    (See https://github.com/antirez/redis/pull/3329 for details).
    """
    if not HAS_REDIS_LIB:
        raise RuntimeError('Unable to import redis module')

    masters = _get_sentinel_masters(hosts, port)
    _, master_ip = masters.get_the_best_one()
    return master_ip


def get_ip(fqdn=None):
    """
    Translate the fqdn to IP address.
    Tries IPv4 first, then IPv6.
    If fqdn is None, returns the IP of the current host.
    """
    if fqdn is None:
        fqdn = socket.gethostname()
    try:
        return socket.gethostbyname(fqdn)
    except Exception:
        LOG.debug('Unable to translate "{}" to IPv4 address'.format(fqdn))
    try:
        return socket.getaddrinfo(fqdn, None, socket.AF_INET6)[0][4][0]
    except Exception:
        LOG.debug('Unable to translate "{}" to IPv6 address'.format(fqdn))
    return fqdn


def get_master(hosts, sentinel_port):
    """
    Get current master IP.
    """
    if len(hosts) == 1:
        return get_ip()
    return _get_sentinel_master(hosts, sentinel_port)


def get_redis_data_folder():
    """
    Redis data folder
    :return:
    """
    return '/var/lib/redis'


def get_aof_filename():
    """
    Append-only file name
    :return:
    """
    return 'appendonly.aof'


def get_aof_dirpath():
    """
    Append-only dir path. Used since Redis 7.0
    :return:
    """
    return os.path.join(get_redis_data_folder(), 'appendonlydir')


def get_aof_fullpath():
    """
    Append-only full path
    :return:
    """
    folder = get_redis_data_folder()
    name = get_aof_filename()
    return os.path.join(folder, name)


def get_rdb_filename():
    """
    RDB file name
    :return:
    """
    return 'dump.rdb'


def get_rdb_fullpath():
    """
    RDB full path
    :return:
    """
    folder = get_redis_data_folder()
    name = get_rdb_filename()
    return os.path.join(folder, name)


def rewrite_aof(password, port=None, test=False, config_path='/etc/redis/redis-main.conf'):
    """
    Call BGREWRITEAOF command.
    """
    filename = get_aof_filename()
    if not test:
        rewrite_aof_cmd = _get_redis_cmd_name('bgrewriteaof', config_path)
        conn = _get_connection(password=password, port=port)
        try:
            conn.execute_command(rewrite_aof_cmd)
        except ResponseError:
            return {filename: 'rewriting has already been started'}
        return {filename: 'rewriting started'}
    return {}


def _get_info_field(host, field, password, port=None):
    """
    Execute INFO on host and return the field of interest.
    """
    conn = _get_connection(host=host, password=password, port=port)
    info = conn.info()
    return info.get(field)


def _get_cluster_info_field(host, field, password, port=None):
    """
    Execute CLUSTER INFO on host and return the field of interest.
    """
    conn = _get_connection(host=host, password=password, port=port)
    info = conn.cluster('info')
    return info.get(field)


def _is_master(host, password, port=None):
    """
    Check if the host has role 'master'.
    """
    role = _get_info_field(host, 'role', password, port=port)
    return role == 'master'


def _is_node_added_to_sharded_cluster(host, password, port=None):
    """
    Check if the host knows about valid cluster.
    """
    state = _get_cluster_info_field(host, 'cluster_state', password, port=port)
    return state == 'ok'


def filter_alive_hosts(hosts):
    """
    Return list of cluster hosts skipping unavailable
    """
    password = get_redis_password()
    ret = []
    for host in hosts:
        try:
            _get_connection(host=host, password=password)
            ret.append(host)
        except Exception as exc:
            LOG.warning('Unable to connect to %s: %r. Skipping as unavailable', host, exc)

    if not ret:
        raise RuntimeError('Unable to find any alive host in {hosts}'.format(hosts=', '.join(hosts)))

    return ret


def ensure_not_master(
    host=None, hosts=None, master_name=None, sentinel_port=None, test=False, timeout=ENSURE_NOT_MASTER_TIMEOUT
):
    """
    Ensure that the host is not a master.
    If no params passed, checks the localhost.
    """
    if sentinel_port is None:
        sentinel_port = get_sentinel_notls_port()
    if not host:
        host = socket.gethostname()
    host = host.strip()
    hosts = hosts or [host]
    if host not in hosts:
        hosts.append(host)
    master_name = master_name or _get_config_option(
        option='sentinel monitor', config_path='/etc/redis/sentinel.conf', index=2
    )

    def current_master():
        return get_master(hosts, sentinel_port)

    if current_master() != get_ip(host):
        return {}
    if not test:
        start_failover(master_name, sentinel_port, test)
        deadline = time.time() + timeout
        while time.time() <= deadline:
            if current_master() != get_ip(host):
                return {'new_master': current_master()}
            time.sleep(1)
        raise RuntimeError('Failed to switch master')
    return {}


def start_failover(master_name, sentinel_port=None, test=False):
    """
    Start manual failover.
    """
    if sentinel_port is None:
        sentinel_port = get_sentinel_notls_port()
    sentinel = _get_connection(None, sentinel_port)
    failover_command = _get_sentinel_cmd_name('failover')
    if test:
        return {}

    try:
        sentinel.execute_command('sentinel', failover_command, master_name)
    except ResponseError as err:
        if 'INPROG' in str(err):
            return {'failover': 'already in progress'}
        raise
    return {'failover': 'started'}


def reset_sentinel(master_name, sentinel_port=None, test=False):
    """
    Reset the Sentinel configuration.
    Wipes out all previously discovered Sentinels and Slaves.
    Rediscovers hosts via the master.
    """
    if sentinel_port is None:
        sentinel_port = get_sentinel_notls_port()
    sentinel = _get_connection(None, sentinel_port)
    reset_command = _get_sentinel_cmd_name('reset')
    if test:
        return {}

    sentinel.execute_command('sentinel', reset_command, master_name)
    return {'/etc/redis/sentinel.conf': 'changed'}


def init_cluster(shards, password, tools_port=None, cluster_port=None, replicas=0, test=False):
    """
    Create cluster with masters only. Attach replicas one by one.

    shards: {
        "shard_id1": {
            "hosts": {
                "host1.net": {},
                ...
            },
            "name": "shard1"
        },
        ...
    }
    """
    master_hosts = []
    to_attach = []  # [(master1, [replica1, replica2]), ...]
    for shard in shards.values():
        hosts = list(shard['hosts'].keys())
        master_hosts.append(hosts[0])
        if len(hosts) > 1:
            to_attach.append((hosts[0], hosts[1:]))
    create_cluster(
        master_hosts, password, tools_port=tools_port, cluster_port=cluster_port, replicas=replicas, test=test
    )
    for master, replica_hosts in to_attach:
        for replica in replica_hosts:
            add_cluster_node(
                host=replica,
                password=password,
                shard_hosts=[master],
                tools_port=tools_port,
                cluster_port=cluster_port,
                test=test,
            )
    return {'cluster init': 'ok'}


def get_cli_list():
    cli = ['redis-cli']
    if tls_enabled():
        cli.extend(['--tls', '--cacert', get_ca_cert()])
    return cli


def get_base_cluster_command(timeout, password):
    cli_list = get_cli_list()
    port = get_redis_replication_port()
    cmd = (
        [
            'timeout',
            str(timeout),
        ]
        + cli_list
        + [
            '-p',
            str(port),
            '--server-config',
            '/etc/redis/redis-main.conf',
        ]
    )
    if password:
        cmd += [
            '-a',
            password,
        ]
    return cmd


def create_cluster(
    hosts, password, tools_port=None, cluster_port=None, replicas=0, timeout=CREATE_CLUSTER_TIMEOUT, test=False
):
    """
    Create Redis cluster.
    Distribute hash slots and attach replicas if needed.
    """
    if test:
        return {}
    if cluster_port is None:
        cluster_port = get_redis_replication_port()
    nodes = ['{ip}:{port}'.format(ip=get_ip(h), port=cluster_port) for h in hosts]
    for host in hosts:
        _reset_cluster_node(host=host, password=password, port=tools_port)
    cmd = get_base_cluster_command(timeout, password)
    cmd += [
        '--cluster-replicas',
        str(replicas),
        '--cluster',
        'create',
    ] + nodes
    proc = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    output, err = proc.communicate(b'yes')
    if proc.returncode:
        raise RuntimeError('out={}:err={}:cmd={}:code={}'.format(output, err, " ".join(cmd), proc.returncode))
    return {
        'cluster created': 'ok',
    }


def get_cluster_node_id(host, password, port=None, use_fallback=False):
    """
    Returns cluster node id by the hostname.
    """
    try:
        conn = _get_connection(host=host, password=password, port=port)
        nodes = ensure_str(conn.execute_command('cluster', 'nodes')).strip().split('\n')
        for node in nodes:
            node_id, _, role = node.split()[:3]
            if role.startswith('myself'):
                return node_id
    except Exception as exc:
        LOG.error('Unable to get node id for %s: %r', host, exc)
        if use_fallback:
            addresses = {x[4][0] for x in socket.getaddrinfo(host, None)}
            with open('/etc/redis/cluster.conf') as cluster_file:
                for line in cluster_file:
                    node_id, ip_port = line.split()[:2]
                    for address in addresses:
                        if ip_port.startswith('{addr}:'.format(addr=address)):
                            return node_id
            return None
    raise RuntimeError('Failed to get node id for {}'.format(host))


def _get_shard_master(shard_hosts, password, port=None):
    """
    Iterate through the list of shard hosts.
    Return the master. Throw RuntimeError if no master has been found.
    """
    masters = [host for host in shard_hosts if _is_master(host=host, password=password, port=port)]
    if len(masters) > 1:
        masters = [
            host for host in masters if _is_node_added_to_sharded_cluster(host=host, password=password, port=port)
        ]
    if len(masters) == 1:
        return masters[0]

    if len(masters) == 0:
        raise RuntimeError('There is no master in shard "{}"'.format(shard_hosts))
    else:
        raise RuntimeError('Master choice is ambiguous in shard "{}"'.format(shard_hosts))


def _is_node_ok(host, password, port=None):
    """
    Check the cluster node state.
    """
    conn = _get_connection(host=host, password=password, port=port)
    state = conn.cluster('info')['cluster_state']
    return state == 'ok'


def add_cluster_node(
    host,
    password,
    old_host=None,
    shard_hosts=None,
    tools_port=None,
    cluster_port=None,
    timeout=ADD_CLUSTER_NODE_TIMEOUT,
    test=False,
):
    """
    Adds a node to the cluster.
    If `shard_hosts` (list) is set, attaches new host as a replica to the shard.
    Otherwise creates an empty master.
    `old_host` can be any existing host of a cluster.
    """
    if test:
        return {}
    if _is_node_ok(host, password, tools_port):
        return {'already added': host}
    _reset_cluster_node(host, password, tools_port)
    master_host = None
    if shard_hosts:
        master_host = _get_shard_master(shard_hosts, password, tools_port)
    if cluster_port is None:
        cluster_port = get_redis_replication_port()
    cmd = get_base_cluster_command(timeout, password)
    cmd += [
        '--cluster',
        'add-node',
        '{}:{}'.format(get_ip(host), cluster_port),
        '{}:{}'.format(get_ip(master_host or old_host), cluster_port),
    ]
    if master_host:
        cmd += ['--cluster-slave', '--cluster-master-id', get_cluster_node_id(master_host, password, port=tools_port)]
    subprocess.check_call(cmd)
    return {'added host': host}


def _get_slave_ip(host, password, port=None):
    slave = _get_info_field(host, 'slave0', password, port=port)
    return slave['ip']


def _execute_cluster_command(host, cmd, password):
    """
    Execute the command on the specified host.
    """
    failover_cmd_name = _get_cluster_cmd_name(cmd)
    tls = tls_enabled()
    conn = _get_connection(host=host, password=password, tls=tls)
    conn.execute_command('cluster', failover_cmd_name)


def _reset_cluster_node(host, password, port=None):
    """
    Clean up the node. Execute 'FLUSHALL' and 'CLUSTER RESET'.
    """
    conn = _get_connection(host=host, password=password, port=port)
    try:
        conn.flushall()
    except ResponseError:
        pass
    _execute_cluster_command(host=host, cmd='reset', password=password)


def _is_orphan(host, password, port=None):
    """
    Check if the node is not part of the cluster.
    """
    conn = _get_connection(host=host, password=password, port=port)
    info = conn.cluster('info')
    state = info['cluster_state']
    cluster_size = info['cluster_size']
    return state == 'fail' and int(cluster_size) == 0


def delete_cluster_node(
    host, password, tools_port=None, cluster_port=None, timeout=DELETE_NODE_TIMEOUT, failover=True, test=False
):
    """
    Delete the host from the cluster.
    If the node is master, switch over first.
    """
    if test:
        return {}
    node_not_available = False
    try:
        if _is_orphan(host, password, port=tools_port):
            return {'already deleted': host}
        if failover and _is_master(host, password, port=tools_port):
            slave_ip = _get_slave_ip(host, password, port=tools_port)
            _execute_cluster_command(slave_ip, 'failover', password)
    except Exception as exc:
        LOG.error('Error connecting to %s: %r', host, exc)
        node_not_available = True
    if cluster_port is None:
        cluster_port = get_redis_replication_port()
    cmd = get_base_cluster_command(timeout, password)
    node_id = get_cluster_node_id(host, password, port=tools_port, use_fallback=node_not_available)
    if not node_id:
        LOG.error('Unable to get node_id for unavailable host %s', host)
        return {'deleted host': host}
    if node_not_available:
        cmd += [
            '--cluster',
            'call',
            '{}:{}'.format(get_ip(), cluster_port),
            'cluster',
            _get_cluster_cmd_name('FORGET'),
            node_id,
        ]
    else:
        cmd += [
            '--cluster',
            'del-node',
            '{}:{}'.format(get_ip(), cluster_port),
            node_id,
        ]
    subprocess.check_call(cmd)
    return {'deleted host': host}


def rebalance_slots(
    password,
    tools_port=None,
    cluster_port=None,
    weights=None,
    unavailable_masters=False,
    timeout=REBALANCE_TIMEOUT,
    test=False,
):
    """
    Rebalance cluster slot distribution.

    weights (dict): hostname -> weight (int)
    """
    if test:
        return {}
    weight_args = []
    if weights:
        weight_args.append('--cluster-weight')
        for host, weight in weights.items():
            node_id = get_cluster_node_id(host, password, port=tools_port)
            weight_args.append('{}={}'.format(node_id, weight))
    if cluster_port is None:
        cluster_port = get_redis_replication_port()
    addr = '{}:{}'.format(get_ip(), cluster_port)
    fix_cmd = get_base_cluster_command(timeout, password)
    fix_cmd += [
        '--cluster',
        'fix',
        addr,
    ]
    if unavailable_masters:
        fix_cmd += ['--cluster-fix-with-unreachable-masters']
        proc = subprocess.Popen(fix_cmd, stdin=subprocess.PIPE)
        proc.communicate(b'yes')
        if proc.returncode != 0:
            raise RuntimeError(
                'Cluster fix cmd failed: "{cmd}" with {code}'.format(cmd=' '.join(fix_cmd), code=proc.returncode)
            )
        return {'fix-with-unreachable': 'ok'}
    else:
        subprocess.check_call(fix_cmd)
    rebalance_cmd = get_base_cluster_command(timeout, password)
    rebalance_cmd += (
        [
            '--cluster',
            'rebalance',
        ]
        + weight_args
        + [
            addr,
            '--cluster-use-empty-masters',
        ]
    )
    subprocess.check_call(rebalance_cmd)
    return {'rebalance': 'ok'}


def remove_shard(
    shard_hosts,
    password,
    tools_port=None,
    cluster_port=None,
    rebalance_timeout=REBALANCE_TIMEOUT,
    delete_node_timeout=DELETE_NODE_TIMEOUT,
    test=False,
):
    """
    Rebalance the slots to free the target shard and
    delete all the shard nodes from the cluster.
    """
    if test:
        return {}
    master_host = None
    unavailable_hosts = 0
    for host in shard_hosts:
        try:
            if _is_master(host, password, port=tools_port) and not _is_orphan(host, password, port=tools_port):
                master_host = host
                break
        except Exception as exc:
            logging.error('Unable to connect to %s: %r', host, exc)
            unavailable_hosts += 1

    if master_host:
        rebalance_slots(
            password=password,
            tools_port=tools_port,
            cluster_port=cluster_port,
            weights={master_host: 0},
            timeout=rebalance_timeout,
            test=test,
        )
        shard_hosts.remove(master_host)
        shard_hosts.append(master_host)
    elif len(shard_hosts) == unavailable_hosts:
        rebalance_slots(
            password=password,
            tools_port=tools_port,
            cluster_port=cluster_port,
            unavailable_masters=True,
            timeout=rebalance_timeout,
            test=test,
        )

    for host in shard_hosts:
        delete_cluster_node(
            host,
            password,
            tools_port=tools_port,
            cluster_port=cluster_port,
            timeout=delete_node_timeout,
            failover=False,
            test=test,
        )
    return {'removed shard': shard_hosts}


def pillar(key, default=_default):
    """
    Like __salt__['pillar.get'], but when the key is missing and no default value provided,
    a KeyError exception will be raised regardless of 'pillar_raise_on_missing' option.
    """
    value = __salt__['pillar.get'](key, default=default)
    if value is _default:
        raise KeyError('Pillar key not found: {0}'.format(key))

    return value


def grains(key, default=_default, **kwargs):
    """
    Like __salt__['grains.get'], but when the key is missing and no default value provided,
    a KeyError exception will be raised.
    """
    value = __salt__['grains.get'](key, default=default, **kwargs)
    if value is _default:
        raise KeyError('Grains key not found: {0}'.format(key))

    return value


def tls_enabled():
    """
    Return True if tls support is enabled.
    """
    return pillar('data:redis:tls:enabled', False)


def get_redis_tls_port():
    return pillar('data:redis:tls:redis_port', 6380)


def get_redis_notls_port():
    return pillar('data:redis:config:port', 6379)


def get_redis_replication_port():
    if tls_enabled():
        return get_redis_tls_port()
    return get_redis_notls_port()


def get_sentinel_tls_port():
    return pillar('data:sentinel:config:port', 26380)


def get_sentinel_notls_port():
    return pillar('data:sentinel:config:port', 26379)


def get_sharded_tls_port():
    return 16380


def get_sharded_notls_port():
    return 16379


def get_tls_ports():
    ports = [get_redis_tls_port(), get_sharded_tls_port(), get_sentinel_tls_port()]
    return sorted(set(ports))


def public_ip_assigned():
    return pillar('data:dbaas:assign_public_ip', False)


def is_sentinel_cluster():
    return pillar('data:redis:config:cluster-enabled', None) != 'yes'


def get_public_ports():
    if _is_porto():
        if tls_enabled():
            ports = [get_redis_tls_port()]
            if is_sentinel_cluster():
                ports.append(get_sentinel_tls_port())
            return sorted(set(ports))

        ports = [get_redis_notls_port()]
        if is_sentinel_cluster():
            ports.append(get_sentinel_notls_port())
        return sorted(set(ports))

    if public_ip_assigned() and tls_enabled():
        ports = [get_redis_tls_port()]
        if is_sentinel_cluster():
            ports.append(get_sentinel_tls_port())
        return sorted(set(ports))
    return []


def get_all_ports():
    ports = [get_redis_notls_port(), get_sharded_notls_port(), get_sentinel_notls_port()]
    if tls_enabled():
        tls_ports = get_tls_ports()
        ports.extend(tls_ports)
    return sorted(set(ports))


def get_shard_hosts():
    return pillar('data:dbaas:shard_hosts', [])


def get_master_from_pillar():
    master = pillar('redis-master', None)
    if not master:
        shard_hosts = get_shard_hosts()
        sentinel_port = get_sentinel_notls_port()
        master = get_master(shard_hosts, sentinel_port)
    else:
        master = get_ip(master)
    return master


def get_slaveof_string():
    slaveof_string = None
    if is_sentinel_cluster():
        master = get_master_from_pillar()
        if master is not None and master not in grains('ip_interfaces:eth0', []) and master != grains('id', None):
            slaveof_string = '{} {}'.format(master, get_redis_replication_port())
    return slaveof_string


def get_master_for_sentinel():
    master = get_master_from_pillar()
    redis_master = master if master is not None else grains('ip_interfaces:eth0')[0]
    return redis_master


def get_splitbrain_state_file():
    return pillar('data:redis:splitbrain_state_file', '/tmp/splitbrain.detected')


def get_replication_state_file():
    return pillar('data:redis:replication_state_file', '/tmp/replication.trouble.detected')


def get_config_name():
    return "/etc/redis/redis.conf"


def is_managed_cluster():
    return pillar('data:dbaas:cluster', None)


def get_redispass_file(user=""):
    if user == "":
        return '/root/.redispass'
    return "/home/{}/.redispass".format(user)


def get_ca_cert():
    return "/etc/redis/tls/ca.crt"


def get_cert():
    return "/etc/redis/tls/server.crt"


def get_subcid():
    return pillar('data:dbaas:subcluster_id')


def get_shards():
    return pillar('data:dbaas:cluster:subclusters:{subcid}:shards'.format(subcid=get_subcid())).items()


def get_backup_warn_count_limit():
    shards_num = len(get_shards())
    single = 32  # some historical value, dont sure why exactly
    sharded = shards_num * (7 + 2)  # 7 days + 2 for some temp troubles
    return single if shards_num == 1 else sharded


def get_output_buffer_limits():
    # client-output-buffer-limit <class> <hard limit> <soft limit> <soft seconds>
    pillar_path = 'data:redis:config'
    normal_path = "client-output-buffer-limit normal"
    try:
        config = pillar(pillar_path)
        limits = config[normal_path]
        hard, soft, _ = limits.split()
    except (ValueError, KeyError) as exc:
        LOG.error('Unable to get %s from %s: %s', normal_path, pillar_path, repr(exc))
        hard, soft = 0, 0
    return {'hard': hard, 'soft': soft}


def flavor_cpu_guarantee():
    return int(pillar('data:dbaas:flavor:cpu_guarantee', 0))


def get_redisctl_path():
    return pillar('data:redis:config:tools:redisctl_path', '/usr/local/yandex/redisctl/redisctl.py')


def walg_get_create_cmd():
    override = pillar('data:walg:create_cmd_override', '')
    if override:
        return override

    if flavor_cpu_guarantee() < 2 or pillar('data:walg:create_cmd_redisctl', False):
        deadline = pillar('data:walg:redisctl_dump_deadline', 60 * 60)
        return 'sudo {} --action=dump --file=- --bgsave-wait={}'.format(get_redisctl_path(), deadline)

    return '/usr/local/bin/redis_cli.sh'


def only_walg_enabled():
    return pillar('data:walg:restore', True)


def is_walg_enabled():
    return pillar('data:walg:enabled', True)


def get_common_data(user, group):
    return {
        'Debian': {
            'user': user,
            'group': group,
        }
    }


def get_default_versions_map():
    return {'major_version': '6.2', 'package_version': '6.2.6-c204699e-yandex--ssl'}


def get_major_version():
    version_d = pillar('data:versions:redis', get_default_versions_map())
    return int(version_d['major_version'].split('.')[0])


def update_config_version(conf):
    version = pillar('data:versions:redis', get_default_versions_map())
    major_human_meta = version.get('major_version')
    pkg_version_meta = version.get('package_version')
    if not pkg_version_meta:
        raise RuntimeError('pillar key data:versions:redis:package_version is not filled')
    if not major_human_meta:
        raise RuntimeError('pillar key data:versions:redis:major_version is not filled')

    conf['version'] = {}
    conf['version']['major_human'] = major_human_meta
    conf['version']['pkg'] = pkg_version_meta
    conf['version']['major_num'] = get_major_num(major_human_meta)


def get_major_num(major_human_meta):
    major, middle = [int(_) for _ in major_human_meta.split('.')]
    return str(major * 100 + middle)


def update_config_tls(conf, tls_folder):
    if conf['tls']['enabled']:
        conf['config'].update(
            {
                'tls-cert-file': '{}/server.crt'.format(tls_folder),
                'tls-key-file': '{}/server.key'.format(tls_folder),
                'tls-ca-cert-file': '{}/ca.crt'.format(tls_folder),
                'tls-auth-clients': 'no',
                'tls-protocols': '"TLSv1.2 TLSv1.3"',
            }
        )


def get_redis_pidfile():
    return '/var/run/redis/redis-server.pid'


def is_major_human_6_plus(major_human):
    return major_human.startswith('6.')


def update_config_io_threads(conf):
    default_threads = 1
    major_human = conf['version']['major_human']
    if not is_major_human_6_plus(major_human):
        return
    is_allowed = pillar('data:redis:io_threads_allowed', True)
    if not is_allowed:
        conf['config']['io-threads'] = default_threads
        return
    cpu_guarantee = flavor_cpu_guarantee()
    spare_cores = 2
    max_threads = 8
    counted_threads = min(cpu_guarantee - spare_cores, max_threads)
    if counted_threads <= 1:
        conf['config']['io-threads'] = default_threads
        return
    conf['config']['io-threads'] = counted_threads


def update_config_oom_score(conf):
    '''https://github.com/redis/redis/blob/6.2/redis.conf#L1190'''

    major_human = conf['version']['major_human']
    if not is_major_human_6_plus(major_human):
        return

    conf['config']['oom-score-adj'] = 'relative'
    conf['config']['oom-score-adj-values'] = '0 100 800'


def get_redis_data(config_folder, user, group, tls_folder):
    conf = deepcopy(get_common_data(user, group))
    conf['Debian'].update(
        {
            'tls': {
                'enabled': tls_enabled(),
                'cli': 'redis_tls',
                'port': get_redis_tls_port(),
            },
            'cli': 'redis',
            'homedir': '/home/{}'.format(user),
            'config': {
                'protected-mode': 'no',
                'port': get_redis_notls_port(),
                'tcp-backlog': 511,
                'timeout': 0,
                'tcp-keepalive': 60,
                'daemonize': 'yes',
                'supervised': 'no',
                'pidfile': get_redis_pidfile(),
                'loglevel': 'notice',
                'logfile': '/var/log/redis/redis-server.log',
                'databases': 16,
                'save': '',
                'stop-writes-on-bgsave-error': 'yes',
                'rdbcompression': 'yes',
                'rdbchecksum': 'yes',
                'dbfilename': 'dump.rdb',
                'dir': get_redis_data_folder(),
                'maxclients': 65000,
                'min-slaves-to-write': 0,
                'slave-serve-stale-data': 'yes',
                'slave-read-only': 'yes',
                'repl-backlog-size': '100mb',
                'repl-backlog-ttl': 3600,
                'repl-diskless-sync': 'yes',
                'repl-disable-tcp-nodelay': 'yes',
                'slave-priority': 100,
                'appendonly': 'yes',
                'appendfilename': get_aof_filename(),
                'appendfsync': 'everysec',
                'no-appendfsync-on-rewrite': 'yes',
                'auto-aof-rewrite-percentage': 100,
                'auto-aof-rewrite-min-size': '64mb',
                'aof-load-truncated': 'yes',
                'lua-time-limit': 5000,
                'slowlog-log-slower-than': 10000,
                'slowlog-max-len': 1000,
                'latency-monitor-threshold': 100,
                'hash-max-ziplist-entries': 512,
                'hash-max-ziplist-value': 64,
                'list-max-ziplist-size': -2,
                'list-compress-depth': 0,
                'set-max-intset-entries': 512,
                'zset-max-ziplist-entries': 128,
                'zset-max-ziplist-value': 64,
                'hll-sparse-max-bytes': 3000,
                'activerehashing': 'yes',
                'client-output-buffer-limit normal': '0 0 0',
                'client-output-buffer-limit replica': '0 0 0',
                'client-output-buffer-limit pubsub': '64mb 32mb 60',
                'hz': 10,
                'aof-rewrite-incremental-fsync': 'yes',
                'rdb-save-incremental-fsync': 'yes',
                'aof-use-rdb-preamble': 'yes',
                'cluster-config-file': '{}/cluster.conf'.format(config_folder),
                'cluster-node-timeout': '15000',
                'cluster-migration-barrier': '2',
                'cluster-require-full-coverage': 'no',
                'cluster-replica-validity-factor': '10',
                'cluster-replica-no-failover': 'no',
            },
        }
    )
    redis = __salt__['grains.filter_by'](conf, merge=pillar('data:redis', {}))

    for source, target in pillar('data:redis:secrets:renames', {}).items():
        redis['config'].update({'rename-command ' + source: target})

    for source, target in pillar('data:redis:secrets:cluster_renames', {}).items():
        redis['config'].update({'rename-cluster-subcommand ' + source: target})

    update_config_version(redis)
    update_config_tls(redis, tls_folder)
    update_config_io_threads(redis)
    update_config_oom_score(redis)
    return redis


def get_sentinel_default_down_after_milliseconds():
    return pillar('data:sentinel:master_config:down-after-milliseconds', 30000)


def get_sentinel_pid():
    return pillar('data:sentinel:tools:pidfile', '/var/run/sentinel/redis-sentinel.pid')


def get_sentinel_data(user, group, tls_folder):
    conf = deepcopy(get_common_data(user, group))
    conf['Debian'].update(
        {
            'tls': {
                'enabled': tls_enabled(),
                'cli': 'sentinel_tls',
                'port': get_sentinel_tls_port(),
            },
            'cli': 'sentinel',
            'config': {
                'port': get_sentinel_notls_port(),
                'daemonize': 'yes',
                'pidfile': get_sentinel_pid(),
                'logfile': '/var/log/redis/redis-sentinel.log',
                'protected-mode': 'no',
                'sentinel set-disable': 'yes',
                'dir': get_redis_data_folder(),
            },
            'master_config': {
                'down-after-milliseconds': get_sentinel_default_down_after_milliseconds(),
                'failover-timeout': 30000,
            },
        }
    )
    sentinel = __salt__['grains.filter_by'](conf, merge=pillar('data:sentinel', {}))

    for source, target in pillar('data:redis:secrets:sentinel_direct_renames', {}).items():
        sentinel['config'].update({'rename-command ' + source: target})

    update_config_version(sentinel)

    update_config_tls(sentinel, tls_folder)
    return sentinel


def get_redis_server_start_wait():
    return pillar('data:redis:tools:server_start_wait', 7200)


def get_redis_server_stop_wait():
    return pillar('data:redis:tools:server_stop_wait', 7200)


def get_redis_server_start_sleep():
    return pillar('data:redis:tools:server_start_sleep', 10)


def get_redis_server_start_restart():
    return pillar('data:redis:tools:server_start_restarts', 10)


def get_redis_password():
    return pillar('data:redis:config:requirepass', '')


def get_redis_process_name():
    return 'redis-server'


def get_bgsave_process_name():
    return 'redis-rdb-bgsave'


def get_redispass_files():
    return (get_redispass_file(), get_redispass_file('redis'), get_redispass_file('monitor'))


def is_redisctl_enabled():
    return pillar('data:redis:tools:redisctl_enabled', True)


def ensure_no_primary_attempts():
    # sleep in script = 5 sec
    return pillar('data:redis:tools:ensure_no_primary_attempts', 720)


def get_repl_data():
    return [
        'repl_backlog_size',
        'master_repl_offset',
        'repl_backlog_histlen',
        'master_failover_state',
        'repl_backlog_active',
        'role',
        'master_replid2',
        'connected_slaves',
        'master_replid',
        'repl_backlog_first_byte_offset',
        'second_repl_offset',
        'master_host',
        'master_link_status',
        'master_last_io_seconds_ago',
        'master_sync_in_progress',
        'slave_repl_offset',
        'slave_priority',
        'slave_read_only',
        'replica_announced',
    ]


def get_redis_shared_metrics_file():
    return '/home/redis/info.json'


def get_wait_prestart_on_crash():
    wait_prestart_on_crash = pillar('data:redis:tools:wait_prestart_on_crash', None)
    return (
        int(wait_prestart_on_crash)
        if wait_prestart_on_crash is not None
        else get_sentinel_default_down_after_milliseconds() / 1000
    )


def get_infinite_wait_prestart_flag():
    return pillar('data:redis:tools:infinite_wait_prestart_flag', True)


def is_aof_enabled():
    appendonly = pillar('data:redis:config:appendonly', 'yes')
    return appendonly == 'yes'


def get_redis_server_stop_statefile():
    return pillar('data:redis:tools:redis_server_stop_statefile', '/var/tmp/redis-server.stop')


def get_redis_slaveof_cmd():
    return pillar('data:redis:secrets:renames:SLAVEOF', 'slaveof')


def get_redis_config_cmd():
    return pillar('data:redis:secrets:renames:CONFIG', 'config')


def get_redis_starting_flag():
    return pillar('data:redis:tools:starting_flag', '/var/tmp/redis-server.starting')


def get_redis_stopping_flag():
    return pillar('data:redis:tools:stopping_flag', '/var/tmp/redis-server.stopping')


def get_sentinel_stopped_flag():
    return pillar('data:sentinel:tools:stopped_flag', '/run/sentinel/wd.stop')


def ensure_str(s, encoding='utf-8', errors='strict'):
    """Coerce *s* to `str`.
    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`
    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    NOTE: The function equals to six.ensure_str that is not present in the salt version of six module.
    """
    if type(s) is str:
        return s
    if six.PY2 and isinstance(s, six.text_type):
        return s.encode(encoding, errors)
    elif six.PY3 and isinstance(s, six.binary_type):
        return s.decode(encoding, errors)
    elif not isinstance(s, (six.text_type, six.binary_type)):
        raise TypeError("not expecting type '%s'" % type(s))
    return s


def get_maxmemory():
    maxmemory = pillar('data:redis:config:maxmemory', None)
    if maxmemory is not None:
        maxmemory = int(maxmemory)
    return maxmemory


def get_maxmemory_policy():
    maxmemory_policy = pillar('data:redis:config:maxmemory-policy', None)
    return maxmemory_policy


def set_maxmemory(maxmemory, policy):
    changed = set_options({'maxmemory': maxmemory, 'maxmemory-policy': policy})
    # failed to change or change restart by deploy (we don't want to wait several times, restart won't help)
    if not changed:
        LOG.error("setting maxmemory failed")
        raise RuntimeError("setting maxmemory failed")
    LOG.debug("setting maxmemory succeeded")


def wait_used_memory_less_or_equal_maxmemory(conn, maxmemory, timeout=1):
    used_memory = 0
    deadline = time.time() + timeout
    while time.time() <= deadline:
        used_memory = int(conn.info("memory")['used_memory'])
        if used_memory <= maxmemory:
            LOG.debug("used memory %d is below maxmemory %d, correct state", used_memory, maxmemory)
            return
        time.sleep(5)

    err_msg = "used memory {} is above maxmemory {} in {} seconds, fault state".format(used_memory, maxmemory, timeout)
    LOG.error(err_msg)
    raise RuntimeError(err_msg)


def append_traceback_to_exception(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as exc:
            raise RuntimeError("{}: {}".format(repr(exc), traceback.format_exc()))

    return wrapper


def get_downscale_check_timeout(conn, old_maxmemory, new_maxmemory, new_policy, timeout):
    if timeout:
        return timeout

    def_timeout = 30
    # https://bb.yandex-team.ru/projects/MDB/repos/redis/browse/src/evict.c#526
    # for noeviction memory cleanup takes constant time, as it's not connected with keys eviction
    if new_policy == 'noeviction':
        return def_timeout

    # https://redis.io/commands/expire/#how-redis-expires-keys
    # 10 times per second test 20 keys -> 200 k/sec
    # seen 205471003 in porto-prod gives 11 days (?) - let's set hard timeout in 30 min
    keys_per_sec = 200  # ?
    max_timeout = 1800

    keys_total = conn.dbsize()
    # we want to evaluate only diff, not all keys
    keys_to_evict = keys_total * (old_maxmemory - new_maxmemory) // old_maxmemory
    eviction_timeout = keys_to_evict // keys_per_sec
    timeout = min(max_timeout, max(30, eviction_timeout))
    LOG.debug("selected downscale timeout %d", timeout)
    return timeout


@append_traceback_to_exception
def check_downscale_available(*args, **kwargs):
    password = get_redis_password()
    conn = _get_connection(password)
    if conn.info("replication")['role'] != 'master':
        return

    def get_maxmemory_values(new_maxmemory):
        new_maxmemory = int(new_maxmemory) if new_maxmemory else get_maxmemory()
        if not new_maxmemory:
            err_msg = 'failed to get new maxmemory'
            LOG.error(err_msg)
            raise RuntimeError(err_msg)

        old_maxmemory = _get_option_from_conn(conn, 'maxmemory')
        if old_maxmemory is None:
            err_msg = 'failed to get current maxmemory'
            LOG.error(err_msg)
            raise RuntimeError(err_msg)

        _, old_maxmemory = old_maxmemory
        old_maxmemory = int(old_maxmemory)
        return old_maxmemory, new_maxmemory

    def get_params(kwargs):
        new_maxmemory = None
        timeout = None
        pillar = kwargs.get('pillar')
        if pillar:
            new_maxmemory = pillar.get('new_maxmemory')
            timeout = pillar.get('timeout')
        old_maxmemory, new_maxmemory = get_maxmemory_values(new_maxmemory)
        new_policy = get_maxmemory_policy()
        timeout = get_downscale_check_timeout(conn, old_maxmemory, new_maxmemory, new_policy, timeout)
        return old_maxmemory, new_maxmemory, new_policy, timeout

    old_maxmemory, new_maxmemory, new_policy, timeout = get_params(kwargs)
    # equality here gives troubles on change restart by deploy - old is already new, restart gives success, no rollback
    if old_maxmemory < new_maxmemory:
        LOG.debug('old maxmemory %d < new maxmemory %d, so no downscale detected', old_maxmemory, new_maxmemory)
        return

    # as we can see here, maxmemory is compared to smth less than used_memory:
    # https://bb.yandex-team.ru/projects/MDB/repos/redis/browse/src/config.c#2283
    # to prevent same calculation as Redis does itself, let's just set maxmemory, wait and
    # then rollback (in worker) if it doesn't work then, as memory could be freed besides keys eviction
    set_maxmemory(new_maxmemory, new_policy)
    wait_used_memory_less_or_equal_maxmemory(conn, new_maxmemory, timeout=timeout)
