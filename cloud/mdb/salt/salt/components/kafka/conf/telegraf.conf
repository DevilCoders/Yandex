{% set environment = salt['pillar.get']('yandex:environment', 'dev') %}
{%- set fqdn = salt.grains.get('id','') -%}
{% set billing_enabled = salt['pillar.get']('data:billing:ship_logs', True) %}
# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  fqdn = "{{ fqdn }}"
  cid = "{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
  dc = "nodc"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "15s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "15s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = false
  ## Log only error level messages.
  #quiet = true

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  logfile = "/var/log/telegraf/telegraf.log"

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  logfile_rotation_interval = "24h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  logfile_rotation_max_size = "10MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  logfile_rotation_max_archives = 7

  ## Override default hostname, if empty use os.Hostname()
  hostname = "{{ fqdn }}"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

[[outputs.mdb_health]]
  mdb_health_agent = "https://{{ salt['pillar.get']('data:mdb_health:host', 'health.db.yandex.net') }}/v1/hostshealth"
  mdbhealth_ca_path = '/opt/yandex/allCAs.pem'
  mdbhealth_cluster_key_file = '/etc/telegraf/cluster_key.pem'
  target_sli_metric_name = "mdb_kafka_health"

[[outputs.http]]
  ## URL is the address to send metrics to
  url = "http://{{ salt['pillar.get']('data:prometheus:address', None) }}"

  ## Data format to output.
  data_format = "prometheusremotewrite"
  namepass = [ 'ch_*','cpu','disk','diskio','mem','net' ]
  name_prefix = 'kafka_'

  [outputs.http.headers]
     Content-Type = "application/x-protobuf"
     Content-Encoding = "snappy"
     X-Prometheus-Remote-Write-Version = "0.1.0"

{% if billing_enabled %}
[[outputs.billing]]
  path = '/var/log/dbaas-billing/billing.log'
  fqdn = "{{ fqdn }}"
  schema = "mdb.db.dc.v1"
{% endif %}

###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################

[[processors.kafka_health]]
  measurement = "kafka_server_ReplicaManager_UnderMinIsrPartitionCount"
  metric_name = "mdb_kafka_health" # how to send to mdb_health
  hostname = "{{ fqdn }}"
  cid = "{{ salt['pillar.get']('data:dbaas:cluster_id') }}"
  order = 1

{% if billing_enabled %}
[[processors.health_to_billing]]
  health_metric_name = "mdb_kafka_health" # metric to get health info
  order = 2
{% endif %}

###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

{% from "components/kafka/conf/prometheus_metrics.jinja" import kafka_prometheus_metrics %}

[[inputs.prometheus]]
  urls = ["http://localhost:7071/metrics"]
  metric_version = 2
  response_timeout = "30s"
  fieldpass = {{ kafka_prometheus_metrics | json }}

[[inputs.cpu]]
  percpu = false
  totalcpu = true
  collect_cpu_time = false
  report_active = false

[[inputs.mem]]

[[inputs.net]]

[[inputs.disk]]
  tagexclude = ["fstype","mode"]
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs", "udev"]

[[inputs.diskio]]
  interval = "1s"
  devices = [ "nvme*" ]

###############################################################################
#                            EVENTS MONITORING                                #
###############################################################################

[[inputs.juggler_runner]]
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = 'sudo -u monitor /usr/local/yandex/monitoring/kafka_cluster_availability.py  2>/dev/null'
    service = "kafka_cluster_availability"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = 'sudo -u monitor /usr/local/yandex/monitoring/kafka_ping.py  2>/dev/null'
    service = "kafka_ping"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = 'sudo -u monitor /usr/local/yandex/monitoring/kafka_ro_partitions.py  2>/dev/null'
    service = "kafka_ro_partitions"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = 'sudo -u monitor /usr/local/yandex/monitoring/kafka_tls.py {{ salt.grains.get('id') }} --pairs {{ salt.pillar.get('data:monrun:tls:pairs', '9091:/etc/kafka/ssl/server.crt') }} -w {{ salt.pillar.get('data:monrun:tls:warn', '30') }} -c {{ salt.pillar.get('data:monrun:tls:crit', '7') }}  2>/dev/null'
    service = "kafka_tls"
