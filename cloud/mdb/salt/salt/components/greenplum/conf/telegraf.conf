{% set environment = salt['pillar.get']('yandex:environment', 'dev') %}
{%- from "components/greenplum/map.jinja" import gpdbvars,pxfvars with context -%}
{%- set fqdn = salt.grains.get('id','') -%}
# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  fqdn = "{{ fqdn }}"
  cid = "{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
  dc = "nodc"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "15s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "15s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = false
  ## Log only error level messages.
  #quiet = true

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  logfile = "/var/log/telegraf/telegraf.log"

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  logfile_rotation_interval = "24h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  logfile_rotation_max_size = "10MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  logfile_rotation_max_archives = 7

  ## Override default hostname, if empty use os.Hostname()
  hostname = "{{ fqdn }}"
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################
{% if salt.dbaas.is_compute() %}
{%   set cfg = salt['pillar.get']('data:solomon_cloud') %}
[[outputs.yandex_monitoring]]
   auth_token_path = "/etc/iam-token-reissuer/iam-token.txt"
   project = "{{ salt['pillar.get']('data:dbaas:cloud:cloud_ext_id', '') }}"
   service = "managed-greenplum"
   cluster = "{{ salt['pillar.get']('data:dbaas:folder:folder_ext_id', '') }}"
   resource_type = "cluster"
   resource_id = "{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
   ca_path = "/opt/yandex/allCAs.pem"
   push_url = "{{ cfg['push_url'] }}"
   namepass = [ 'gp','cpu','disk','diskio','mem','net' ]

[[outputs.yandex_monitoring]]
   auth_token_path = "/etc/iam-token-reissuer/iam-token.txt"
   project = "yandexcloud"
   service = "yandexcloud_dbaas"
   cluster = "mdb_{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
   resource_type = "cluster"
   resource_id = "{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
   ca_path = "/opt/yandex/allCAs.pem"
   push_url = "{{ cfg['push_url'] }}"
   namepass = [ 'gp','cpu','disk','diskio','mem','net' ]
   juggler_name = "yandex_monitoring_internal"
{% else %}
{%   set cfg = salt['pillar.get']('data:solomon') %}
[[outputs.yandex_monitoring]]
   project = "{{ cfg['project'] }}"
   service = "managed-greenplum"
   cluster = "mdb_{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
   resource_type = "cluster"
   resource_id = "{{ salt['pillar.get']('data:dbaas:cluster_id', '') }}"
   push_url = "{{ cfg['push_url'] }}"
   namepass = [ 'gp','cpu','disk','diskio','mem','net' ]
   oauth_token = "{{ cfg['oauth_token'] }}"
   auth_scheme = "OAuth"
{% endif %}

[[outputs.juggler_search]]
    ca_path = '/opt/yandex/allCAs.pem'
    push_url = 'http://juggler-push.search.yandex.net:80/events'
    http_timeout = 10 # seconds
    send_interval = 60 # seconds
    hostname = "{{ salt.dbaas.managed_hostname() }}"

{% if salt.pillar.get('data:dbaas:subcluster_name') == 'master_subcluster' %}
[[outputs.mdb_dns_upper]]
    ca_path = '/opt/yandex/allCAs.pem'
    cluster_key_file = '/etc/telegraf/cluster_key.pem'
    push_url = "https://{{ salt['pillar.get']('data:mdb_dns:host', 'mdb-dns-test.db.yandex.net') }}/v1/dns"

[[outputs.billing]]
{%   if salt['pillar.get']('data:billing:use_cloud_logbroker', False) %}
  path = '/var/log/dbaas-billing/billing-yc.log'
{%   else %}
  path = '/var/log/dbaas-billing/billing.log'
{%   endif %}
  fqdn = "{{ fqdn }}"
{% endif %}

[[outputs.mdb_health]]
    flush_interval = "30s"
    mdb_health_agent = "https://{{ salt['pillar.get']('data:mdb_health:host', 'health.db.yandex.net') }}/v1/hostshealth"
    mdbhealth_ca_path = '/opt/yandex/allCAs.pem'
    mdbhealth_cluster_key_file = '/etc/telegraf/cluster_key.pem'
    target_sli_metric_name = "mdb_greenplum_health"

###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################
[[processors.converter]]
  [processors.converter.fields]
    tag = ["host*","resgr*","array_state"]

[[processors.rename]]
  namepass = [ "diskio" ]
  [[processors.rename.replace]]
    tag = "name"
    dest = "dev"

[[processors.juggler_level]]
  service = "gp_connect_used_percent"
  description_ok = "Current {{ '{{' }}.Value{{ '}}' }}%"
  description_warning = "{{ '{{' }}.Value{{ '}}' }}% >= {{ '{{' }}.WLevel{{ '}}' }}%"
  description_critical = "{{ '{{' }}.Value{{ '}}' }}% >= {{ '{{' }}.CLevel{{ '}}' }}%"
  source = "gp"
  field = "connect_percent"
  level_warning = 70
  level_critical = 85

[[processors.juggler_level]]
  service = "unispace_rootfs"
  source = "disk"
  field = "used_percent"
  level_warning = 90
  level_critical = 97
  description_ok = "Current {{ '{{' }}.Value{{ '}}' }}% path: '{{ '{{' }}.Tags.path{{ '}}' }}' fstype: {{ '{{' }}.Tags.fstype{{ '}}' }} total: {{ '{{' }}.Fields.total{{ '}}' }}"
  description_warning = "{{ '{{' }}.Value{{ '}}' }}% >= {{ '{{' }}.WLevel{{ '}}' }}% path: '{{ '{{' }}.Tags.path{{ '}}' }}' fstype: {{ '{{' }}.Tags.fstype{{ '}}' }} total: {{ '{{' }}.Fields.total{{ '}}' }}"
  description_critical = "{{ '{{' }}.Value{{ '}}' }}% >= {{ '{{' }}.CLevel{{ '}}' }}% path: '{{ '{{' }}.Tags.path{{ '}}' }}' fstype: {{ '{{' }}.Tags.fstype{{ '}}' }} total: {{ '{{' }}.Fields.total{{ '}}' }}"
  [processors.juggler_level.tagpass]
    path = [ "/" ]

{% if salt.pillar.get('data:dbaas:subcluster_name') == 'master_subcluster' %}
[[processors.juggler_mdb_health]]
	measurement = "mdb_greenplum_health"
	input_service = "gp_ping" # juggler service name
	metric = "odyssey" # how to send to mdb_health
	is_master = false
	can_read = true
	can_write = true
	from_tags = false

{% else %}
[[processors.juggler_mdb_health]]
	measurement = "mdb_greenplum_health"
	input_service = "gp_segment_pxf" # juggler service name
	metric = "pxf" # how to send to mdb_health
	is_master = false
	can_read = false
	can_write = false
{% if salt.pillar.get('data:add_settings:use_meta_master_health_service') %}
[[processors.juggler_mdb_health]]
	measurement = "mdb_greenplum_health"
	input_service = "META" # juggler service name
	metric = "meta" # how to send to mdb_health
	is_master = true
	can_read = true
	can_write = true
{% endif %}
{% endif %}

{% if salt.pillar.get('data:dbaas:subcluster_name') == 'master_subcluster' %}
[[processors.juggler_to_monitoring]]
	measurement = "gp"
	input_service = "gp_ping" # juggler service name
	field_name = "ping" # how to send to yandex_monitoring
	ok = 1
	warn = 1
	crit = 0

{% else %}
[[processors.juggler_to_monitoring]]
	measurement = "gp"
	input_service = "gp_segment_pxf" # juggler service name
	field_name = "pxf_is_alive" # how to send to yandex_monitoring
	ok = 1
	warn = 1
	crit = 0

{% endif %}

###############################################################################
#                            AGGREGATOR PLUGINS                               #
###############################################################################


###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

[[inputs.juggler_runner]]
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = '/usr/local/yandex/telegraf/scripts/if_errors.sh -c 100'
    service = "if_errors"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    service = "syslogd"
    name = "systemd_check"
    [inputs.juggler_runner.Checks.args]
        service_name = "rsyslog.service"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    command = '/usr/local/yandex/telegraf/scripts/ntp_stratum.sh'
    service = "ntp_stratum"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 300
    execution_timeout = 30
    command = '/usr/local/yandex/telegraf/scripts/load_average.sh -w 0.9 -c 2.1'
    service = "load_average_relative"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 300
    execution_timeout = 30
{% if salt['pillar.get']('data:dbaas:vtype') == 'compute' %}
    command = '/usr/local/yandex/telegraf/scripts/ext_ip_dns.py'
{% else %}
    name = "meta_check"
{% endif %}
    service = "ext_ip_dns"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 30
    name = "meta_check"
    service = "restarts"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 3570
    execution_interval = 3600
    execution_timeout = 3590
    service = "salt_minion_status"
    name = "systemd_check"
    [inputs.juggler_runner.Checks.args]
        service_name = "salt-minion.service"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 30
    name = "meta_check"
    service = "load_average"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 30
    service = "cron"
    name = "systemd_check"
    [inputs.juggler_runner.Checks.args]
        service_name = "cron.service"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 300
    execution_timeout = 30
    command = '/usr/local/yandex/telegraf/scripts/sumcores.sh'
    service = "sumcores"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 300
    execution_timeout = 30
    command = '/usr/local/yandex/telegraf/scripts/salt_minion_version.py -v {{ salt['pillar.get']('data:salt_version', '3000.9+ds-1+yandex0').split('+')[0] }}'
    service = "salt_minion_version"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 45
    execution_interval = 300
    execution_timeout = 30
    command = '/usr/local/yandex/telegraf/scripts/ro_fs.py'
    service = "ro_fs"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 90
    execution_interval = 3600
    execution_timeout = 300
    command = '/usr/local/yandex/telegraf/scripts/apt_check_updates.py'
    service = "apt_check_updates"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 90
    execution_interval = 3600
    execution_timeout = 300
    command = '/usr/local/yandex/telegraf/scripts/correct_dc.py'
    service = "correct_dc"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 90
    execution_interval = 3600
    execution_timeout = 300
    command = 'nice -n 20 /usr/local/yandex/telegraf/scripts/errata_updates.py -u {{ salt['pillar.get']('data:errata:url', 'https://errata.s3.yandex.net/errata.json.gz') }} -s trapdoor.yandex.net:514/udp 2>/dev/null -w {{ salt['pillar.get']('data:errata:warn_thresh', 14) }} -c {{ salt['pillar.get']('data:errata:crit_thresh', 30) }}'
    service = "errata_updates"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 90
    execution_interval = 1200
    execution_timeout = 100
    command = '/usr/local/yandex/telegraf/scripts/gp_xlog_files.py'
    service = "gp_xlog_files"
  [[inputs.juggler_runner.Checks]]
    random_sleep = 47
    execution_interval = 97
    execution_timeout = 20
    command = 'sudo -u statbox /usr/local/yandex/telegraf/scripts/pushclient.py'
    service = "pushclient"
  [[inputs.juggler_runner.Checks]]
    service = "META"
    random_sleep = 1
    execution_interval = 60
    execution_timeout = 50
    name = "meta_check"
  [[inputs.juggler_runner.Checks]]
    service = "yasmagent" # gag for stopping juggler message MDB-13538
    random_sleep = 1
    execution_interval = 60
    execution_timeout = 50
    name = "meta_check"
  [[inputs.juggler_runner.Checks]]
    service = "linux_raid"
    random_sleep = 1
    execution_interval = 60
    execution_timeout = 50
{% if salt.dbaas.is_compute() %}
    name = "raid_check"
{% else %}
    name = "meta_check"
{% endif %}
  [[inputs.juggler_runner.Checks]]
    service = "solomon_sender"
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    name = "cache_check"
    [inputs.juggler_runner.Checks.args]
        cache_key = "yandex_monitoring"
        timeout = 300000000000
{% if environment == 'compute-prod' or environment == 'compute-preprod' %}
  [[inputs.juggler_runner.Checks]]
    service = "solomon_internal_sender"
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    name = "cache_check"
    [inputs.juggler_runner.Checks.args]
        cache_key = "yandex_monitoring_internal"
        timeout = 300000000000
{% endif %}
{% if salt.pillar.get('data:dbaas:subcluster_name') == 'master_subcluster' %}
  [[inputs.juggler_runner.Checks]]
    service = "mdb_dns_upper_sender"
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    name = "greenplum_cache_check"
    [inputs.juggler_runner.Checks.args]
        cache_key = "mdb_dns_upper"
        timeout = 300000000000
  [[inputs.juggler_runner.Checks]]
    service = "mdb_health_sender"
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    name = "greenplum_cache_check"
    [inputs.juggler_runner.Checks.args]
        cache_key = "mdb_health"
        timeout = 300000000000

  [[inputs.juggler_runner.Checks]]
    service = "gp_ping"
    random_sleep = 5
    execution_interval = 10
    execution_timeout = 50
    name = "greenplum_ping_check"
    [inputs.juggler_runner.Checks.args]
        connection_name = "conn_db"
  [[inputs.juggler_runner.Checks]]
    service = "gp_s3_backup_age"
    random_sleep = 100
    execution_interval = 1200
    execution_timeout = 300
    command = "cd / && /usr/local/yandex/telegraf/scripts/pg_walg_backup_age.py -w 2 -c 4"
  [[inputs.juggler_runner.Checks]]
    service = "replication_lag"
    random_sleep = 5
    execution_interval = 10
    execution_timeout = 50
    name = "greenplum_query_check"
    [inputs.juggler_runner.Checks.args]
        connection_name = "conn_db"
        warn_if_error = true
        query = """
with cte as (
    select max(round((sent_location-replay_location)/1024/1024)::int) AS replication_lag_mb
        from pg_stat_replication
)
select
        case
            when replication_lag_mb > 100 then 2
            when replication_lag_mb > 10 then 1
            else 0
        end status,
        case
            when replication_lag_mb is null then 'NO REPLICATION'
            else 'replication lag: '|| replication_lag_mb || ' MB, warn: 10 MB, crit: 100 MB'
        end descr
    from
        cte;
        """
  [[inputs.juggler_runner.Checks]]
    service = "segments_up"
    random_sleep = 5
    execution_interval = 10
    execution_timeout = 50
    name = "greenplum_query_check"
    [inputs.juggler_runner.Checks.args]
        connection_name = "conn_db"
        warn_if_error = true
        query = """
with cte0 as (
    select distinct hostname
        from gp_segment_configuration
        where status = 'd'
            or (content = -1 and role = 'm' and
              not exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
            )
),
cte as (
    select count(*) d_cnt, string_agg(hostname, ', ') d_hosts
        from cte0
),
roles_check as (
    select hostname from gp_segment_configuration where role != preferred_role
),
roles_cnt as (
    select count(*) r_cnt, string_agg(hostname, ', ') r_hosts
        from roles_check
)
select
        case
            when d_cnt > 0 then 2
            when r_cnt > 0 then 1
            else 0
        end status,
        case
            when d_cnt > 0 then 'fail hosts: '||d_hosts
            when r_cnt > 0 then 'segment unbalanced: '||r_hosts
            else 'OK'
        end descr
    from
        cte, roles_cnt;
        """
  [[inputs.juggler_runner.Checks]]
    service = "xid_wraparound"
    random_sleep = 5
    execution_interval = 10
    execution_timeout = 50
    name = "greenplum_query_check"
    [inputs.juggler_runner.Checks.args]
        connection_name = "conn_db"
        warn_if_error = true
        query = """
with cluster as (
    select gp_segment_id, datname, age(datfrozenxid) age from pg_database where datallowconn
    union all
    select gp_segment_id, datname, age(datfrozenxid) age from gp_dist_random('pg_database')
),
cte as (
    select  round((max(age)::float*100/current_setting('xid_warn_limit')::int)) percent_xid_wraparound
        from cluster
)
select
        case
            when percent_xid_wraparound > 50 then 2
            when percent_xid_wraparound > 45 then 1
            else 0
        end status,
        'percent_xid_wraparound: '|| percent_xid_wraparound  descr
    from
        cte;
        """
  [[inputs.juggler_runner.Checks]]
    service = "superusers_check"
    random_sleep = 5
    execution_interval = 900
    execution_timeout = 50
    name = "greenplum_query_check"
    [inputs.juggler_runner.Checks.args]
        connection_name = "conn_db"
        warn_if_error = true
        query = """
        WITH cte AS (
          SELECT COUNT(*) cnt FROM pg_roles WHERE rolsuper AND rolname not in ('gpadmin','monitor')
        )
        SELECT
          CASE WHEN cnt = 0 THEN 0 ELSE 2 END status,
          CASE WHEN cnt = 0 THEN 'OK' ELSE 'Found unwanted superusers' END descr
        FROM cte;
        """
{% endif %}

  [[inputs.juggler_runner.Checks]]
    service = "gp_segment_pxf"
    random_sleep = 45
    execution_interval = 60
    execution_timeout = 50
    name = "pxf_check"
    [inputs.juggler_runner.Checks.args]
      port = 5888
      version = {{ pxfvars.major_version }}

[[inputs.cpu]]
  percpu = false
  totalcpu = true
  collect_cpu_time = false
  report_active = false

[[inputs.mem]]

[[inputs.net]]

[[inputs.disk]]
  tagexclude = ["fstype","mode"]
  ignore_fs = ["tmpfs", "devtmpfs", "devfs", "iso9660", "overlay", "aufs", "squashfs", "udev"]

[[inputs.diskio]]
  interval = "1s"
  devices = [ {% for disk in salt['grains.get']('disks') %}"{{ disk }}"{%- if not loop.last -%},{% endif %}{% endfor %} ]

{% if salt.pillar.get('data:dbaas:subcluster_name') == 'master_subcluster' %}
{% set monitor_pwd = salt['pillar.get']('data:greenplum:users:monitor:password', 'None') %}
[[inputs.gp_connection]]
  address = "host=localhost user=monitor password={{ monitor_pwd }} dbname=postgres port={{ gpdbvars.master_port }}"
  name = "conn_db"
  max_connections = 3
  max_idle_connections = 2
  recovery_conf_path = "{{ salt['pillar.get']('data:gp_master_directory', '') }}/master/gpseg-1/recovery.conf"

[[inputs.dns_upper]]
  connection_name = "conn_db"
	check_timeout = 10000000000
	db_type = "greenplum"
  service = "mdb_dns_resolve_checker"
  warn_if_error = true

[[inputs.billing]]
	connection_name = "conn_db"
	check_timeout = 10000000000 # 1s = 1000000000
	db_type = "greenplum"
	service = "mdb_billing_resolve_checker" # juggler event
	dbaas_cfg_path = "/etc/dbaas.conf"

[[inputs.mdb_health]]
	metric_name = "mdb_greenplum_health"
	connection_name = "conn_db"
	check_timeout = 10000000000
	db_type = "greenplum"
	fqdn = "{{ fqdn }}"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select round(sum(cpu)) as rg_cpu_used,
        sum(memory_used) as rg_mem_used,
        sum(memory_available) as rg_mem_available,
        rsgname as resgroup
      from gp_toolkit.gp_resgroup_status_per_host
      group by rsgname;
   """
  output_tags = "resgroup"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select sum(wups.size) as spill_files_size_per_seghost,
        sum(coalesce(wups.numfiles, 0)) as spill_files_num_per_seghost,
        sc.hostname as hostname
      from gp_toolkit.gp_workfile_usage_per_segment wups
      inner join gp_segment_configuration sc on sc.content = wups.segid
      group by sc.hostname;
   """
  output_tags = "hostname"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select
        sc.hostname,
        max(case when role = 'p' then 1 else 0 end) is_master
      from
        gp_segment_configuration sc
      where content=-1
      group by sc.hostname;
   """
  output_tags = "hostname"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select
        sc.hostname,
        max(case
              when
                status = 'u' and
                (not (content = -1 and role = 'm') or
                  exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
                )
              then 1 else 0 end) has_alive,
        case when min(case
                        when
                          status = 'u' and
                          (not (content = -1 and role = 'm') or
                            exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
                          )
                        then 1 else 0 end) = 0 then 1 else 0 end has_dead,
        case when max(case
                        when
                          status = 'u' and
                          (not (content = -1 and role = 'm') or
                            exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
                          )
                        then 1 else 0 end) = 0 then 1 else 0 end is_dead,
        case when min(case
                        when
                          status = 'u' and
                          (not (content = -1 and role = 'm') or
                            exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
                          )
                        then 1 else 0 end) = 1 then 1 else 0 end is_alive
      from
        gp_segment_configuration sc
        group by sc.hostname;
   """
  output_tags = "hostname"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select
        sc.hostname,
        sc.content::text content,
        case
          when
            status = 'u' and
            (not (content = -1 and role = 'm') or
              exists (select 1 from pg_stat_replication where state = 'streaming' and usename = 'gpadmin')
            )
          then 1 else 0 end is_alive_segment,
        sc.role
      from
        gp_segment_configuration sc;
   """
  output_tags = "hostname,content,role"
  ignore_fields = "content,role"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select count(*) as conn_count,
        count(*) filter (where state = 'idle') as conn_idle,
        count(*) filter (where state = 'idle in transaction (aborted)') as conn_aborted,
        count(*) filter (where state = 'idle in transaction') as conn_idle_in_transaction,
        count(*) filter (where state = 'active') as conn_active,
        count(*) filter (where state = 'active' and waiting) as conn_waiting
      from pg_stat_activity
      where pid <> pg_backend_pid();
   """

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select count(*) filter (where content != '-1' ) as seg_count,
        count(*) filter (where status = 'd' ) as seg_down,
        count(*) filter (where mode = 'n' and content != '-1' ) as seg_not_synch,
        count(*) filter (where role != preferred_role) as seg_not_pref_role
      from gp_segment_configuration;
   """

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    WITH cluster AS (
        SELECT gp_segment_id, datname, age(datfrozenxid) age FROM pg_database
        UNION ALL
        SELECT gp_segment_id, datname, age(datfrozenxid) age FROM gp_dist_random('pg_database')
      )
      SELECT datname as db_name, round(max(age)/current_setting('xid_warn_limit')::int) AS percent_xid_wraparound
      FROM cluster
      group by datname;
   """
  output_tags = "db_name"
  ignore_fields = "db_name"

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    SELECT count(*) AS master_replication_state
      FROM pg_stat_replication
      WHERE state = 'streaming';
   """

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    SELECT round((sent_location-replay_location)/1024/1024)::int AS replication_lag_MB
      FROM pg_stat_replication;
   """

[[inputs.gp_request]]
	measurement = "gp" # default is gp_request
	connection_name = "conn_db"
	query="""
    select ((select count(*) from pg_stat_activity)*100.0/(select setting from pg_settings where name = 'max_connections')::int) as connect_percent;
   """

{% else %}

{% endif %}
