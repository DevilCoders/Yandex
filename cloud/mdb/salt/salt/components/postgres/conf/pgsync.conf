{% from "components/postgres/pg.jinja" import pg with context %}
[global]
# Effective UID.
daemon_user = postgres

# Full or relative to working_dir path to logfile.
log_file = /var/log/pgsync/pgsync.log

# Possible values are: debug, info, warning, error, critical.
log_level = {{ salt['pillar.get']('data:pgsync:log_level', 'debug') }}

# Pid-file.
pid_file = /var/run/pgsync/pgsync.pid

# Path to start daemon from.
working_dir = /tmp

# String for connecting to local DB.
local_conn_string = {{ salt['pillar.get']('data:pgsync:local_conn_string', 'host=/var/run/postgresql dbname=postgres user=postgres connect_timeout=1 options=\'-c log_statement=none\'') }}

# Extra params to use while connecting to master host.
# Used for calling pg_rewind.
append_rewind_conn_string = {{ salt['pillar.get']('data:pgsync:append_rewind_conn_string', 'port=5432 dbname=postgres user=admin connect_timeout=10' + (' sslmode=verify-full' if salt['pillar.get']('data:pg_ssl', True) else '')) }}

# Used for checking master health through libpq.
append_master_conn_string = {{ salt['pillar.get']('data:pgsync:append_master_conn_string', 'port=6432 dbname=postgres user=monitor connect_timeout=1' + (' sslmode=verify-full' if salt['pillar.get']('data:pg_ssl', True) else '')) }}

# Time to sleep between iterations.
iteration_timeout = {{ salt['pillar.get']('data:pgsync:iteration_timeout', '1') }}

# Comma-separated list of Zookeeper hosts.
zk_hosts = {{ salt['pillar.get']('data:pgsync:zk_hosts', 'zk-df-e2e01f.db.yandex.net:2181,zk-df-e2e01h.db.yandex.net:2181,zk-df-e2e01k.db.yandex.net:2181') }}

{% set zk_lockpath_prefix = salt['pillar.get']('data:pgsync:zk_lockpath_prefix', '/pgsync/' + salt['pillar.get']('data:dbaas:cluster_id', salt['grains.get']('id').split('.')[0][:-1])) + '/' %}
zk_lockpath_prefix = {{ zk_lockpath_prefix }}

# Path to PostgreSQL binaries (pg_rewind, pg_controldata, pg_ctl)
bin_path = {{ pg.bin_path }}

# Commands for managing PostgreSQL/pgbouncer
pg_service_cmd = sudo service {{ pg.service }} %s
pgbouncer_service_cmd = sudo service {{ pg.connection_pooler }} %s

# Path to plugins
plugins_path = {{ salt['pillar.get']('data:pgsync:plugins_path', '/etc/pgsync/plugins') }}

# Relative path to recovery.conf from data directory
{% if pg.version.major_num >= 1200 %}
recovery_conf_rel_path = conf.d/recovery.conf
{% endif %}

# Should we create and drop replication slots during failover/returning
# to cluster
{% if salt['pillar.get']('data:use_replication_slots', True) %}
use_replication_slots = yes
{% else %}
use_replication_slots = no
{% endif %}

# Max number of attempts to return to cluster with pg_rewind
max_rewind_retries = {{ salt['pillar.get']('data:pgsync:max_rewind_retries', '3') }}

# Timeout for running pg_service_cmd start/stop
postgres_timeout = {{ salt['pillar.get']('data:pgsync:postgres_timeout', '600') }}

# Timeout for election
election_timeout = {{ salt['pillar.get']('data:pgsync:election_timeout', 10 * salt['pillar.get']('data:dbaas:cluster_hosts', [1, 2, 3])|length + 10) }}

# Priority of host for becoming master. Replica with the highest priority
# will become synchronous.
{% if salt['pillar.get']('data:pgsync:priority', 100500) != 100500 %}
priority = {{ salt['pillar.get']('data:pgsync:priority') }}
{% else %}
{% set my_dc = salt['grains.get']('ya:short_dc', 'msk') %}
{% if my_dc == 'man' %}
priority = 0
{% elif my_dc == 'sas' or my_dc == 'vla' %}
priority = 5
{% else %}
priority = 10
{% endif %}
{% endif %}

# Should we write priority to ZK on start of pgsync or not?
update_prio_in_zk = {{ salt['pillar.get']('data:pgsync:update_prio_in_zk', 'yes') }}

# Command to generate recovery.conf
# # %m - hostname of master
# # %p - full path to recovery.conf
{% set delay_arg = '' %}
{% if salt['pillar.get']('data:config:recovery_min_apply_delay', False) %}
{% set delay_arg = ' -d ' + salt['pillar.get']('data:config:recovery_min_apply_delay') %}
{% endif %}
{% set slots_arg = '' %}
{% if not salt['pillar.get']('data:use_replication_slots', True) %}
{% set slots_arg = ' -s' %}
{% endif %}
{% set recovery_conf_arg = '' %}
{% if pg.version.major_num >= 1200 %}
{% set recovery_conf_arg = ' -e' %}
{% endif %}
gen_recovery_conf_cmd = /usr/local/yandex/populate_recovery_conf.py {{- delay_arg }} {{- slots_arg }} {{- recovery_conf_arg }} -p %p %m

# Use standalone pgbouncer or not
{% if pg.connection_pooler == 'odyssey' or salt['pillar.get']('data:pgbouncer:count', 1) == 1 %}
standalone_pgbouncer = yes
{% else %}
standalone_pgbouncer = no
{% endif %}
{% if salt['pillar.get']('data:pgsync:replication_source', None) %}
stream_from = {{ salt['pillar.get']('data:pgsync:replication_source', None) }}
{% endif %}

{% if salt['pillar.get']('data:pgsync:autofailover', True) %}
autofailover = yes
{% else %}
autofailover = no
{% endif %}
quorum_commit = {{ salt['pillar.get']('data:pgsync:quorum_commit', 'yes') }}
use_lwaldump = {{ salt['pillar.get']('data:pgsync:use_lwaldump', 'yes') }}

[master]
# Should we change replication type from synchronous to asynchronous and back?
# This is done only in case of holding lock in ZK.
change_replication_type = {{ salt['pillar.get']('data:pgsync:change_replication_type', 'yes') }}

# Should we turn off synchronous replication in maintenance.
# 'no' - turn off synchronous replication (change to async)
# 'yes' - run maintenance with synchronous replication
sync_replication_in_maintenance = {{ salt['pillar.get']('data:pgsync:sync_replication_in_maintenance', 'no') }}

# On what basis should we change replication type (comma-separated list):
# 'count' - replication would become async when all replics are dead, and sync
# when at least one of them is alive.
# 'load' - replication would become async when number of sessions would become
# more than overload_sessions_ratio and vice versa.
# 'time' - changing replication type would occur only in desired hours (could
# be specified only with one of 'count' of 'load').
change_replication_metric = {{ salt['pillar.get']('data:pgsync:change_replication_metric', 'count,load,time') }}

# Amount of sessions (all sessions, not active) by which turn sync replication
# off. In percentage from max_connections.
overload_sessions_ratio = {{ salt['pillar.get']('data:pgsync:overload_sessions_ratio', '75') }}

# Hours when we can turn synchronous replication OFF with
# change_replication_metric=time. From 10 to 22 in weekdays and never
# in weekend days.
weekday_change_hours = {{ salt['pillar.get']('data:pgsync:weekday_change_hours', '10-22') }}
weekend_change_hours = {{ salt['pillar.get']('data:pgsync:weekend_change_hours', '0-0') }}

# Drop this after 315-01d1d0b release
min_replicas = {{ salt['pillar.get']('data:pgsync:min_replicas', '2') }}

# Amount of checks by which old master becomes replica of new master.
remaster_checks = {{ salt['pillar.get']('data:pgsync:master_remaster_checks', '6') }}

[replica]
# Amount of checks by which synchronous replica becomes master.
failover_checks = {{ salt['pillar.get']('data:pgsync:failover_checks', '10') }}

# Timoute to wait master became alive
master_unavailability_timeout = {{ salt['pillar.get']('data:pgsync:master_unavailability_timeout', '15') }}

# Should we start pgbouncer on replica if everything is good with it?
start_pgbouncer = {{ salt['pillar.get']('data:pgsync:start_pgbouncer', 'yes') }}

# Amount of checks by which replica does remastering.
remaster_checks = {{ salt['pillar.get']('data:pgsync:replica_remaster_checks', '10') }}

# Time in seconds during which another failover could not be done after last one.
min_failover_timeout = {{ salt['pillar.get']('data:pgsync:min_failover_timeout', '3600') }}

# Should we do failover in case when last known replication type was async?
allow_potential_data_loss = {{ salt['pillar.get']('data:pgsync:allow_potential_data_loss', 'no') }}

# Timeout for finishing recovery while trying to return to cluster before
# to do a way harded - with the use of pg_rewind.
recovery_timeout = {{ salt['pillar.get']('data:pgsync:recovery_timeout', '1200') }}

# Use reload or restart when perform remaster
{% if pg.version.major_num >= 1300 or pg.version.major_num == 1200 and salt['pillar.get']('yandex:environment', 'dev') in ('dev', 'qa') %}
{%   set default_remaster_restart = 'no' %}
{% else %}
{%   set default_remaster_restart = 'yes' %}
{% endif %}
remaster_restart = {{ salt['pillar.get']('data:pgsync:remaster_restart', default_remaster_restart) }}

[commands]
promote = {{ pg.bin_path }}/pg_ctl promote -D %p
{%
    set rewind_source_server = 'host=%m port=5432 dbname=postgres user=admin connect_timeout=10{ssl}'.format(
            ssl=' sslmode=verify-full' if salt['pillar.get']('data:pg_ssl', True) else '')
%}
{% if pg.version.major_num >= 1200 %}
rewind = {{ pg.bin_path }}/pg_rewind --restore-target-wal --target-pgdata=%p --source-server='{{ rewind_source_server }}'
{% else %}
rewind = {{ pg.bin_path }}/pg_rewind --target-pgdata=%p --source-server='{{ rewind_source_server }}'
{% endif %}
get_control_parameter = {{ pg.bin_path }}/pg_controldata %p | grep '%a:'
# pgsync hung on service postgresql start MDB-5918. We use 3600 timeout because pg_ctl_options = '--timeout=3600'
pg_start = timeout -s SIGTERM 3600 sudo service {{ pg.service }} restart
pg_stop = {{ pg.bin_path }}/pg_ctl stop -s -m fast -w -t %t -D %p
pg_status = sudo service {{ pg.service }} status >/dev/null 2>&1
pg_reload = {{ pg.bin_path }}/pg_ctl reload -s -D %p
bouncer_start = sudo service {{ pg.connection_pooler }} start
bouncer_stop = sudo service {{ pg.connection_pooler }} stop
bouncer_status = sudo service {{ pg.connection_pooler }} status >/dev/null 2>&1
generate_recovery_conf = /usr/local/yandex/populate_recovery_conf.py {{- delay_arg }} {{- slots_arg }} {{- recovery_conf_arg }} -p %p %m
