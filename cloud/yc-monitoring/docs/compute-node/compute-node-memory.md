[Алерт в Juggler](https://juggler.yandex-team.ru/aggregate_checks/?query=service%3Dcompute-node-memory)

## compute-node-memory

Проверяет oversubscription по памяти (системное потребление). Новые инстансы на oversubscribe'нутые NUMA node'ы
приезжать не будут, но вот существующие инстансы потенциально могут OOM'нуться, хотя предсказать вероятность тут
довольно сложно.

## Подробности

Проблемных нод у нас на самом деле очень много. Так много, что руками и не перебрать. При этом у нас сейчас нет никакого
механизма, который бы контролировал равномерное распределение сервисной памяти по NUMA-нодам.

Поэтому было решено свести эту задачу к такой, которую мы сейчас можем решить. Введены два порога:
1. Scheduling threshold - 5 GB per NUMA node
2. Alerting threshold - 0 GB per NUMA node

Инфра прописывает в конфиг Compute Node для каждой NUMA node количество памяти, отведенной под сервисную нагрузку (24GB).
Compute Node мониторит сервисное потребление и:

1. Если на NUMA node сервисное потребление больше чем `infra_limit - scheduling_threshold` — запрещает шедулеру селить
   инстансы на эту NUMA-ноду. Т. е., при текущих настройках, Scheduler перестанет селить инстансы на NUMA-ноду, если на
   ней сервисное потребление превышает `24GB - 5GB = 19GB`.
2. Если на Compute Node суммарное сервисное потребление больше чем `infra_limit - alerting_threshold` — зажигается
   алерт. Т. к. у нас сейчас нет никаких механизмов, которые сдерживали бы разбалансировку сервисной нагрузки по
   NUMA-нодам, мониторить разбалансировку нет смысла — в текущей архитектуре она обязательно будет возникать, поэтому
   тут мониторится именно суммарное потребление. При текущих настройках алерт сработает, если сервисное потребление на
   ноде превысит `(24GB - 0GB) * 2 = 48GB`. Ноль выбран не от хорошей жизни, а чтобы начать хоть с чего-то. Безусловно,
   алертить надо раньше, но прямо сейчас увеличение порога даст нам такое количество нод, которые мы просто не сможем
   переварить — поэтому начинаем с нуля.

В случае срабатывания алерт будет иметь следующий description:
`Service memory is oversubscribed by 2.2 GiB; NUMA node #0 has 5.2 GiB oversubscription; NUMA node #1 has 4.4 GiB oversubscription`
, где первая цифра — это общий oversubscription на основе Alerting threshold, а последующие per-NUMA-нодные — то, что мы
отдаем в шедулер (фактически — память, которую мы потеряли для шедулинга).

## Диагностика

Диагностировать виновника тут зачастую очень сложно: нужно смотреть на сервисное потребление в контексте NUMA-нод, часть
памяти может утечь где-то в ядре ([CLOUD-73034](https://st.yandex-team.ru/CLOUD-73034)). У Инфры есть дашборды по
сервисному потреблению и примерное понимание, сколько кто должен занимать. Скорее всего, она сначала будет смотреть на
этот дашборд в надежде найти явного виновника, но в абсолютном большинстве случаев все будет заканчиваться ребутом.

## Ссылки
- [CLOUD-68034](https://st.yandex-team.ru/CLOUD-68034)
