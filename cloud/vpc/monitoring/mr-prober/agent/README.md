_## Агент

Агент запускает маленькие тесты (_проберы_) по расписанию и отправляет метрики
об успешных и неуспешных запусках в локальный `solomon-agent` или `unified-agent`.

Метрики — главный результат работы агента, но кроме метрик агент пишет логи
всех запусков, а также может отгружать их в Object Storage (S3), если
это указано в конфиге пробера.

Команды, которые нужно запускать, интервал запуска, таймауты и другие параметры читаются 
из S3 при запуске агента и обновляются раз в минуту (см. `settings.UPDATE_AGENT_CONFIG_FROM_S3_INTERVAL_SECONDS`).

Проберу для запуска могут требоваться какие-то файлы (например, bash-скрипты) — они также скачиваются 
из S3 и сохраняются во временную директорию.

## Запуск

Можно запустить агента в virtualenv Мистера Пробера. Для минимальной работы 
нужны переменные окружения с идентификатором кластера и ключами для доступа к S3. Остальные параметры, 
которые можно менять, см. в [settings.py](../settings.py).

```bash
export CLUSTER_ID=1
export S3_ACCESS_KEY_ID=...
export S3_SECRET_ACCESS_KEY=...
PYTHONPATH=. agent/main.py
```

Если в проберах используется параметр `default_routing_interface` (позволяет управлять 
маршрутизацией трафика, выходящего из пробера), то агент должен быть запущен из-под root-а.

Аналогично нужны права root-а, если используется параметр `dns_resolving_interface`, позволяющий
управлять DNS-резолвингом из пробера. Кроме того, этот параметр вообще работает только
изнутри виртуальных машин Yandex Cloud с оверлейными сетевыми интерефейсами.

Про то, как работают эти параметры и почему им нужны права root-а, читайте ниже.

## Локальная сборка докер-образа

Собирать образ надо из корневой папки `mr-prober`.

```bash
docker build . -t cr.yandex/crpni6s1s1aujltb5vv7/agent -f agent/Dockerfile
```

## Внутреннее устройство

При старте агент запускает три потока:
1. `ConfigUpdater` раз в минуту (или раз в `settings.UPDATE_AGENT_CONFIG_FROM_S3_INTERVAL_SECONDS`)
обновляет конфигурацию из S3 — список проберов, файлов для них и конфигов;
2. `AgentTelemetry` отправляет общие агентские метрики;
3. Основной цикл, в котором все проберы запускаются по кругу. Несколько раз в секунду
агент опрашивает статус запущенных проберов, и если время истекло, убивает их, а также
перезапускает, если пришла пора. Гарантируется, что пробер не будет запущен повторно,
пока его предыдущая версия не закончила выполнение.

Кроме того, телеметрия запуска пробера (логи и метрики) пишутся асинхронно, для чего 
создаются отдельные треды 
(см. [common/async_logging.py](../common/async_logging.py) и `AsyncSolomonClient` в [common/telemetry/solomon.py](../common/monitoring/solomon.py)).

## Поддержка параметра `default_routing_interface`

В конфиге пробера можно указать интерфейс, через который надо отправлять весь трафик.

Это работает так: процесс пробера помещается в cgroup. Если параметр `default_routing_interface`
не задан, то используется родительская cgroup, иначе создаётся новая. Для каждого сетевого интерфейса
создаётся максимум одна cgroup, таким образом они переиспользуются между проберами, для которых
указан один и тот же `default_routing_interface`.

В cgroup выполняется единственная настройка — в файл `net_cls.classid` помещается число, 
по которому можно отличить трафик процессов в этой cgroup. Затем создаётся `iptables` (и `ip6tables`)
правило, помечающее трафик от процессов с этим `classid` специальной меткой, которая называется
`fwmark`. Пример такого правила:

```shell
iptables -t mangle -A OUTPUT -m cgroup --cgroup 0xD00D0011 -j MARK --set-mark 11
```

Номер метки уникален для каждой cgroup.

В конце создаётся отдельная таблица маршрутизации (`ip route table`), в которую добавляется
дефолтный маршрут через указанный интерфейс. Трафик направляется в эту таблицу маршрутизации через
специальный `ip rule`, который тригерится на пакеты, помеченные определённым fwmark-ом. Пример команд:

```shell
ip rule add fwmark 11 table mr-prober-eth0
ip route add default dev eth0 table mr-prober-eth0
```

Кроме того, используется хак, чтобы пакеты выходили с правильными Source Address 
(см. [CLOUD-97924](https://st.yandex-team.ru/CLOUD-97924)). Хак добавляет `iptables`-правило, 
делающее SNAT всем отправляемым пакетам. Например,

```shell
ip6tables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 2a02:6b8:c03:501:0:fc5d:37:248
```

Вся вышеописанная логика реализована в классах `RoutingTableManager` и `CgroupManager`.

Подробнее о такой схеме можно прочитать [здесь](https://www.evolware.org/?p=369). 
Смотрите также [CLOUD-90470](https://st.yandex-team.ru/CLOUD-90470) для деталей.

## Поддержка параметра `dns_resolving_interface`

В конфиге пробера можно указать интерфейс, адрес оверлейного DNS-сервера с которого будет
использован при запуске этого пробера.

Это работает так: процесс пробера создаёт себе отдельный 
[mount namespace](https://man7.org/linux/man-pages/man7/mount_namespaces.7.html), в которой
подменяется `/etc/resolv.conf`.
Для этого последовательно вызываются два syscall-а: [unshare](https://man7.org/linux/man-pages/man2/unshare.2.html)
и [mount](https://man7.org/linux/man-pages/man2/mount.2.html).

Эта логика реализована в классе `NamespaceManager`.
Подробнее о такой схеме можно прочитать 
[здесь](https://distracted-it.blogspot.com/2017/03/installer-or-command-that-hangs-use.html)
и в тикете [CLOUD-96961](https://st.yandex-team.ru/CLOUD-96961).

Важно заметить, что всё это работает **только в линуксе**, так как использует линуксовые 
syscall-ы. На других операционных системах этот параметр игнорируется.

### Как агент по имени интерфейса узнаёт, какой адрес прописать в `/etc/resolv.conf`?

Достаточно топорно. Поддерживается только Yandex Cloud, в котором адрес оверлейного DNS-сервера
это [второй адрес подсети](https://cloud.yandex.ru/docs/vpc/concepts/network#subnet).

Сначала агент пытается получить IPv4-адрес сети и вычислить в нём адрес второго хоста, а
если это не получается, то же самое делается с IPv6-адресом сети._