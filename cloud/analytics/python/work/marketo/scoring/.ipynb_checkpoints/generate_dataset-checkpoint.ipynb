{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, datetime, ast, os,sys, pymysql, logging, requests\n",
    "module_path = os.path.abspath(os.path.join('/home/ktereshin/yandex/arcadia/cloud/analytics/python/work'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from data_loader import clickhouse\n",
    "from global_variables import (\n",
    "    metrika_clickhouse_param_dict,\n",
    "    cloud_clickhouse_param_dict\n",
    ")\n",
    "from nile.api.v1 import (\n",
    "    clusters,\n",
    "    aggregators as na,\n",
    "    extractors as ne,\n",
    "    filters as nf,\n",
    "    Record\n",
    ")\n",
    "from vault_client import instances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query, cluster, alias, token, timeout=600):\n",
    "    logger.info(\"Executing query: %s\", query)\n",
    "    proxy = \"http://{}.yt.yandex.net\".format(cluster)\n",
    "    s = requests.Session()\n",
    "    url = \"{proxy}/query?database={alias}&password={token}\".format(proxy=proxy, alias=alias, token=token)\n",
    "    resp = s.post(url, data=query, timeout=timeout)\n",
    "    if resp.status_code != 200:\n",
    "        logger.error(\"Response status: %s\", resp.status_code)\n",
    "        logger.error(\"Response headers: %s\", resp.headers)\n",
    "        logger.error(\"Response content: %s\", resp.content)\n",
    "    resp.raise_for_status()\n",
    "    rows = resp.content.strip().split('\\n')\n",
    "    logger.info(\"Time spent: %s seconds, rows returned: %s\", resp.elapsed.total_seconds(), len(rows))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "client = instances.Production()\n",
    "yt_creds = client.get_version('ver-01d33pgv8pzc7t99s3egm24x47')\n",
    "cluster_yt = clusters.yt.Hahn(\n",
    "    token = yt_creds['value']['token'],\n",
    "    pool = yt_creds['value']['pool'],\n",
    "    \n",
    ").env(\n",
    "\n",
    "    templates=dict(\n",
    "        dates='{2019-03-28..2019-03-31}'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 'hahn'\n",
    "alias = \"*ch_public\"\n",
    "token = '%s' % (yt_creds['value']['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = cluster_yt.read('//home/cloud_analytics/scoring/events').as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events['event'] = events['event'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events = events[events['event'] != '/api/billing/setpaidaccount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_events = events.groupby(['puid'])['event'].agg(lambda x: '\\t'.join(list(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_count_vectors(bag_of_events):\n",
    "    vectorizer = CountVectorizer(token_pattern=u'[^\\t]+')\n",
    "    vectorizer.fit(list(bag_of_events['event']))\n",
    "\n",
    "    count_vec_transform = vectorizer.fit_transform(list(bag_of_events['event']))\n",
    "    return pd.concat(\n",
    "        [\n",
    "            bag_of_events[['puid']],\n",
    "            pd.DataFrame(count_vec_transform.toarray(), columns =vectorizer.get_feature_names())\n",
    "        ],\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "def get_tfidf_vectors(bag_of_events):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=u'[^\\t]+')\n",
    "    vectorizer.fit(list(bag_of_events['event']))\n",
    "\n",
    "    count_vec_transform = vectorizer.fit_transform(list(bag_of_events['event']))\n",
    "    return pd.concat(\n",
    "        [\n",
    "            bag_of_events[['puid']],\n",
    "            pd.DataFrame(count_vec_transform.toarray(), columns =vectorizer.get_feature_names())\n",
    "        ],\n",
    "        axis = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_bag = get_count_vectors(bag_of_events).rename(columns = lambda x: 'count_v_' + str(x) if x != 'puid' else str(x))\n",
    "tfidf_bag = get_tfidf_vectors(bag_of_events).rename(columns = lambda x: 'tfidf_' + str(x) if x != 'puid' else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event2vec(events):\n",
    "    bag_of_events = events.sort_values(by=['puid', 'timestamp']).groupby(['puid'])['event'].agg(list).reset_index()\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(bag_of_events['event']))]\n",
    "    doc2vec_model = Doc2Vec(documents, vector_size=50, window=5, min_count=1, workers=4)\n",
    "    return doc2vec_model, pd.concat(\n",
    "        [\n",
    "            bag_of_events[['puid']],\n",
    "            pd.DataFrame(list(bag_of_events['event'].apply(lambda x:doc2vec_model.infer_vector(x)))).rename(columns = lambda x: 'event2vec_'+str(x))\n",
    "        ],\n",
    "        axis = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model, doc2vec_df = get_event2vec(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.merge(\n",
    "    cluster_yt.read('//home/cloud_analytics/scoring/meta_info').as_dataframe(),\n",
    "    tfidf_bag,\n",
    "    on = 'puid',\n",
    "    how = 'left'\n",
    ").fillna(-100)\n",
    "result = pd.merge(\n",
    "    result,\n",
    "    count_bag,\n",
    "    on = 'puid',\n",
    "    how = 'left'\n",
    ").fillna(-100)\n",
    "result = pd.merge(\n",
    "    result,\n",
    "    doc2vec_df,\n",
    "    on = 'puid',\n",
    "    how = 'left'\n",
    ").fillna(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in result.columns:\n",
    "    if len(col) > 255:\n",
    "        result.drop(col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_yt.write('//home/cloud_analytics/scoring/learning_dataset', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2-venv",
   "language": "python",
   "name": "python2-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
