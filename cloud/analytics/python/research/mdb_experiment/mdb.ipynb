{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy as sp\n",
    "import scipy.stats as sps\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg import eigvals as eig\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from tqdm import tqdm_notebook\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.linalg import eigvals\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict\n",
    "from dateutil.parser import parse\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def execute_query(query, cluster, alias, token, timeout=600):\n",
    "    proxy = \"http://{}.yt.yandex.net\".format(cluster)\n",
    "    s = requests.Session()\n",
    "    url = \"{proxy}/query?database={alias}&password={token}&enable_optimize_predicate_expression=0\".format(proxy=proxy, alias=alias, token=token)\n",
    "    resp = s.post(url, data=query, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    rows = resp.text.strip().split('\\n')\n",
    "    return rows\n",
    "\n",
    "def chyt_execute_query(query, cluster, alias, token, columns):\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            result = execute_query(query=query, cluster=cluster, alias=alias, token=token)\n",
    "            users = pd.DataFrame([row.split('\\t') for row in result], columns = columns)\n",
    "            return users\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            i += 1\n",
    "            if i > 5:\n",
    "                print('Break Excecution')\n",
    "                break\n",
    "\n",
    "cluster = \"hahn\"\n",
    "alias = \"*cloud_analytics\"\n",
    "token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"\"\"\n",
    "SELECT\n",
    "    puid,\n",
    "    event, \n",
    "    url,\n",
    "    referer,\n",
    "    toDateTime(ts) as timestamp\n",
    "FROM \"//home/cloud_analytics/import/console_logs/events\"\n",
    "WHERE \n",
    "    response >= '200' \n",
    "and\n",
    "    response < '300'\n",
    "and \n",
    "    puid != ''\n",
    "AND \n",
    "    timestamp > addDays(today(), -30)\n",
    "ORDER BY ts;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_request = \"\"\"\n",
    "SELECT\n",
    "    puid,\n",
    "    max(billing_account_id) as billing_account_id,\n",
    "    max(user_settings_email) as email,\n",
    "    max(cloud_id) as cloud_id,\n",
    "    max(first_first_trial_consumption_datetime) as trial_started,\n",
    "    max(first_first_paid_consumption_datetime) as paid_started,\n",
    "    sum(if (service_name == 'mdb', trial_consumption + real_consumption, 0)) as mdb_consumption\n",
    "FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "GROUP BY puid\n",
    "HAVING \n",
    "    isNotNull(cloud_id)\n",
    "AND\n",
    "    isNotNull(email)\n",
    "AND\n",
    "    mdb_consumption < 1;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mails = ['%ClickHouse-Step-by-step%',\n",
    "        '%MDB-overview%',\n",
    "        '%MDB-overview-v2%',\n",
    "        '%MDB-upsell-backup%',\n",
    "        '%MDB-value%',\n",
    "        '%Mongo-Step-by-step%',\n",
    "        '%MySQL-Step-by-step%',\n",
    "        '%PostgreSQL-Step-by-step%',\n",
    "        '%Redis-Step-by-step%',\n",
    "        '%MDB-exp%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_mail_req = f\"\"\"\n",
    "SELECT\n",
    "    \"email\",\n",
    "    \"mailing_name\"\n",
    "FROM \"//home/cloud_analytics_test/cubes/emailing/cube\"\n",
    "WHERE {\"mailing_name like '\" + \"' or mailing_name like '\".join(mails) + \"'\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_event_df():\n",
    "    columns = [\"puid\", \"event\", \"url\", \"referer\", \"datetime\"]\n",
    "    df = chyt_execute_query(query=request, cluster=cluster, \n",
    "                       alias=alias, token=token, columns=columns)\n",
    "    df[\"event\"] = df[\"event\"].str.replace(\".com\", \".ru\")\n",
    "    df[\"url\"] = df[\"url\"].str.replace(\".com\", \".ru\")\n",
    "    df[\"referer\"] = df[\"referer\"].str.replace(\".com\", \".ru\")\n",
    "    df[\"puid\"] = df[\"puid\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def make_info_df():\n",
    "    columns = [\"puid\", \n",
    "               \"billing_account_id\", \n",
    "               \"email\", \"cloud_id\",\n",
    "               \"trial_started\",\n",
    "               \"paid_started\",\n",
    "               \"mdb_consumption\"]\n",
    "    df = chyt_execute_query(query=info_request, cluster=cluster, \n",
    "                       alias=alias, token=token, columns=columns)\n",
    "    df[\"puid\"] = df[\"puid\"].astype(str)\n",
    "    df.replace(\"\\\\N\", np.nan, inplace = True)\n",
    "    return df\n",
    "\n",
    "def find_previous_mails_df():\n",
    "    columns = [\"mail\", \n",
    "               \"mail_name\"]\n",
    "    df = chyt_execute_query(query=previous_mail_req, cluster=cluster, \n",
    "                       alias=alias, token=token, columns=columns)\n",
    "    df.replace(\"\\\\N\", np.nan, inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def works_with_emails(mail_):\n",
    "    mail_parts = str(mail_).split('@')\n",
    "    if len(mail_parts) > 1:\n",
    "        if 'yandex.' in mail_parts[1].lower() or 'ya.' in mail_parts[1].lower():\n",
    "            domain = 'yandex.ru'\n",
    "            login = mail_parts[0].lower().replace('.', '-')\n",
    "            return login + '@' + domain\n",
    "        else:\n",
    "            return mail_.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = set([\"postgresql\", \"mysql\", \"clickhouse\", \"mongodb\", \"redis\"])\n",
    "types = [\"docs\", \"prices\", \"services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mails_with_mailing_before():\n",
    "    previous_mails = find_previous_mails_df()\n",
    "    old_emails = previous_mails[\"mail\"].apply(works_with_emails).unique()\n",
    "    return old_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dfs():\n",
    "    old_emails = get_mails_with_mailing_before()\n",
    "    df = make_event_df()\n",
    "    info_df = make_info_df()\n",
    "    return df, info_df, old_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_database_inside_event(event):\n",
    "    ans = re.findall(r\"|\".join(databases), event)\n",
    "    if ans:\n",
    "        return ans[0]\n",
    "    return False\n",
    "\n",
    "def check_type_inside_event(event):\n",
    "    ans = re.findall(r\"|\".join(types), event)\n",
    "    if ans:\n",
    "        return ans[0]\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_previous_mails(curr_df_orig, old_emails):\n",
    "    curr_df = curr_df_orig.copy()\n",
    "    curr_df[\"email\"] = curr_df[\"email\"].apply(works_with_emails)\n",
    "    curr_df = curr_df[~curr_df[\"email\"].isin(old_emails)]\n",
    "    return curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_mdb_sample(old_emails, df, info_df):\n",
    "    events = df[\"event\"].unique()\n",
    "    df_grouped = df.groupby(\"puid\")\n",
    "    puid_results = {}\n",
    "    for puid, df_puid in tqdm(df_grouped):\n",
    "        events = np.array(df_puid[\"event\"])[::-1]\n",
    "        times = np.array(df_puid[\"datetime\"])[::-1]\n",
    "        for event, time in zip(events, times):\n",
    "            if \"create-cluster\" in event:\n",
    "                result_database = check_database_inside_event(event)\n",
    "                if result_database:\n",
    "                    puid_results[puid] = (puid, result_database, time)\n",
    "                    break\n",
    "    first_sample_df = pd.DataFrame.from_dict(puid_results, \\\n",
    "        columns = [\"puid\", \"last_service\", \"dt_last_vis_create-cluster\"], orient = \"index\")\n",
    "    final_first_sample = pd.merge(first_sample_df, info_df, on = \"puid\", how = \"inner\")\n",
    "    first_puids = final_first_sample[\"puid\"].unique()\n",
    "    final_first_sample_df = remove_previous_mails(final_first_sample, old_emails)\n",
    "    return final_first_sample_df, first_puids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_event(dict_of_open_events, event, time):\n",
    "    dict_of_open_events[event] = time\n",
    "    return\n",
    "\n",
    "def close_event_func(dict_of_open_events, event, time, closed_events):\n",
    "    try:\n",
    "        db = check_database_inside_event(event)\n",
    "        closed_events[db] += (time - dict_of_open_events[event]).total_seconds()\n",
    "        print(event)\n",
    "        print(dict_of_open_events[event], time)\n",
    "        del dict_of_open_events[event]\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_previous_mails(curr_df_orig, old_emails):\n",
    "    curr_df = curr_df_orig.copy()\n",
    "    curr_df[\"email\"] = curr_df[\"email\"].apply(works_with_emails)\n",
    "    curr_df = curr_df[~curr_df[\"email\"].isin(old_emails)]\n",
    "    return curr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_mdb_sample(old_emails, df, info_df, first_puids):\n",
    "    events = df[\"event\"].unique()\n",
    "    df_grouped = df.groupby(\"puid\")\n",
    "    puid_results = {}\n",
    "    __SESSION_MAX_TIME = 30 * 60\n",
    "    resulted_df = {}\n",
    "    \n",
    "    for puid, df_puid in tqdm(df_grouped):\n",
    "        curr_open_events = {}\n",
    "        visited_databases = defaultdict(set)\n",
    "        closed_events = Counter()\n",
    "        prev_time = parse(\"2018-01-01\")\n",
    "\n",
    "        for _, row in df_puid.iterrows():\n",
    "            event = row[\"event\"]\n",
    "            close_event = row[\"referer\"]\n",
    "            time = parse(row[\"datetime\"])\n",
    "            if (time - prev_time).total_seconds() > __SESSION_MAX_TIME:\n",
    "                curr_open_events = {}\n",
    "\n",
    "            result_database = check_database_inside_event(event)\n",
    "            typed = check_type_inside_event(event)\n",
    "            close_database = check_database_inside_event(close_event)\n",
    "            close_type = check_type_inside_event(close_event)\n",
    "\n",
    "            if close_database and close_type:\n",
    "                close_event_func(curr_open_events, close_type + close_database, time, closed_events)\n",
    "\n",
    "            if result_database and typed:\n",
    "                visited_databases[result_database].add(typed)\n",
    "                open_event(curr_open_events, typed + result_database, time)\n",
    "\n",
    "        candidate = list()\n",
    "        ans = sorted(visited_databases.items(), key= lambda x: -len(x[1]))\n",
    "        if not ans:\n",
    "            continue\n",
    "\n",
    "        max_len = len(ans[0][1])\n",
    "        for key, val in ans:\n",
    "            if len(visited_databases[key]) > 1 and max_len == len(visited_databases[key]):\n",
    "                candidate.append(key)\n",
    "\n",
    "        if len(candidate) > 0:\n",
    "            resulted_df[puid] = (puid, candidate[0], str(visited_databases[candidate[0]]))\n",
    "            \n",
    "    second_sample_df = pd.DataFrame.from_dict(resulted_df, \\\n",
    "                    columns = [\"puid\", \"last_service\", \"visited_pages\"], orient = \"index\")\n",
    "    final_second_sample = pd.merge(second_sample_df, info_df, on = \"puid\", how = \"inner\")\n",
    "    final_second_sample_without_first_puids = \\\n",
    "    final_second_sample[~final_second_sample[\"puid\"].isin(first_puids)]\n",
    "    final_second_sample_df = remove_previous_mails(final_second_sample_without_first_puids, old_emails)\n",
    "    return final_second_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group(df, number_of_groups = 4):\n",
    "    df = df.sample(frac=1)\n",
    "    sz = df.shape[0]\n",
    "    df[\"group\"] = df.index\n",
    "    df[\"group\"] = df[\"group\"].apply(lambda x: chr(ord('A') + x % number_of_groups))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_samples():\n",
    "    df, info_df, old_emails = get_all_dfs()\n",
    "    print(\"Get info\")\n",
    "    final_first_sample_df, first_puids = get_first_mdb_sample(old_emails, df, info_df)\n",
    "    print(\"Get first sample\")\n",
    "    final_second_sample_df = get_second_mdb_sample(old_emails, df, info_df, first_puids)\n",
    "    print(\"Get second sample\")\n",
    "    final_first_sample_df = add_group(final_first_sample_df, number_of_groups = 4)\n",
    "    final_second_sample_df = add_group(final_second_sample_df, number_of_groups = 4)\n",
    "    final_second_sample_df.to_excel(\"second_sample_experiment_mdb.xlsx\", index = False)\n",
    "    final_first_sample_df.to_excel(\"first_sample_experiment_mdb.xlsx\", index = False)\n",
    "    return final_first_sample_df, final_second_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get info\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d34b7a2c8c344bebbb5781bf3056b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38047), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get first sample\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3c7a5ac444600b90bdf74817fda41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38047), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get second sample\n"
     ]
    }
   ],
   "source": [
    "final_first_sample_df, final_second_sample_df = get_final_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
