{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import xmltodict\n",
    "import pprint\n",
    "import json\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIB\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import pandas as pd\n",
    "from dateutil.parser import *\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "from io import StringIO\n",
    "from calendar import monthrange\n",
    "import yt.wrapper as yt\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import operator\n",
    "import requests, os, sys, pymysql,time\n",
    "from statsmodels.stats import multitest\n",
    "from collections import namedtuple\n",
    "import yt.transfer_manager.client as tm\n",
    "\n",
    "try:\n",
    "    from vault_client.instances import Production as VaultClient\n",
    "    client = VaultClient(decode_files=True)\n",
    "    secret_uuid = \"sec-01e2dyrwvwrmnk1r2q7rnyejcy\"\n",
    "    tokens = client.get_version(secret_uuid)\n",
    "    clickhouse_token = tokens[\"value\"][\"clickhouse_token\"]\n",
    "    metrika_password = tokens[\"value\"][\"metrika\"]\n",
    "    crm_password = tokens[\"value\"][\"crm\"]\n",
    "    startrek_token = tokens[\"value\"][\"startrek\"]\n",
    "    grafana_token = tokens[\"value\"][\"grafana_token\"]\n",
    "    mdb_auth_token = tokens[\"value\"][\"mdb_auth\"]\n",
    "except Exception:\n",
    "    clickhouse_token = sys.argv[1]\n",
    "    metrika_password = sys.argv[2]\n",
    "    crm_password = sys.argv[3]\n",
    "\n",
    "yt.config[\"proxy\"][\"url\"] = \"hahn\"\n",
    "yt.config[\"token\"] = clickhouse_token\n",
    "cluster = \"hahn\"\n",
    "alias = \"*cloud_analytics\"\n",
    "\n",
    "def raw_execute_yt_query(query, timeout=600):\n",
    "    token = clickhouse_token\n",
    "    proxy = \"http://{}.yt.yandex.net\".format(cluster)\n",
    "    s = requests.Session()\n",
    "    url = \"{proxy}/query?database={alias}&password={token}&enable_optimize_predicate_expression=0\".format(proxy=proxy,\n",
    "                                                                                            alias=alias, token=token)\n",
    "    resp = s.post(url, data=query, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    rows = resp.text.strip().split('\\n')\n",
    "    return rows\n",
    "\n",
    "yt.config[\"proxy\"][\"url\"] = \"hahn\"\n",
    "yt.config[\"token\"] = clickhouse_token\n",
    "cluster = \"hahn\"\n",
    "alias = \"*cloud_analytics\"\n",
    "\n",
    "def raw_execute_yt_query(query, timeout=600):\n",
    "    token = clickhouse_token\n",
    "    proxy = \"http://{}.yt.yandex.net\".format(cluster)\n",
    "    s = requests.Session()\n",
    "    url = \"{proxy}/query?database={alias}&password={token}&enable_optimize_predicate_expression=0\".format(proxy=proxy,\n",
    "                                                                                            alias=alias, token=token)\n",
    "    resp = s.post(url, data=query, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    rows = resp.text.strip().split('\\n')\n",
    "    return rows\n",
    "\n",
    "def raw_grafana_execute_query(query):\n",
    "    url = 'https://{host}:8443/?database={db}&query={query}'.format(\n",
    "        host='sas-tt9078df91ipro7e.db.yandex.net',\n",
    "        db='cloud_analytics',\n",
    "        query=query)\n",
    "    auth = {\n",
    "        'X-ClickHouse-User': 'lunindv',\n",
    "        'X-ClickHouse-Key': grafana_token,\n",
    "    }\n",
    "    res = requests.get(\n",
    "        url,\n",
    "        headers=auth,\n",
    "        verify='/usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt')\n",
    "    res.raise_for_status()\n",
    "    rows = res.text.strip().split('\\n')\n",
    "    return rows\n",
    "\n",
    "\n",
    "def raw_chyt_execute_any_query(query, request_func, columns = None):\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            result = request_func(query=query)\n",
    "            if columns is None:\n",
    "                users = pd.DataFrame([row.split('\\t') for row in result[1:]], columns = result[0].split('\\t'))\n",
    "            else:\n",
    "                users = pd.DataFrame([row.split('\\t') for row in result], columns=columns)\n",
    "            return users\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            i += 1\n",
    "            if i > 5:\n",
    "                print('Break Excecution')\n",
    "                break\n",
    "\n",
    "\n",
    "def update_automatically_types(df_raw):\n",
    "    df = df_raw.copy()\n",
    "    for column in df.columns:\n",
    "        if (\"id\" in column and \"paid\" not in column) or \\\n",
    "        (len(df[column].replace(np.nan, \"\").max()) > 9 and\n",
    "                              (\".\" not in df[column].replace(np.nan, \"\").max())):\n",
    "            continue\n",
    "        try:\n",
    "            df[column] = df[column].astype(int)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            df[column] = df[column].astype(float)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            df[column] = df[column].apply(lambda x: json.loads(x))\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            df[column] = df[column].apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def execute_query_body(query, request_func, columns = None):\n",
    "    df = raw_chyt_execute_any_query(query, request_func, columns)\n",
    "    df = df.replace('\\\\N', np.NaN)\n",
    "    df = update_automatically_types(df)\n",
    "    if \"email\" in df.columns:\n",
    "        df[\"email\"] = df[\"email\"].apply(lambda x: works_with_emails(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def execute_query(query, columns = None):\n",
    "    \"\"\"Execute query, returns pandas dataframe with result\n",
    "    :param query: query to execute on cluster\n",
    "    :param columns: name of dataframe columns\n",
    "    :return: pandas dataframe, the result of query\n",
    "    \"\"\"\n",
    "    return execute_query_body(query, raw_execute_yt_query, columns)\n",
    "\n",
    "\n",
    "def works_with_emails(mail_):\n",
    "    \"\"\"mail processing\n",
    "    :param mail_: mail string\n",
    "    :return: processed string\n",
    "    \"\"\"\n",
    "    mail_parts = str(mail_).split('@')\n",
    "    if len(mail_parts) > 1:\n",
    "        if 'yandex.' in mail_parts[1].lower() or 'ya.' in mail_parts[1].lower():\n",
    "            domain = 'yandex.ru'\n",
    "            login = mail_parts[0].lower().replace('.', '-')\n",
    "            return login + '@' + domain\n",
    "        else:\n",
    "            return mail_.lower()\n",
    "\n",
    "\n",
    "def works_with_phones(phone_):\n",
    "    \"\"\"phone processing\n",
    "    :param phone_: phone (string or int)\n",
    "    :return: processed phone as string (with only digits)\n",
    "    \"\"\"\n",
    "    if not pd.isnull(phone_):\n",
    "        phone_ = str(phone_)\n",
    "        return ''.join(c for c in phone_ if c.isdigit())\n",
    "    return phone_\n",
    "\n",
    "        \n",
    "def concatenate_tables(df_array):\n",
    "    \"\"\"Concatenates array of dataframes with possible Nan inside array\n",
    "    :param df_array: array of dataframes\n",
    "    :return: Nane or dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df_array = [df for df in df_array if df is not None]\n",
    "    if len(df_array) == 0:\n",
    "        return None\n",
    "    res_df = pd.concat(df_array, axis = 0)\n",
    "    res_df.index = np.arange(0, res_df.shape[0], 1)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def apply_type(raw_schema, df):\n",
    "    if raw_schema is not None:\n",
    "        for key in raw_schema:\n",
    "            if raw_schema[key] != 'list':\n",
    "                df[key] = df[key].astype(raw_schema[key])\n",
    "    schema = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == int:\n",
    "            schema.append({\"name\":col, 'type':'int64'})\n",
    "        elif df[col].dtype == float:\n",
    "            schema.append({\"name\":col, 'type':'double'})\n",
    "        elif raw_schema is not None and raw_schema.get(col) is not None and raw_schema[col] == 'list':\n",
    "            schema.append({\"name\": col, 'type_v3':\n",
    "                {\"type_name\": 'list', \"item\": {\"type_name\":\"optional\", \"item\":\"string\"}}})\n",
    "        else:\n",
    "            schema.append({\"name\":col, 'type':'string'})\n",
    "    return schema\n",
    "\n",
    "def save_table(file_to_write, path, table, schema = None, append=False):\n",
    "    assert(path[-1] != '/')\n",
    "\n",
    "    df = table.copy()\n",
    "    real_schema = apply_type(schema, df)\n",
    "    json_df_str = df.to_json(orient='records')\n",
    "    path = path + \"/\" + file_to_write\n",
    "    json_df = json.loads(json_df_str)\n",
    "    if not yt.exists(path) or not append:\n",
    "        yt.create(type=\"table\", path=path, force=True,\n",
    "              attributes={\"schema\": real_schema})\n",
    "    tablepath = yt.TablePath(path, append=append)\n",
    "    yt.write_table(tablepath, json_df,\n",
    "               format = yt.JsonFormat(attributes={\"encode_utf8\": False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_type(type_v3):\n",
    "    if type_v3['type_name'] == 'list':\n",
    "        return f\"Array({get_new_type(type_v3['item'])})\"\n",
    "    else:\n",
    "        return type_v3['item'][0].upper() + type_v3['item'][1:]\n",
    "\n",
    "\n",
    "def create_schema_for_grafana(yt_path, table_name, sort_col=None):\n",
    "    schema = yt.get(yt_path + \"/@schema\")\n",
    "\n",
    "    body = f\"CREATE TABLE {table_name} (\\n    \"\n",
    "    time_cols = set()\n",
    "    columns = set()\n",
    "    int_cols = set()\n",
    "    for key in schema:\n",
    "        name = key[\"name\"]\n",
    "        columns.add(name)\n",
    "        curr_type = get_new_type(key[\"type_v3\"])\n",
    "        if curr_type == \"Datetime\":\n",
    "            time_cols.add(name)\n",
    "        if curr_type == \"Int64\":\n",
    "            int_cols.add(name)\n",
    "        if key.get('sort_order') is not None and sort_col is None:\n",
    "            sort_col = name\n",
    "        body = body + name + \" \" + curr_type + \",\\n    \"\n",
    "\n",
    "    body = body[:-6]\n",
    "    body += \"\\n)\\n\"\n",
    "    if len(time_cols) > 0:\n",
    "        col = list(time_cols)[0]\n",
    "    elif len(int_cols) > 0:\n",
    "        col = list(int_cols)[0]\n",
    "    else:\n",
    "        col = list(columns)[0]\n",
    "    if sort_col is None:\n",
    "        sort_col = col\n",
    "    partition_col = sort_col\n",
    "    if sort_col in time_cols:\n",
    "        partition_col = f\"toYYYYMM({sort_col})\"\n",
    "    body += f\"ENGINE = ReplicatedMergeTree('/clickhouse/tables/{{shard}}/{table_name}', '{{replica}}')\\n\" \\\n",
    "            f\"ORDER BY {sort_col}\\n\"\n",
    "    return body\n",
    "\n",
    "\n",
    "def post_grafana_sql(query):\n",
    "    hosts = ['sas-tt9078df91ipro7e.db.yandex.net',\n",
    "             \"vla-2z4ktcci90kq2bu2.db.yandex.net\"]\n",
    "    auth = {\n",
    "        'X-ClickHouse-User': 'lunindv',\n",
    "        'X-ClickHouse-Key': grafana_token,\n",
    "    }\n",
    "    for host in hosts:\n",
    "        url = 'https://{host}:8443/?database={db}&query={query}'.format(\n",
    "            host=host,\n",
    "            db='cloud_analytics',\n",
    "            query=query)\n",
    "        r = requests.post(url=url,\n",
    "                      headers=auth,\n",
    "                      verify=\\\n",
    "                      '/usr/local/share/ca-certificates/Yandex/YandexInternalRootCA.crt')\n",
    "        if r.status_code == 200:\n",
    "            continue\n",
    "        else:\n",
    "            # print(host + r.text)\n",
    "            pass\n",
    "    return\n",
    "\n",
    "\n",
    "def drop_grafana_table(grafana_table_name):\n",
    "    try:\n",
    "        post_grafana_sql('DROP TABLE ' + grafana_table_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def save_table_from_yt_to_grafana(yt_path, grafana_table_name,\n",
    "                                  sort_col=None, schema_table=None):\n",
    "    raw_grafana_table_name = grafana_table_name + \"_\" + str(int(datetime.now().timestamp()))\n",
    "    if schema_table is None:\n",
    "        schema_table=\\\n",
    "        create_schema_for_grafana(yt_path, raw_grafana_table_name, sort_col=sort_col)\n",
    "    post_grafana_sql(schema_table)\n",
    "\n",
    "    params = {\n",
    "        'clickhouse_copy_options': {\n",
    "            'command': 'append',\n",
    "        },\n",
    "        'clickhouse_credentials': {\n",
    "            'password': grafana_token,\n",
    "            'user': 'lunindv',\n",
    "        },\n",
    "        'mdb_auth': {\n",
    "            'oauth_token': mdb_auth_token,\n",
    "        },\n",
    "        'mdb_cluster_address': {\n",
    "            'cluster_id': \"07bc5e8c-c4a7-4c26-b668-5a1503d858b9\",\n",
    "        },\n",
    "        'clickhouse_copy_tool_settings_patch': {\n",
    "            'clickhouse_client': {\n",
    "                'per_shard_quorum': 'all',\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    task=tm.add_task(source_cluster=\"hahn\",\n",
    "            source_table=yt_path,\n",
    "            destination_cluster='mdb-clickhouse',\n",
    "            destination_table=raw_grafana_table_name,\n",
    "            params=params,\n",
    "            sync=False)\n",
    "    task_info = tm.get_task_info(task)\n",
    "    while task_info['state'] in ('pending', 'running'):\n",
    "            time.sleep(5)\n",
    "            task_info = tm.get_task_info(task)\n",
    "\n",
    "    if task_info['state'] != 'completed':\n",
    "            raise Exception(\n",
    "                'Transfer manager task failed with '\n",
    "                'the following state: %s' % task_info['state'])\n",
    "\n",
    "    drop_grafana_table(grafana_table_name)\n",
    "    move_req = f\"\"\"\n",
    "    RENAME TABLE {raw_grafana_table_name}\n",
    "        TO {grafana_table_name} ON CLUSTER \"cloud_analytics\"\n",
    "    \"\"\"\n",
    "    post_grafana_sql(move_req)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_date_as_str():\n",
    "    return str(datetime.date(datetime.now()))\n",
    "\n",
    "def date_to_string(date):\n",
    "    return datetime.strftime(date, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_API = \"http://webservicefarm.interfax.ru/IfaxWebService/ifaxwebservice.asmx\"\n",
    "params = {'Authmethod': \"Login:yandex\"}\n",
    "body = requests.get(SPARK_API, auth=(\"yandex\", \"QZ0nEzj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zeep\n",
    "wsdl = 'http://www.soapclient.com/xml/soapresponder.wsdl'\n",
    "client = zeep.Client(wsdl=wsdl)\n",
    "print(client.service.Method1('Zeep', 'is cool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsdl = \"http://webservicefarm.interfax.ru/IfaxWebService/ifaxwebservice.asmx?WSDL\"\n",
    "client = zeep.Client(wsdl=wsdl)\n",
    "print(client.service.Authmethod(\"yandex\", \"QZ0nEzj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(raw_result):\n",
    "    # Черная json магия, сюда не лезть \n",
    "    req = str(raw_result).encode('utf-8')\n",
    "    jsn_req = json.dumps(xmltodict.parse(req))\n",
    "    return json.loads(jsn_req)[\"Response\"][\"Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_str(phone):\n",
    "    try:\n",
    "        return \"+7(\" + phone[\"@Code\"] + \")\" + phone[\"@Number\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phones(raw_res_GetCompanyShortReport):\n",
    "    raw_res = raw_res_GetCompanyShortReport\n",
    "    answer_list = []\n",
    "    try:\n",
    "        phones = get_dict(raw_res)[\"Report\"][\"PhoneList\"][\"Phone\"]\n",
    "    except Exception:\n",
    "        return answer_list\n",
    "\n",
    "    # Либо лист телефонов, либо словарь с одним телефоном\n",
    "    if isinstance(phones, list):\n",
    "        for phone in phones:\n",
    "            answer_list.append(get_phone_str(phone))\n",
    "    else:\n",
    "        answer_list.append(get_phone_str(phones))\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leader_str(lieder):\n",
    "    try:\n",
    "        return lieder['@FIO'] + \", \" + lieder['@Position']\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return lieder['@FIO']\n",
    "    except Exception:\n",
    "        pass\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leader(raw_res_GetCompanyShortReport):\n",
    "    res = get_dict(raw_res_GetCompanyShortReport)\n",
    "    answer_list = []\n",
    "    try:\n",
    "        lieders = res[\"Report\"][\"LeaderList\"]['Leader']\n",
    "    except Exception:\n",
    "        return answer_list\n",
    "    # Либо лист руководителей, либо словарь с одним руководителем\n",
    "    if isinstance(lieders, list):\n",
    "        for lieder in lieders:\n",
    "            answer_list.append(get_leader_str(lieder))            \n",
    "    else:\n",
    "        answer_list.append(get_leader_str(lieders))\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finance_results(raw_res_CompanyExtendedReport):\n",
    "    raw_res = raw_res_CompanyExtendedReport\n",
    "    finance_results = {}\n",
    "    results = {}\n",
    "    # Пройдемся по всей истории выручки и за каждый год сохраним выручку\n",
    "    try:\n",
    "        for fin_period in get_dict(raw_res)[\"Report\"][\"Finance\"]['FinPeriod']:\n",
    "            period = int(fin_period['@PeriodName'])\n",
    "            mini_fin_results = fin_period['StringList']['String']\n",
    "            for val in mini_fin_results:\n",
    "                if val[\"@Name\"] == 'Выручка':\n",
    "                    finance_results[period] = int(val[\"@Value\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Если нет истории на последний год, то пусть будет доход в 1 рубль\n",
    "    try:\n",
    "        finance_results[last_year]\n",
    "        # Сохраним выручку за последний год\n",
    "        results[f\"Revenue_{last_year}\"] = finance_results[last_year]\n",
    "    except Exception:\n",
    "        finance_results[last_year] = 1\n",
    "        results[f\"Revenue_{last_year}\"] = 0\n",
    "\n",
    "    # Если нет истории на предпоследний последний год, \n",
    "    # то пусть будет доход, как в последний год, \n",
    "    # тогда тип выручки будет не растущим и не падающим\n",
    "    try:\n",
    "        finance_results[last_year - 1]\n",
    "    except Exception:\n",
    "        finance_results[last_year - 1] =  finance_results[last_year]\n",
    "    \n",
    "    # Посмотрим на рост прибыли\n",
    "    results[f\"Percentage_growth_from_{last_year - 1}\"] =\\\n",
    "    (finance_results[last_year] - \n",
    "     finance_results[last_year - 1]) / finance_results[last_year - 1] * 100\n",
    "    \n",
    "    # Тип компании: растущая (рост выручки увеличился на 5 процентов)\n",
    "    # убывающая (выручка упала на 5 процентов), иначе стабильно\n",
    "    results[\"Revenue_type\"] = \"stable\"\n",
    "    if results[f\"Percentage_growth_from_{last_year - 1}\"] > 5:\n",
    "        results[\"Revenue_type\"] = \"growth\"\n",
    "    if results[f\"Percentage_growth_from_{last_year - 1}\"] < -5:\n",
    "        results[\"Revenue_type\"] = \"decline\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_by_one_company(company):\n",
    "    ans_dict = company.copy()\n",
    "    spark_id = ans_dict['SparkID']\n",
    "    raw_res_CompanyShortReport =\\\n",
    "        client.service.GetCompanyShortReport(spark_id, \"\", \"\")['xmlData']\n",
    "    ans_dict[\"Phones_from_spark\"] = get_phones(raw_res_CompanyShortReport)\n",
    "    ans_dict[\"Supervisor\"] = get_leader(raw_res_CompanyShortReport)\n",
    "    \n",
    "    raw_res_CompanyExtendedReport =\\\n",
    "    client.service.GetCompanyExtendedReport(spark_id)['xmlData']\n",
    "    ans_dict = {**ans_dict,\n",
    "                **get_finance_results(raw_res_CompanyExtendedReport)}\n",
    "    return ans_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_by_inn(inn):\n",
    "    try:\n",
    "        spark_id =\\\n",
    "        get_dict(client.service.CheckCompanyStatus('',inn, '')['xmlData'])['SparkID']\n",
    "        raw_res_CompanyByCode =\\\n",
    "        client.service.FindCompanyByCode(inn, \"\", \"0\")['xmlData']\n",
    "        companies = get_dict(raw_res_CompanyByCode)[\"Company\"]\n",
    "    except Exception:\n",
    "        return None\n",
    "    answer_list = []\n",
    "    if isinstance(companies, list):\n",
    "        for company in companies:\n",
    "            if company[\"SparkID\"] == spark_id:\n",
    "                return get_row_by_one_company(company)\n",
    "    else:\n",
    "        return get_row_by_one_company(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_puid_get_inn_table():\n",
    "    req = \"\"\"\n",
    "    SELECT\n",
    "        passport_id as puid,\n",
    "        argMax(inn, dt) as inn,\n",
    "        argMax(name, dt) as name_for_cloud,\n",
    "        argMax(phone, dt) as phone_for_cloud\n",
    "    FROM \"//home/cloud_analytics/import/balance/balance_persons\"\n",
    "    WHERE puid != 0\n",
    "            AND isNotNull(puid)\n",
    "    GROUP BY puid\n",
    "    HAVING isNotNull(inn)\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = execute_query(req)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_year=2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = by_puid_get_inn_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"puid\"] == '1045911341']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inns = []\n",
    "for inn in df[\"inn\"].unique():\n",
    "    inns.append(inn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = []\n",
    "for inn in tqdm(df[\"inn\"].unique()):\n",
    "    try:\n",
    "        new_rows.append(get_row_by_inn(inn))\n",
    "    except Exception:\n",
    "        print(inn)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "for arr in new_rows:\n",
    "    curr_row = []\n",
    "    if arr is not None: \n",
    "        curr_row = arr\n",
    "        all_rows.append(curr_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = pd.merge(spark_df, df, left_on=\"INN\", right_on=\"inn\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.drop(columns=[\"inn\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df[\"phone_for_cloud\"] = spark_df[\"phone_for_cloud\"].apply(lambda x: works_with_phones(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df[spark_df[\"INN\"] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table(\"spark_cube\", \n",
    "           \"//home/cloud_analytics/lunin-dv/spark\", spark_df, schema={\n",
    "           \"Supervisor\":\"list\", \"Phones_from_spark\": 'list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_from_yt_to_grafana(\"//home/cloud_analytics/lunin-dv/spark/spark_cube\", \n",
    "                              \"cloud_analytics.spark_cube\", sort_col='SparkID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_grafana_table(\"spark_cube_1587059554\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
