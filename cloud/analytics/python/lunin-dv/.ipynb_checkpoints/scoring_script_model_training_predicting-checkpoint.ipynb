{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ast\n",
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil.parser import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import typing as tp\n",
    "from sklearn.preprocessing import normalize\n",
    "import my_library as lib\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_TYPE = 'undefined'\n",
    "JSON_EMPTY_VALUE = 'zero_len'\n",
    "CURRENT_DATE = lib.get_current_date_as_str()\n",
    "VALIDATION_TEST_SIZE = 2000\n",
    "TRAIN_SIZE = 20000\n",
    "PAID_THRESHOLD = 0.05\n",
    "CALL_THRESHOLD = 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"//home/cloud_analytics/scoring_v2/data_tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_get_all_table_names():\n",
    "    tables_name_req = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \"//home/cloud_analytics/scoring_v2/data_tables/table_names_for_scoring_model\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    tables_name_df = lib.execute_query(tables_name_req)\n",
    "    return tables_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_type_dict():\n",
    "    column_req = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \"//home/cloud_analytics/scoring_v2/data_tables/type_table\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    column_df = lib.execute_query(column_req)\n",
    "    column_df['column_name'] = column_df[['column_name', 'table_name']].apply(\n",
    "        lambda x: x['table_name'] + \"__\" + x['column_name'], axis=1)\n",
    "\n",
    "    column_df.index = column_df['column_name']\n",
    "    column_dict = column_df.to_dict()['type']\n",
    "    return column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_table(name):\n",
    "    full_path = path + \"/\" + name\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \"{full_path}\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = lib.execute_query(req)\n",
    "    old_columns = df.columns\n",
    "    new_columns = [name +\"__\" + column for column in old_columns]\n",
    "    df.columns = new_columns\n",
    "    df['billing_account_id'] = df[name + \"__\" + 'billing_account_id']\n",
    "    df['scoring_date'] = df[name + \"__\" + 'scoring_date']\n",
    "    if 'is_training_group' in old_columns:\n",
    "        df['is_training_group'] = df[name + \"__\" + 'is_training_group']\n",
    "        df.drop(columns = [name + \"__\" + 'is_training_group'], inplace=True)\n",
    "    df.drop(columns = [name + \"__\" + 'billing_account_id', \n",
    "                       name + \"__\" + 'scoring_date'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_table():\n",
    "    tables_name_df = get_get_all_table_names()\n",
    "    dataframe_dict = {}\n",
    "    for name in tables_name_df['table_names'].tolist():\n",
    "        dataframe_dict[name] = get_one_table(name)\n",
    "\n",
    "    df = dataframe_dict['main_info_table']\n",
    "    for name in dataframe_dict:\n",
    "        if name != 'main_info_table':\n",
    "            df = pd.merge(df, dataframe_dict[name], on=['billing_account_id', \n",
    "                                                    'scoring_date'], how='left')\n",
    "    del dataframe_dict\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_replace(df, column):\n",
    "    df[column].fillna(0, inplace=True)\n",
    "    df[column] = df[column].astype(float)\n",
    "    \n",
    "def category_replace(df, column):\n",
    "    df[column].fillna(EMPTY_TYPE, inplace=True)\n",
    "    df[column] = df[column].astype(str)\n",
    "    \n",
    "def json_replace(df, column):\n",
    "    df[column].fillna('', inplace=True)\n",
    "    df[column] = df[column].apply(lambda x: [] if x == '' else x)\n",
    "    if np.sum(df[column].apply(lambda x: isinstance(x, list))) != df.shape[0]:\n",
    "        df[column] = df[column].apply(lambda x: \n",
    "            x if isinstance(x, list) else \n",
    "            ast.literal_eval(x.replace(\"\\\\\", \"\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_without_nan_table(df_raw, column_dict):\n",
    "    df = df_raw.copy()\n",
    "    dropped_columns = ['billing_account_id', 'scoring_date', \n",
    "                       'is_training_group']\n",
    "    df['is_training_group'].fillna(0, inplace=True)\n",
    "    for column in df.columns:\n",
    "        if column not in dropped_columns:\n",
    "            if 'numeric' in column_dict[column] or 'binary' in column_dict[column]:\n",
    "                number_replace(df, column)\n",
    "            elif 'category' in column_dict[column]:\n",
    "                category_replace(df, column)\n",
    "            elif 'json' in column_dict[column]:\n",
    "                json_replace(df, column)\n",
    "            else:\n",
    "                assert False, f'no type for column {column}'\n",
    "    assert np.sum(np.sum(df.isna())) == 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_changer(df, column):\n",
    "    if 'consumption' in column.split(\"__\")[1]:\n",
    "        min_val = df[column].min()\n",
    "        df[column] = df[column].apply(lambda x: np.log(x + 1 + min_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pct_pattern(array, pattern_value):\n",
    "    cnt = 0\n",
    "    for array_value in array:\n",
    "        if pattern_value == str(array_value):\n",
    "            cnt += 1\n",
    "    if len(array) == 0 and pattern_value == JSON_EMPTY_VALUE:\n",
    "        return 100\n",
    "    return cnt / (len(array) + 1e-5) * 100\n",
    "\n",
    "\n",
    "def is_pattern_inside(array, pattern_value):\n",
    "    for array_value in array:\n",
    "        if pattern_value == str(array_value):\n",
    "            return 1\n",
    "    if len(array) == 0 and pattern_value == JSON_EMPTY_VALUE:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_get_interesting_values(df, json_column, column_dict):\n",
    "    list_of_lists = df[json_column].tolist()\n",
    "    counter = Counter()\n",
    "    for curr_list in list_of_lists:\n",
    "        for value in curr_list:\n",
    "            counter[str(value)] += 1\n",
    "        if len(curr_list) == 0:\n",
    "            counter[JSON_EMPTY_VALUE] += 1\n",
    "    json_num = int(column_dict[json_column].split(\"__\")[1])\n",
    "    final_interesting_values = counter.most_common(json_num)\n",
    "    return final_interesting_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_changer(df, column, column_dict):\n",
    "    interesting_values = json_get_interesting_values(df, column, column_dict)\n",
    "    for value, number_of_finds in interesting_values:\n",
    "        df[column + \"___\" + value] = df[column].apply(lambda x:\n",
    "                                        is_pattern_inside(x, value))\n",
    "        df[column + \"___\" + value] = df[column + \"___\" + value].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_pct_changer(df, column, column_dict):\n",
    "    interesting_values = json_get_interesting_values(df, column, column_dict)\n",
    "    for value, number_of_finds in interesting_values:\n",
    "        df[column + \"___\" + value] = df[column].apply(lambda x:\n",
    "                                        find_pct_pattern(x, value))\n",
    "        df[column + \"___\" + value] = df[column + \"___\" + value].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy(df, cat_column):\n",
    "    dum = pd.get_dummies(df[cat_column])\n",
    "    dum.columns = [cat_column + \"___\"+ str(val) for val in dum.columns]\n",
    "    df = pd.concat([df, dum], axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ml_table_without_category(df, column_dict):\n",
    "    test_df = df.copy()\n",
    "    dropped_columns = ['billing_account_id', 'scoring_date', \n",
    "                       'is_training_group']\n",
    "    for column in test_df.columns:\n",
    "        if column not in dropped_columns:\n",
    "            if 'numeric' in column_dict[column]:\n",
    "                numeric_changer(test_df, column)\n",
    "            if column_dict[column].split(\"__\")[0] == 'json':\n",
    "                json_changer(test_df, column, column_dict)\n",
    "            if column_dict[column].split(\"__\")[0] == 'json_pct':\n",
    "                json_pct_changer(test_df, column, column_dict)\n",
    "            if 'category' in column_dict[column]:\n",
    "                create_dummy(test_df, column)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_full_table()\n",
    "column_dict = get_column_type_dict()\n",
    "df = get_without_nan_table(df, column_dict)\n",
    "ml_df = make_ml_table_without_category(df, column_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = []\n",
    "for column in ml_df.columns:\n",
    "    if 'target_table' in column:\n",
    "        target_columns.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['target_table__paid_target', 'target_table__call_target']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_dataset_for_model_cleaning(df_raw, curr_target_column):\n",
    "    df = df_raw.copy()\n",
    "    y = df[curr_target_column]\n",
    "    df.drop(columns=target_columns, inplace=True)\n",
    "    dropped_columns = ['billing_account_id', 'scoring_date', \n",
    "                       'is_training_group']\n",
    "    df.drop(columns = dropped_columns, inplace=True)\n",
    "    json_columns = []\n",
    "    for column in column_dict:\n",
    "        if 'json' in column_dict[column]:\n",
    "            json_columns.append(column)\n",
    "    df.drop(columns = json_columns, inplace=True)\n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ml_df.shape[0] == ml_df['billing_account_id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(ml_df, scoring_date, curr_target_column):\n",
    "    full_train_df = ml_df[ml_df['is_training_group'] == 1]\n",
    "    full_train_df = full_train_df.sort_values(by='scoring_date')\n",
    "    \n",
    "    to_predict_df = ml_df[ml_df['scoring_date'] == scoring_date]    \n",
    "    validation_train_df = full_train_df\n",
    "    validation_train_df.index = np.arange(0, len(validation_train_df))\n",
    "\n",
    "#     kostya = kostya_df.sort_values(by='billing_account_id')\n",
    "#     validation_test_df = validation_train_df[validation_train_df['billing_account_id'].isin(\n",
    "#         kostya['billing_account_id'])]\n",
    "    \n",
    "    validation_test_df = validation_train_df.tail(VALIDATION_TEST_SIZE)\n",
    "    validation_train_df = validation_train_df[~validation_train_df.index.isin(validation_test_df.index)]\n",
    "    validation_train_df = validation_train_df[((validation_train_df['scoring_date'] >= '2020-04-01') |\n",
    "                                              (full_train_df['scoring_date'] < '2020-01-01')) &\n",
    "                                              (full_train_df['scoring_date'] > '2019-06-01')]\n",
    "    ####################################################################################################\n",
    "    validation_train_df = validation_train_df.tail(TRAIN_SIZE)\n",
    "    validation_train_df, validation_train_y = final_dataset_for_model_cleaning(validation_train_df, \n",
    "                                                                               curr_target_column)\n",
    "    validation_test_df, validation_test_y = final_dataset_for_model_cleaning(validation_test_df,\n",
    "                                                                             curr_target_column)\n",
    "    to_predict_df, _ = final_dataset_for_model_cleaning(to_predict_df,\n",
    "                                                        curr_target_column)\n",
    "    ####################################################################################################\n",
    "    full_train_df_for_model = lib.concatenate_tables([validation_train_df, validation_test_df])\n",
    "    full_train_df_for_model_y = lib.concatenate_tables([validation_train_y, validation_test_y])\n",
    "    \n",
    "    return validation_train_df, validation_train_y,\\\n",
    "           validation_test_df, validation_test_y,\\\n",
    "           full_train_df_for_model, full_train_df_for_model_y,\\\n",
    "           to_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11882430647291942"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_target_column = 'target_table__paid_target'\n",
    "ml_df[(ml_df['main_info_table__is_already_paid'] == ml_df[curr_target_column]) &\n",
    "      (ml_df[curr_target_column] == 1)].shape[0] / \\\n",
    "ml_df[(ml_df[curr_target_column] == 1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_target_column = 'target_table__paid_target'\n",
    "\n",
    "validation_train_df, validation_train_y,\\\n",
    "validation_test_df, validation_test_y,\\\n",
    "full_train_df, full_train_y,\\\n",
    "to_predict_df = prepare_datasets(ml_df, CURRENT_DATE, curr_target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_target_column = 'target_table__paid_target'\n",
    "# kostya_df = lib.execute_query(\"\"\"\n",
    "# SELECT\n",
    "#     DISTINCT billing_account_id,\n",
    "#     addDays(toDate(first_trial_consumption_date), 15) as scoring_date,\n",
    "#     if (score > 0.2, 1, 0) as predicted_by_kostya,\n",
    "#     score as proba_by_kostya\n",
    "# FROM \"//home/cloud_analytics/scoring/leads/leads\"\n",
    "# WHERE scoring_date >= toDate('2020-04-01')\n",
    "# AND scoring_date < toDate('2020-06-01')\n",
    "# FORMAT TabSeparatedWithNames\n",
    "# \"\"\")\n",
    "# for_kostya = ml_df[['billing_account_id', 'scoring_date', curr_target_column, 'is_training_group']]\n",
    "# kostya_df = pd.merge(kostya_df, for_kostya, on=['billing_account_id'])\n",
    "# kostya_df = kostya_df[kostya_df['is_training_group'] == 1]\n",
    "# kostya_pred = kostya_df['predicted_by_kostya']\n",
    "# proba_by_kostya = kostya_df['proba_by_kostya']\n",
    "# y_true = kostya_df[curr_target_column]\n",
    "# recall_score(y_true, kostya_pred), precision_score(y_true, kostya_pred)\n",
    "# ###################################\n",
    "# curr_target_column = 'target_table__paid_target'\n",
    "\n",
    "# validation_train_df, validation_train_y,\\\n",
    "# validation_test_df, validation_test_y,\\\n",
    "# full_train_df, full_train_y,\\\n",
    "# to_predict_df = prepare_datasets(ml_df, CURRENT_DATE, curr_target_column)\n",
    "# ###################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryChanger(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.changer = {}\n",
    "\n",
    "    def work_with_category(self, table_raw, y_train=None, train=False):\n",
    "        table = table_raw.copy()\n",
    "        columns = table.columns\n",
    "        if train:\n",
    "            self.changer = {}\n",
    "            table[\"y\"] = y_train\n",
    "            self.possible_vals = []\n",
    "            self.dummy_changer = None\n",
    "            for col in columns:\n",
    "                if col not in column_dict or 'category' not in column_dict[col]:\n",
    "                    continue\n",
    "                woe_df = table.groupby(col)[\"y\"].mean()\n",
    "                woe_df = pd.DataFrame(woe_df)\n",
    "                woe_df = woe_df.rename(columns={\"y\":\"Good\"})\n",
    "                woe_df[\"Bad\"] = 1 - woe_df[\"Good\"]\n",
    "                woe_df[\"Bad\"] = np.where(woe_df[\"Bad\"] == 0, 1e-5, woe_df[\"Bad\"])\n",
    "                woe_df[\"WoE\"] = np.log(woe_df[\"Good\"] / woe_df[\"Bad\"] + 1e-5) \n",
    "                fe = woe_df[\"WoE\"].to_dict()\n",
    "                self.changer[col] = fe\n",
    "        else:\n",
    "            for col in columns:\n",
    "                if col not in column_dict or 'category' not in column_dict[col]:\n",
    "                    continue\n",
    "                table[col] = table[col].map(self.changer[col])\n",
    "                if table[col].isna().sum() > 0:\n",
    "                    table[col] = table[col].replace(np.nan, \n",
    "                                                    min(self.changer[col].values()))\n",
    "        return table\n",
    "    \n",
    "    def fit(self, X_raw, y):\n",
    "        X = X_raw.copy()\n",
    "        self.work_with_category(X, y, train=True)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        X = X_raw.copy()\n",
    "        res = self.work_with_category(X)\n",
    "        return res #np.matrix(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, prob_threshold, **model_params):\n",
    "        super().__init__()\n",
    "        self.model = model.set_params(**model_params)\n",
    "        self.prob_threshold = prob_threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.model.set_params(**params)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predicts = self.predict_proba(X)\n",
    "        return predicts[:, 1] > self.prob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_treshold_pipeline(pipeline, params, prob_threshold, calibrated=True):\n",
    "    last_model = pipeline.set_params(**params)[-1]\n",
    "    if calibrated:\n",
    "        last_model = CalibratedClassifierCV(last_model, cv=3, method='sigmoid')\n",
    "    all_steps = pipeline.steps.copy()\n",
    "    all_steps[-1] = (\"Threshold\", ThresholdModel(last_model,  prob_threshold=prob_threshold))\n",
    "    return Pipeline(all_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_xgboost_model_with_grid_search(xgboost_pipeline, \n",
    "                                               X, y):\n",
    "    \"\"\"\n",
    "    xgboost_pipeline = Pipeline([('category_changer', CategoryChanger(category_columns)),\n",
    "                                 ('minmaxscaler', MinMaxScaler()),\n",
    "                                 ('XGBoost', XGBClassifier())])\n",
    "    \"\"\"\n",
    "    init_p = np.sum(y) / y.shape[0]\n",
    "    final_params = {\"XGBoost__base_score\": init_p,\n",
    "                    \"XGBoost__seed\": 42, \"XGBoost__n_jobs\": -1, \n",
    "                    'XGBoost__max_depth': 3, \n",
    "                    'XGBoost__n_estimators': 100, \n",
    "                    'XGBoost__random_state': 8}\n",
    "    \n",
    "    params1 = {'XGBoost__max_depth': [3, 4, 5]}\n",
    "    params2 = {'XGBoost__n_estimators': [150, 200, 300]}\n",
    "    params3 = {\"XGBoost__gamma\": [0, 0.5, 1, 1.5]}\n",
    "    params4 = {'XGBoost__subsample': [0.4, 0.7, 1]}\n",
    "    params5 = {\"XGBoost__min_child_weight\": [1, 3, 5]}\n",
    "    params6 = {'XGBoost__subsample': [0.4, 0.7, 1]}\n",
    "    params = [params1, params2, params3, params4, params5, params6]\n",
    "\n",
    "    for param in params:\n",
    "        xgboost_pipeline.set_params(**final_params)\n",
    "        search_model = GridSearchCV(xgboost_pipeline, param, \n",
    "                                    cv = 3, n_jobs = -1, scoring ='recall')\n",
    "        search_model.fit(X, y)\n",
    "        final_params = {**final_params, **search_model.best_params_}\n",
    "    xgboost_pipeline.set_params(**final_params)\n",
    "    return xgboost_pipeline, final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_random_forest_model_with_grid_search(rf_pipeline, \n",
    "                                               X, y):\n",
    "    final_params = {\"random_forest__n_estimators\": 800,\n",
    "                    \"random_forest__n_jobs\": -1, \n",
    "                    'random_forest__random_state': 8}\n",
    "    \n",
    "    params1 = {'random_forest__criterion': ['gini', 'entropy']}\n",
    "    params2 = {'random_forest__max_features': ['auto', 'sqrt', 'log2']}\n",
    "    params3 = {\"random_forest__min_samples_split\": [0.05, 0.1, 0.5, 2, 4]}\n",
    "    params4 = {\"random_forest__min_samples_leaf\": [1, 2, 3]}\n",
    "    params5 = {\"random_forest__min_impurity_decrease\": [0, 10**-4, 10**-2, 10**-1]}\n",
    "    params = [params1, params2, params3, params4, params5]\n",
    "\n",
    "    for param in params:\n",
    "        rf_pipeline.set_params(**final_params)\n",
    "        search_model = GridSearchCV(rf_pipeline, param, \n",
    "                                    cv = 3, n_jobs = -1, scoring ='recall')\n",
    "        search_model.fit(X, y)\n",
    "        final_params = {**final_params, **search_model.best_params_}\n",
    "    rf_pipeline.set_params(**final_params)\n",
    "    return rf_pipeline, final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_catboost_model_with_grid_search(catboost_pipeline, \n",
    "                                                X, y):\n",
    "\n",
    "    final_params = {\n",
    "                    'catboost__iterations': 1000,\n",
    "                    'catboost__depth':1,\n",
    "                    'catboost__learning_rate':0.5,\n",
    "                    'catboost__bootstrap_type': 'Bernoulli',\n",
    "                    'catboost__loss_function':'Logloss',\n",
    "                    'catboost__verbose': False,\n",
    "                    'catboost__random_state': 8,\n",
    "                    'catboost__early_stopping_rounds': 20}\n",
    "    \n",
    "    params1 = {'catboost__bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS']}\n",
    "    params2 = {'catboost__depth': [1, 2, 3, 4]}\n",
    "    params3 = {\"catboost__learning_rate\": [0.01, 0.03, 0.5, 0.8, 1]}\n",
    "    params4 = {'catboost__iterations': [800, 900, 1000]}\n",
    "    params = [params1, params2, params3, params4]\n",
    "\n",
    "    for param in params:\n",
    "        catboost_pipeline.set_params(**final_params)\n",
    "        search_model = GridSearchCV(catboost_pipeline, param, \n",
    "                                    cv = 3, n_jobs = -1, scoring ='recall')\n",
    "        search_model.fit(X, y)\n",
    "        final_params = {**final_params, **search_model.best_params_}\n",
    "    catboost_pipeline.set_params(**final_params)\n",
    "    return catboost_pipeline, final_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results_of_model(name, pipeline, validation_train_df, validation_train_y, cv_num=5):\n",
    "    print(name)\n",
    "    answer_dict = {}\n",
    "    cv = StratifiedKFold(n_splits=cv_num, shuffle=False)\n",
    "    metrics = ['precision', 'recall']\n",
    "    cros_res = cross_validate(pipeline, validation_train_df, validation_train_y, cv=cv,\n",
    "                              scoring=metrics, return_train_score=False)\n",
    "    \n",
    "    answer_dict['precision'] = cros_res['test_precision']\n",
    "    answer_dict['recall'] = cros_res['test_recall']\n",
    "    for key in metrics:\n",
    "        curr_arr = answer_dict[key]\n",
    "        print(f\"{key}: mean={round(np.mean(curr_arr), 2)}; std={round(np.std(curr_arr), 3)}\")\n",
    "        print(curr_arr)\n",
    "        print(\"========================\")\n",
    "    return answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_columns(df_raw, y, max_columns=300):\n",
    "    df = df_raw.copy()\n",
    "    df = CategoryChanger().fit_transform(df, y)\n",
    "    model = CatBoostClassifier(random_state=8, verbose=False)\n",
    "    model.fit(df, y)\n",
    "    \n",
    "    fi = pd.Series(model.feature_importances_,\n",
    "                   index=df.columns)\n",
    "    fi = fi.sort_values(ascending=False)\n",
    "    fi = pd.DataFrame(fi)\n",
    "    fi.columns = ['feature_weight']\n",
    "    interest_columns = fi.iloc[:max_columns].index\n",
    "    \n",
    "    row = {'scoring_date': CURRENT_DATE,\n",
    "           'important_columns_500': str(list(interest_columns)),\n",
    "           'important_columns_100': str(list(interest_columns)[:100])}\n",
    "    columns_df = pd.DataFrame([row])\n",
    "    lib.save_table(\"important_columns\", \n",
    "                   \"//home/cloud_analytics/scoring_v2/helping_folder_for_model\", \n",
    "                   columns_df)\n",
    "    return interest_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cross_validated_results_df(results):\n",
    "    row = {}\n",
    "    row['scoring_date'] = CURRENT_DATE\n",
    "    for key in results:\n",
    "        row[key + \"_array\"] = str(results[key])\n",
    "        mean = np.mean(results[key])\n",
    "        std = np.std(results[key])\n",
    "        row[key + \"_mean\"] = mean\n",
    "        row[key + \"_std\"] = std\n",
    "        left = max(mean - 2 * std, 0)\n",
    "        right = min(mean + 2 * std, 1)\n",
    "        row[key + \"_95_confidence_interval\"] = f\"[{round(left, 3)}, {round(right, 3)}]\"\n",
    "    return pd.DataFrame([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_and_predict_on_dataset(metrics_name, pipeline, params, threshold,\n",
    "                                     train_dataset, train_y, test_dataset, \n",
    "                                     validation=False, test_y=None):\n",
    "    assert metrics_name in ['paid', 'call_answer']\n",
    "    final_pipeline = make_treshold_pipeline(pipeline, params, \n",
    "                                            prob_threshold=threshold,\n",
    "                                            calibrated=True)\n",
    "    trained_pipeline = final_pipeline.fit(train_dataset, train_y)\n",
    "    prediction = trained_pipeline.predict(test_dataset)\n",
    "    prediction_proba = trained_pipeline.predict_proba(test_dataset)\n",
    "    if validation:\n",
    "        assert test_y is not None\n",
    "        recall = recall_score(test_y, prediction)\n",
    "        precision = precision_score(test_y, prediction)\n",
    "        print(f\"validation results: \"\n",
    "              f\"recall={round(recall, 3)}; \"\n",
    "              f\"precision={round(precision, 3)};\")\n",
    "        row = {'scoring_date': CURRENT_DATE,\n",
    "               'test_size': len(test_y),\n",
    "               'precision': precision,\n",
    "               'recall': recall}\n",
    "        test_validation_results_df = pd.DataFrame([row])\n",
    "        lib.save_table(metrics_name + \"_last_users_validation_results\", \n",
    "                   \"//home/cloud_analytics/scoring_v2/helping_folder_for_model\", \n",
    "                   test_validation_results_df)\n",
    "    else:\n",
    "        results = get_cv_results_of_model('cross_validation:', final_pipeline,\n",
    "                                          train_dataset, train_y, cv_num=5)\n",
    "        df_results = make_cross_validated_results_df(results)\n",
    "        lib.save_table(metrics_name + \"_cross_validation_results\", \n",
    "                       \"//home/cloud_analytics/scoring_v2/helping_folder_for_model\", \n",
    "                       df_results)\n",
    "    return trained_pipeline, prediction, prediction_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_target_column = 'target_table__paid_target'\n",
    "\n",
    "validation_train_df, validation_train_y,\\\n",
    "validation_test_df, validation_test_y,\\\n",
    "full_train_df, full_train_y,\\\n",
    "to_predict_df = prepare_datasets(ml_df, CURRENT_DATE, curr_target_column)\n",
    "\n",
    "\n",
    "interest_columns = get_final_columns(full_train_df, full_train_y, max_columns=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_target_column = 'target_table__paid_target'\n",
    "\n",
    "validation_train_df, validation_train_y,\\\n",
    "validation_test_df, validation_test_y,\\\n",
    "full_train_df, full_train_y,\\\n",
    "to_predict_df = prepare_datasets(ml_df, CURRENT_DATE, curr_target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoost_pipeline = Pipeline([('category_changer', CategoryChanger()),\n",
    "                              ('MinMaxScaler', MinMaxScaler()),\n",
    "                              ('feature_selection',\\\n",
    "                               SelectFromModel(CatBoostClassifier(random_state=8, verbose=False),\n",
    "                                               max_features=500)),\n",
    "                              ('catboost', CatBoostClassifier(random_state=8, verbose=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation results: recall=0.926; precision=0.37;\n",
      "time in min: 2.0612407962481183\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "paid_pipeline, _, _ = fit_model_and_predict_on_dataset(\n",
    "                                     'paid', CatBoost_pipeline, {}, PAID_THRESHOLD,\n",
    "                                     validation_train_df, validation_train_y, \n",
    "                                     validation_test_df, \n",
    "                                     validation=True, test_y=validation_test_y)\n",
    "t1 = time.time()\n",
    "print(\"time in min:\", (t1 - t0) / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validation:\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "paid_pipeline, paid_prediction, paid_prediction_proba = fit_model_and_predict_on_dataset(\n",
    "                                     'paid', CatBoost_pipeline, {}, PAID_THRESHOLD,\n",
    "                                     full_train_df, full_train_y, \n",
    "                                     to_predict_df, \n",
    "                                     validation=False)\n",
    "t1 = time.time()\n",
    "print(\"time in min:\", (t1 - t0) / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paid_prediction.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_target_column = 'target_table__call_target'\n",
    "\n",
    "validation_train_df, validation_train_y,\\\n",
    "validation_test_df, validation_test_y,\\\n",
    "full_train_df, full_train_y,\\\n",
    "to_predict_df = prepare_datasets(ml_df, CURRENT_DATE, curr_target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "call_pipeline, _, _ = fit_model_and_predict_on_dataset(\n",
    "                                     'call_answer', CatBoost_pipeline, {}, CALL_THRESHOLD,\n",
    "                                     validation_train_df, validation_train_y, \n",
    "                                     validation_test_df, \n",
    "                                     validation=True, test_y=validation_test_y)\n",
    "t1 = time.time()\n",
    "print(\"time in min:\", (t1 - t0) / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "call_pipeline, call_prediction, call_prediction_proba = fit_model_and_predict_on_dataset(\n",
    "                                     'call_answer', CatBoost_pipeline, {}, CALL_THRESHOLD,\n",
    "                                     full_train_df, full_train_y, \n",
    "                                     to_predict_df, \n",
    "                                     validation=False)\n",
    "t1 = time.time()\n",
    "print(\"time in min:\", (t1 - t0) / 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_prediction.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_save_final_crm_prediction_table(df, call_prediction, paid_prediction,\n",
    "                                             call_prediction_proba, paid_prediction_proba):\n",
    "    to_predict_df = df[df['scoring_date'] == CURRENT_DATE]\n",
    "    to_predict_df['call_prediction'] = call_prediction.astype(int)\n",
    "    to_predict_df['paid_prediction'] = paid_prediction.astype(int)\n",
    "    to_predict_df['call_proba'] = call_prediction_proba[:, 1]\n",
    "    to_predict_df['paid_proba'] = paid_prediction_proba[:, 1]\n",
    "    \n",
    "    final_current_date_table = to_predict_df[['billing_account_id', \n",
    "                                              'scoring_date', \n",
    "                                              'paid_prediction', 'paid_proba',\n",
    "                                              'call_prediction', 'call_proba']]\n",
    "    lib.save_table(f'scored_users',\n",
    "                   '//home/cloud_analytics/scoring_v2/helping_folder_for_model',\n",
    "                   final_current_date_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " make_and_save_final_crm_prediction_table(df, call_prediction, paid_prediction,\n",
    "                                             call_prediction_proba, paid_prediction_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_table = pd.DataFrame([{\"current_predicting_scoring_date\":CURRENT_DATE}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.save_table(f'model_scoring_date',\n",
    "                   '//home/cloud_analytics/scoring_v2/helping_folder_for_model',\n",
    "                   date_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
