{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from dateutil.parser import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import calendar\n",
    "import sys\n",
    "import os.path\n",
    "import time\n",
    "import typing as tp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "from catboost import CatBoostClassifier\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt.wrapper as yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_start_date = '2020-09-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-inf, 0),\n",
       " (3.4965075614664802, 1),\n",
       " (4.204692619390966, 2),\n",
       " (5.808142489980444, 3),\n",
       " (6.907755278982137, 4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treshold_paid = [(0, 0), (33, 1), (67, 2), (333, 3), (1000, 4)]\n",
    "treshold_paid_log = [(np.log(x), ind) for x, ind in treshold_paid]\n",
    "treshold_paid_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_token = sys.argv[1]\n",
    "\n",
    "yt.config[\"proxy\"][\"url\"] = \"hahn\"\n",
    "yt.config[\"token\"] = yt_token\n",
    "cluster = \"hahn\"\n",
    "alias = \"*cloud_analytics\"\n",
    "\n",
    "\n",
    "def _raw_execute_yt_query(query: str, timeout=600) -> tp.List[str]:\n",
    "    '''\n",
    "    Executes chyt query, returns array of strings\n",
    "    :param query: query to execute on cluster\n",
    "    :return: list of strings (rows of table)\n",
    "    '''\n",
    "    token = yt_token\n",
    "    proxy = \"http://{}.yt.yandex.net\".format(cluster)\n",
    "    s = requests.Session()\n",
    "    url = f\"{proxy}/query?database={alias}&password={token}\"\\\n",
    "        \"&enable_optimize_predicate_expression=0\"\n",
    "\n",
    "    resp = s.post(url, data=query, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    rows = resp.text.strip().split('\\n')\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "def _raw_chyt_execute_query(query: str,\n",
    "                            columns: tp.Optional[str] = None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Executes chyt query, returns pd.DataFrame\n",
    "    :param query: query to execute on cluster\n",
    "    :return: pd.DataFrame (final table)\n",
    "    '''\n",
    "    counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            result = _raw_execute_yt_query(query=query)\n",
    "            if columns is None:\n",
    "                df_raw = pd.DataFrame([row.split('\\t')\n",
    "                                       for row in result[1:]],\n",
    "                                      columns=result[0].split('\\t'))\n",
    "            else:\n",
    "                df_raw = pd.DataFrame([row.split('\\t')\n",
    "                                       for row in result], columns=columns)\n",
    "            return df_raw\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            counter += 1\n",
    "            if counter > 5:\n",
    "                print('Break Excecution')\n",
    "                break\n",
    "                \n",
    "                \n",
    "            \n",
    "def _update_automatically_types(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update all string types in table to list, int, float, str.\n",
    "    if 'id' in column name and column type may\n",
    "    be like int - does not update this column.\n",
    "\n",
    "    :param df_raw: raw dataframe which column types need to be updated\n",
    "    :return: pd.DataFrame (updated table)\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    for column in df.columns:\n",
    "        # list updater\n",
    "        try:\n",
    "            df[column] = df[column].apply(lambda x: json.loads(x)\n",
    "                                          if isinstance(json.loads(x), list)\n",
    "                                          else throw_exeption_in_code)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            df[column] = df[column].apply(lambda x:\n",
    "                                          json.loads(x.replace(\"'\", '\"'))\n",
    "                                          if isinstance(\n",
    "                                              json.loads(x.replace(\"'\", '\"')),\n",
    "                                              list)\n",
    "                                          else throw_exeption_in_code\n",
    "                                          )\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        # id checker\n",
    "        try:\n",
    "            if (\"id\" in column and \"paid\" not in column)\\\n",
    "                    and not pd.isnull(df[column].astype(int).sum()):\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        # int updater\n",
    "        try:\n",
    "            df[column] = df[column].astype(int)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # float updater\n",
    "        try:\n",
    "            df[column] = df[column].astype(float)\n",
    "            continue\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def works_with_emails(mail_):\n",
    "    \"\"\"mail processing\n",
    "    :param mail_: mail string\n",
    "    :return: processed string\n",
    "    \"\"\"\n",
    "    mail_parts = str(mail_).split('@')\n",
    "    if len(mail_parts) > 1:\n",
    "        if 'yandex.' in mail_parts[1].lower()\\\n",
    "                or 'ya.' in mail_parts[1].lower():\n",
    "            domain = 'yandex.ru'\n",
    "            login = mail_parts[0].lower().replace('.', '-')\n",
    "            return login + '@' + domain\n",
    "        else:\n",
    "            return mail_\n",
    "        \n",
    "        \n",
    "def execute_query(query: str,\n",
    "                  columns: tp.Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Execute query, returns pandas dataframe as result\n",
    "    :param query: query to execute on cluster\n",
    "    :param columns: name of dataframe columns\n",
    "    :return: pandas dataframe, the result of query\n",
    "    \"\"\"\n",
    "    df = _raw_chyt_execute_query(query, columns)\n",
    "    df = df.replace('\\\\N', np.NaN)\n",
    "    df = _update_automatically_types(df)\n",
    "    if \"email\" in df.columns:\n",
    "        df[\"email\"] = df[\"email\"].apply(lambda x: works_with_emails(x))\n",
    "    return df\n",
    "\n",
    "\n",
    "def time_to_unix(time_str: str) -> int:\n",
    "    \"\"\"\n",
    "    String time to int\n",
    "    \"\"\"\n",
    "    dt = parse(time_str)\n",
    "    timestamp = dt.replace(tzinfo=timezone.utc).timestamp()\n",
    "    return int(timestamp)\n",
    "\n",
    "\n",
    "schema_type = tp.List[tp.Dict[str, tp.Union[str, tp.Dict[str, str]]]]\n",
    "\n",
    "\n",
    "\n",
    "def _apply_type(raw_schema: tp.Optional[schema_type],\n",
    "                df: pd.DataFrame) -> tp.List[schema_type]:\n",
    "    \"\"\"\n",
    "    Create schema for dataset in YT.\n",
    "    Supports datetime, int64, double, list with string and int and float.\n",
    "    \"\"\"\n",
    "    if raw_schema is not None:\n",
    "        for key in raw_schema:\n",
    "            if 'list:' not in raw_schema[key]\\\n",
    "                    and raw_schema[key] != 'datetime':\n",
    "                df[key] = df[key].astype(raw_schema[key])\n",
    "            if raw_schema[key] == 'datetime':\n",
    "                df[key] = df[key].apply(lambda x: time_to_unix(x))\n",
    "\n",
    "    schema: tp.List[tp.Dict[str, tp.Union[str, tp.Dict[str, str]]]] = []\n",
    "    for col in df.columns:\n",
    "        if raw_schema is not None\\\n",
    "                and raw_schema.get(col) is not None\\\n",
    "                and raw_schema[col] == 'datetime':\n",
    "            schema.append({\"name\": col, 'type': 'datetime'})\n",
    "            continue\n",
    "        if df[col].dtype == int:\n",
    "            schema.append({\"name\": col, 'type': 'int64'})\n",
    "        elif df[col].dtype == float:\n",
    "            schema.append({\"name\": col, 'type': 'double'})\n",
    "        elif raw_schema is not None\\\n",
    "                and raw_schema.get(col) is not None\\\n",
    "                and 'list:' in raw_schema[col]:\n",
    "            second_type = raw_schema[col].split(\"list:\")[-1]\n",
    "            schema.append({\"name\": col, 'type_v3':\n",
    "                           {\"type_name\": 'list', \"item\":\n",
    "                            {\"type_name\": \"optional\", \"item\": second_type}}})\n",
    "        else:\n",
    "            schema.append({\"name\": col, 'type': 'string'})\n",
    "    return schema\n",
    "\n",
    "\n",
    "def save_table(file_to_write: str, path: str,\n",
    "               table: pd.DataFrame,\n",
    "               schema: tp.Optional[schema_type] = None,\n",
    "               append: str = False) -> None:\n",
    "    \"\"\"\n",
    "    Save table in HAHN.\n",
    "    :param file_to_write: table name in HAHN\n",
    "    :param path: '//path/to/folder', folder, where to save table\n",
    "    :param schema: schema for table\n",
    "    :param append: append to the end or not.\n",
    "    Creates new table if does not exisists\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    assert(path[-1] != '/')\n",
    "\n",
    "    df = table.copy()\n",
    "    real_schema = _apply_type(schema, df)\n",
    "    json_df_str = df.to_json(orient='records')\n",
    "    path = path + \"/\" + file_to_write\n",
    "    json_df = json.loads(json_df_str)\n",
    "    if not yt.exists(path) or not append:\n",
    "        yt.create(type=\"table\", path=path, force=True,\n",
    "                  attributes={\"schema\": real_schema})\n",
    "    tablepath = yt.TablePath(path, append=append)\n",
    "    yt.write_table(tablepath, json_df,\n",
    "                   format=yt.JsonFormat(attributes={\"encode_utf8\": False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tables_in_hahn_folder(path):\n",
    "    tables = []\n",
    "    for table in yt.search(path, node_type=[\"table\"],\n",
    "                           attributes=[\"account\"]):\n",
    "        tables.append(table)\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_current_date_as_str():\n",
    "    return str(datetime.date(datetime.now()))\n",
    "\n",
    "def date_to_string(date):\n",
    "    return datetime.strftime(date, \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def date_range(end_date, freq_interval=None, start_date=None, freq='D'):\n",
    "    if freq_interval is None:\n",
    "        return pd.date_range(start=start_date, end=end_date, freq=freq).to_pydatetime().tolist()\n",
    "    return pd.date_range(end=end_date, periods=freq_interval, freq=freq).to_pydatetime().tolist()\n",
    "\n",
    "\n",
    "def replace_column_in_df(df_raw, column, where=\"front\"):\n",
    "    df = df_raw.copy()\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(column)\n",
    "    if where == \"front\":\n",
    "        columns = [column] + cols\n",
    "    else:\n",
    "        columns = cols + [column]\n",
    "    return df[columns]\n",
    "\n",
    "\n",
    "def concatenate_tables(df_array):\n",
    "    \"\"\"Concatenates array of dataframes with possible Nan inside array\n",
    "    :param df_array: array of dataframes\n",
    "    :return: Nane or dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df_array = [df for df in df_array if df is not None]\n",
    "    if len(df_array) == 0:\n",
    "        return None\n",
    "    res_df = pd.concat(df_array, axis=0)\n",
    "    res_df.index = np.arange(0, res_df.shape[0], 1)\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(index_to_check, dict_of_groups):\n",
    "    \"\"\"raw function for addABTestingGroup, returns group name depends of index\"\"\"\n",
    "    for key in dict_of_groups:\n",
    "        if index_to_check in dict_of_groups[key]:\n",
    "            return key\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def add_AB_testing_group(df_raw, dict_of_groups):\n",
    "    \"\"\" Add new column with group for AB testing\n",
    "    :param df_raw: raw dataframe, does not change\n",
    "    :param dict_of_groups: dict of groups, for example {\"A\": list_of_df_indexes_in_group_A}\n",
    "    :return: new dataframe with column \"Group\", where groups are written\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    df[\"Group\"] = df.index.map(lambda x: get_group(x, dict_of_groups))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Создание общего индекса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_id_by_cloud_id\n",
    "- get_id_by_billing_id\n",
    "- get_biliing_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"============================================================================================================\"\"\"\n",
    "\n",
    "\n",
    "## META ID\n",
    "\n",
    "class IdGenerator:\n",
    "    def __init__(self, number_of_digits: int = 9):\n",
    "        self.given_id: tp.Set[int] = set()\n",
    "        self.number_of_digits: int = number_of_digits\n",
    "\n",
    "    def __generate_new_id(self) -> int:\n",
    "        if len(self.given_id) == 10 ** (self.number_of_digits) - 10 ** (self.number_of_digits - 1):\n",
    "            raise ValueError(\"Sorry, no free id\")\n",
    "        while True:\n",
    "            current_id = np.random.randint(10 ** (self.number_of_digits - 1), 10 ** (self.number_of_digits))\n",
    "            if current_id not in self.given_id:\n",
    "                self.given_id.add(current_id)\n",
    "                return current_id\n",
    "\n",
    "    __call__ = __generate_new_id\n",
    "\n",
    "\n",
    "class MetaInformationClass:\n",
    "\n",
    "    def __init__(self, interested_columns=[]):\n",
    "        self.dict_cloud: tp.Dict[str, int] = {}  # by cloud_id get id\n",
    "        self.dict_billing: tp.Dict[str, int] = {}  # by billing_id get id\n",
    "        self.dict_id_to_folders: tp.Dict[int, tp.Set[str]] = defaultdict(set)  # by meta_id get floders\n",
    "        self.dict_id_to_billing: tp.Dict[int, tp.List[str]] = defaultdict(list)  # by id get get all billings\n",
    "        self.dict_id_to_cloud: tp.Dict[int, tp.List[str]] = defaultdict(list)  # by id get get all clouds\n",
    "        self.dict_id_to_last_billing: tp.Dict[int, str] = {}  # by id get get last billing by time in cube\n",
    "        self.gen_id = IdGenerator()\n",
    "        self.visited: tp.Set[str] = set()\n",
    "        self.billing_to_new_billing: tp.Dict[str, str] = {}  # matching billing to last billing\n",
    "        self.columns = interested_columns\n",
    "\n",
    "    def __get_for_billing_id(self) -> pd.DataFrame:\n",
    "        part_req = [f\"max(if (event == 'ba_created', {col}, '')) as {col},\" for col in self.columns\n",
    "                    if col != \"company_name\"]\n",
    "        if \"company_name\" in self.columns:\n",
    "            part_req.append(\"\"\"\n",
    "            max(multiIf(account_name IS NOT NULL AND account_name != 'unknown',account_name,\n",
    "                    balance_name IS NOT NULL AND balance_name != 'unknown',balance_name,\n",
    "                    promocode_client_name IS NOT NULL AND promocode_client_name !='unknown',\n",
    "                    promocode_client_name, CONCAT(first_name, ' ', last_name)\n",
    "                    )) as company_name,\n",
    "            \"\"\")\n",
    "        part_req = \"\\n\".join(part_req)\n",
    "        request = f\"\"\"\n",
    "    SELECT\n",
    "        billing_account_id as billing,\n",
    "        groupUniqArray(cloud_id) as array,\n",
    "        {part_req}\n",
    "        max(event_time) as date\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "    WHERE billing_account_id != ''\n",
    "    AND event == 'ba_created'\n",
    "    AND segment != 'VAR'\n",
    "    AND cloud_id != ''\n",
    "    GROUP BY billing_account_id\n",
    "    ORDER BY date DESC\n",
    "    FORMAT TabSeparatedWithNames\n",
    "        \"\"\"\n",
    "        df = execute_query(request)\n",
    "        return df\n",
    "\n",
    "    def __get_for_cloud_id(self) -> pd.DataFrame:\n",
    "        folder_paths = find_tables_in_hahn_folder(\n",
    "            \"//home/cloud_analytics/import/iam/cloud_folders/1h\")\n",
    "        folder_path = max(folder_paths, key=lambda x: yt.row_count(x))\n",
    "        request = f\"\"\"\n",
    "    SELECT\n",
    "        cloud_id as cloud,\n",
    "        groupUniqArray(billing_account_id) as array,\n",
    "        max(folders) as folders\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\" as a\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            groupUniqArray(folder_id) as folders,\n",
    "            cloud_id\n",
    "        FROM \"{folder_path}\"\n",
    "        WHERE cloud_status == 'ACTIVE'\n",
    "        GROUP BY cloud_id\n",
    "    ) as b\n",
    "    ON a.cloud_id == b.cloud_id\n",
    "    WHERE billing_account_id != ''\n",
    "    AND event == 'ba_created'\n",
    "    AND cloud_id != ''\n",
    "    AND segment != 'VAR'\n",
    "    GROUP BY cloud_id\n",
    "    FORMAT TabSeparatedWithNames\n",
    "        \"\"\"\n",
    "        df = execute_query(request)\n",
    "        return df\n",
    "\n",
    "    def __add_info(self, billing_or_cloud: str, current_id: int, where_to_add: str) -> None:\n",
    "        getattr(self, \"dict_id_to_\" + where_to_add)[current_id].append(billing_or_cloud)\n",
    "        getattr(self, \"dict_\" + where_to_add)[billing_or_cloud] = current_id\n",
    "\n",
    "    def __dfs_visit(self, billing: str, current_id: int) -> None:\n",
    "        if billing in self.visited:\n",
    "            return\n",
    "        self.visited.add(billing)\n",
    "        self.__add_info(billing, current_id, \"billing\")\n",
    "        self.billing_to_new_billing[billing] = self.dict_id_to_last_billing[current_id]\n",
    "\n",
    "        for cloud in self.billing_as_vertex_dict[billing]:\n",
    "            self.__add_info(cloud, current_id, \"cloud\")\n",
    "            curr_add = self.cloud_to_folders_dict[cloud]\n",
    "            self.dict_id_to_folders[current_id].update(curr_add)\n",
    "            for chld_billing in self.cloud_as_vertex_dict[cloud]:\n",
    "                self.__dfs_visit(chld_billing, current_id)\n",
    "\n",
    "    def __create_dict(self, where_to_add: str) -> pd.DataFrame:\n",
    "        vertex_df = getattr(self, f\"_MetaInformationClass__get_for_{where_to_add}_id\")()\n",
    "        vertex_df.index = vertex_df[where_to_add]\n",
    "        vertex_df.drop(columns=[where_to_add], inplace=True)\n",
    "        curr_dict = vertex_df.to_dict()[\"array\"]\n",
    "        if where_to_add == 'cloud':\n",
    "            self.cloud_to_folders_dict = vertex_df.to_dict()[\"folders\"]\n",
    "        setattr(self, f\"{where_to_add}_as_vertex_dict\", curr_dict)\n",
    "        return vertex_df\n",
    "\n",
    "    def create_users_id(self):\n",
    "        cloud_as_vertex_df = self.__create_dict(\"cloud\")\n",
    "        billing_as_vertex_df = self.__create_dict(\"billing\")\n",
    "\n",
    "        for col in self.columns:\n",
    "            setattr(self, f\"billing_to_{col}\", {})\n",
    "\n",
    "        for billing, row in billing_as_vertex_df.iterrows():\n",
    "            for col in self.columns:\n",
    "                getattr(self, f\"billing_to_{col}\")[billing] = row[col]\n",
    "            if billing not in self.visited:\n",
    "                current_id = self.gen_id()\n",
    "                self.dict_id_to_last_billing[current_id] = billing\n",
    "                self.__dfs_visit(billing, current_id)\n",
    "\n",
    "    def get_id_by_cloud_id(self, cloud_id: str) -> str:\n",
    "        return str(self.dict_cloud[cloud_id])\n",
    "\n",
    "    def get_id_by_billing_id(self, billing_id: str) -> str:\n",
    "        return str(self.dict_billing[billing_id])\n",
    "\n",
    "    def get_billing_by_id(self, meta_id: str) -> str:\n",
    "        return self.dict_id_to_last_billing[int(meta_id)]\n",
    "\n",
    "    def get_all_accosiated_billings(self, billing_id: str) -> tp.List[str]:\n",
    "        try:\n",
    "            meta_id = self.dict_billing[billing_id]\n",
    "            return self.dict_id_to_billing[meta_id]\n",
    "        except Exception:\n",
    "            return ['billing_id']\n",
    "\n",
    "    def get_info_from_column(self, billing_id: str, column: str):\n",
    "        assert column in self.columns\n",
    "        meta_id = self.dict_billing[billing_id]\n",
    "        billings = self.dict_id_to_billing[meta_id]\n",
    "        answer_list = []\n",
    "        for billing in billings:\n",
    "            answer_list.append(getattr(self, f\"billing_to_{column}\")[billing])\n",
    "        answer_list = set(answer_list)\n",
    "        bad_words = [\"unknown\", \"-\", \"\"]\n",
    "        for word in bad_words:\n",
    "            if word in answer_list:\n",
    "                answer_list.remove(word)\n",
    "        return list(answer_list)\n",
    "\n",
    "    def get_dataframe_with_grouped_information(self):\n",
    "        result_dict = defaultdict(list)\n",
    "        for billing in self.dict_billing:\n",
    "            meta_id = self.dict_billing[billing]\n",
    "            result_dict[\"billing_account_id\"].append(billing)\n",
    "            result_dict[\"last_active_billing\"].append(self.billing_to_new_billing[billing])\n",
    "            result_dict[\"associated_billings\"].append(\n",
    "                list(set(self.dict_id_to_billing[meta_id])))\n",
    "            result_dict[\"associated_clouds\"].append(list(set(\n",
    "                self.dict_id_to_cloud[meta_id])))\n",
    "            result_dict[\"associated_folders\"].append(list(self.dict_id_to_folders[meta_id]))\n",
    "            for column in self.columns:\n",
    "                result_dict[column].append(self.get_info_from_column(billing, column))\n",
    "        result_df = pd.DataFrame.from_dict(result_dict)\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = MetaInformationClass(interested_columns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.create_users_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = user.get_dataframe_with_grouped_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df[['billing_account_id', 'last_active_billing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.index = res_df['billing_account_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "builling_to_last_active_billing = res_df['last_active_billing'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Создание таблиц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забыть:\n",
    "\n",
    "1) Создать запросы, зависящие от времени\n",
    "\n",
    "2) самому прописать все типы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 VM_CUBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vm_request(date: str) -> str:\n",
    "    prod_part_req = \"\"\n",
    "    for prod in cube_products:\n",
    "        prod_part_req += \"max(if(vm_product_name == '\" + prod + \"'\" +', 1, 0)) as \"' + prod + '\",\\n'\n",
    "        vm_cube_request = f\"\"\"\n",
    "SELECT\n",
    "    cloud_id,\n",
    "    avg(vm_cores) as num_of_cores_at_vm,\n",
    "    length(groupUniqArray(vm_id)) as num_of_vm,\n",
    "    count(DISTINCT node_az) as az_num,\n",
    "    avg(vm_cores_real) as cores_real,\n",
    "    avg(vm_memory_real) as vm_memory_real,\n",
    "    avg(vm_memory_to_cores_ratio) as vm_memory_to_cores_ratio,\n",
    "    avg(vm_preemptible) as preemptible,\n",
    "    max(vm_age) as  vm_age,\n",
    "    avg(vm_core_fraction) as core_fraction,\n",
    "    {prod_part_req}\n",
    "    max(if(vm_is_service == 'service', 1, 0)) as service,\n",
    "    max(if(vm_is_service == 'customer', 1, 0)) as customer\n",
    "FROM \"//home/cloud_analytics/compute_logs/vm_cube/vm_cube\"\n",
    "WHERE toDate(vm_finish) < '{date}'\n",
    "AND toDate(vm_finish) >= addDays(toDate('{date}'), -7)\n",
    "GROUP BY cloud_id\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "    return vm_cube_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_fraction_type(core: float) -> str:\n",
    "    if core < 25:\n",
    "        return \"small\"\n",
    "    if core < 100:\n",
    "        return \"medium\"\n",
    "    return \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vm_cube_info_df(date: str) -> pd.DataFrame:\n",
    "    vm_cube_request = create_vm_request(date)\n",
    "    vm_cube_df = execute_query(vm_cube_request)\n",
    "    vm_cube_df[\"core_fraction\"] = vm_cube_df[\"core_fraction\"].apply(lambda x: core_fraction_type(x))\n",
    "    return vm_cube_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Managed Databes ON VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mdb_on_vm_list(date: str) -> tp.Set[str]:\n",
    "    request = f\"\"\"\n",
    "SELECT\n",
    "billing_account_id\n",
    "FROM \"//home/cloud_analytics/import/network-logs/db-on-vm/data\"\n",
    "WHERE toDate(date) < '{date}'\n",
    "AND toDate(date) >= addDays(toDate('{date}'), -14)\n",
    "GROUP BY billing_account_id\n",
    "FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = execute_query(request)\n",
    "    return set(df[\"billing_account_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Managed Databes types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 artificial intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4  Paid consumption information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tan(target: tp.List[float]) -> float:\n",
    "    target = normalize(np.array(target)[:,np.newaxis], axis=0).ravel()\n",
    "    x = np.arange(0, target.shape[0])\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), np.array(target))\n",
    "    return model.coef_[0]\n",
    "\n",
    "def find_angle(tan1: float, tan2: float) -> float:\n",
    "    return -math.degrees(math.asin((tan1 - tan2) / math.sqrt((tan2 * tan2 + 1) * (tan1 * tan1 + 1))))\n",
    "\n",
    "def log_apply(val: float) -> float:\n",
    "    return np.log(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paid_req(date: str, days: int) -> str:\n",
    "    req = f\"\"\"\n",
    "SELECT\n",
    "    billing_account_id,\n",
    "    groupArray(paid) as paid_arr_{days}\n",
    "FROM (\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        date,\n",
    "        sum(if(toDate(date) == toDate(event_time), real_consumption, 0)) as paid\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\" as a,\n",
    "    (\n",
    "        SELECT\n",
    "            dt as date\n",
    "        FROM (\n",
    "            SELECT arrayMap(x -> addDays(toDate('{date}'), -x-1), range({days})) as dt\n",
    "        )\n",
    "        ARRAY JOIN dt\n",
    "    ) as b\n",
    "    WHERE sku_lazy == 0\n",
    "    GROUP BY billing_account_id, date\n",
    "    ORDER BY billing_account_id, date\n",
    ")\n",
    "GROUP BY billing_account_id\n",
    "HAVING arraySum(paid_arr_{days}) > 0\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "    return req\n",
    "\n",
    "\n",
    "def create_paid_df(date: str) -> pd.DataFrame:\n",
    "    df_30 = execute_query(create_paid_req(date, 30))\n",
    "    df_7 = execute_query(create_paid_req(date, 7))\n",
    "    paid_df = pd.merge(df_7, df_30, on = \"billing_account_id\", how =\"inner\")\n",
    "    for col in paid_df.columns:\n",
    "        if \"paid_arr\" in col:\n",
    "            paid_df[\"paid_coeff_\" + col.split(\"_\")[-1]] = paid_df[col].apply(lambda x: predict_tan(x))\n",
    "    paid_df[\"delta\"] = paid_df[[\"paid_coeff_7\", \"paid_coeff_30\"]].apply(lambda row: \n",
    "                                                                        find_angle(row[\"paid_coeff_30\"],\n",
    "                                                                                   row[\"paid_coeff_7\"]), axis = 1)\n",
    "    paid_df[\"cons_avg_last_week\"] = paid_df[\"paid_arr_7\"].apply(lambda val: np.mean(val))\n",
    "    paid_df[\"cons_std_last_week\"] = paid_df[\"paid_arr_7\"].apply(lambda val: np.std(val))\n",
    "    paid_df[\"consumer_plateau\"] = (paid_df[\"cons_std_last_week\"] / paid_df[\"cons_avg_last_week\"]) < 0.15\n",
    "    paid_df[\"cons_avg_last_week\"] = np.log(paid_df[\"cons_avg_last_week\"])\n",
    "    paid_df[\"consumer_plateau\"] = paid_df[\"consumer_plateau\"].astype(int)\n",
    "    paid_df.drop(columns = [\"paid_arr_7\", \"paid_arr_30\"], inplace=True)\n",
    "    return paid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_services_cons_last_week_req(date: str) -> str:\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        SUM(real_consumption) as all_consumption,\n",
    "        SUM(if(database == 'clickhouse', real_consumption, 0)) / all_consumption * 100 as clickhouse_pct,\n",
    "        SUM(if(database == 'postgres', real_consumption, 0)) / all_consumption * 100 as pg_pct,\n",
    "        SUM(if(database == 'redis', real_consumption, 0)) / all_consumption * 100 as redis_pct,\n",
    "        SUM(if(database == 'mongo', real_consumption, 0)) / all_consumption * 100 as mongo_pct,\n",
    "        SUM(if(database like '%sql%', real_consumption, 0)) / all_consumption * 100 as mysql_pct,\n",
    "        SUM(if(sku_name like '%speech%', real_consumption, 0)) / all_consumption * 100 as speech_pct,\n",
    "        SUM(if(sku_name like '%translate%', real_consumption, 0)) / all_consumption * 100 as translate_pct,\n",
    "        SUM(if(sku_name like '%vision%', real_consumption, 0)) / all_consumption * 100 as vision_pct,\n",
    "    \n",
    "        SUM(if(lower(service_long_name) like '%compute%', real_consumption, 0)) / all_consumption * 100 as compute_pct,\n",
    "        SUM(if(lower(service_long_name) like '%managed_databases%', real_consumption, 0)) / all_consumption * 100 \n",
    "        as managed_databases_pct,\n",
    "        SUM(if(lower(service_long_name) like '%artificial_intelligence%', real_consumption, 0)) / all_consumption * 100 \n",
    "        as artificial_intelligence_pct,\n",
    "        SUM(if(lower(service_long_name) like '%marketplace%', \n",
    "        real_consumption, 0)) / all_consumption * 100 as marketplace_pct,\n",
    "        SUM(if(lower(service_long_name) like '%storage%', real_consumption, 0)) / all_consumption * 100 as storage_pct,\n",
    "        SUM(if(lower(service_long_name) like '%network%', real_consumption, 0)) / all_consumption * 100 as network_pct,\n",
    "        SUM(if(lower(service_long_name) not like '%network%' and\n",
    "               lower(service_long_name) not like '%storage%' and\n",
    "               lower(service_long_name) not like '%marketplace%' and\n",
    "               lower(service_long_name) not like '%artificial_intelligence%' and\n",
    "               lower(service_long_name) not like '%managed_databases%' and\n",
    "               lower(service_long_name) not like '%compute%', real_consumption, 0)) / all_consumption * 100 as other_pct\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "    WHERE \n",
    "        toDate(event_time) < '{date}'\n",
    "    AND toDate(event_time) >= addDays(toDate('{date}'), -7)\n",
    "    AND sku_lazy == 0\n",
    "    GROUP BY billing_account_id\n",
    "    HAVING all_consumption > 0\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    return req\n",
    "\n",
    "def create_services_cons_last_week_df(date: str) -> pd.DataFrame:\n",
    "    service_df = execute_query(create_services_cons_last_week_req(date))\n",
    "    service_df.drop(columns = [\"all_consumption\"], inplace=True)\n",
    "    return service_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5  Visit information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visits_req(req_date: str) -> str:\n",
    "    part_req_1 = \"\"\n",
    "    days = 365\n",
    "    req_date = date_to_string(parse(req_date) - timedelta(1))\n",
    "    for date in date_range(req_date, freq_interval=days):\n",
    "        str_date = date_to_string(date)\n",
    "        part_req_1 += \\\n",
    "        f\"\"\"if(toDate('{str_date}') == date and cons > 0, 1, 0) as \"{str_date} tmp\",\\n\"\"\"\n",
    "    part_req_1 = part_req_1[:-2]\n",
    "    \n",
    "    part_req_2 = \"\"\n",
    "    for date in date_range(req_date, freq_interval=days):\n",
    "        str_date = date_to_string(date)\n",
    "        part_req_2 += \\\n",
    "        f\"\"\"max(\"{str_date} tmp\"),\\n\"\"\"\n",
    "    part_req_2 = part_req_2[:-2]\n",
    "    \n",
    "    req = f\"\"\"\n",
    "SELECT\n",
    "    billing_account_id,\n",
    "    arrayFilter((x, i) -> i >= start_ind, visits_arr, arrayEnumerate(visits_arr)) as visits,\n",
    "    total_consumption\n",
    "FROM (\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        array({part_req_2}) as visits_arr,\n",
    "        arrayFirstIndex(x -> (x != 0), visits_arr) as start_ind,\n",
    "        sum(cons) as total_consumption\n",
    "    FROM (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            toDate(event_time) as date,\n",
    "            sum(real_consumption) as cons,\n",
    "            {part_req_1}\n",
    "        FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "        WHERE sku_lazy == 0\n",
    "        GROUP BY billing_account_id, date\n",
    "    )\n",
    "    GROUP BY billing_account_id\n",
    "    HAVING arraySum(visits_arr) > 0\n",
    ")\n",
    "ORDER BY billing_account_id\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_arr(curr_visits_arr: tp.List[int], active_type: int = 1) -> tp.List[int]:\n",
    "    curr_visits = 0\n",
    "    ans_arr = []\n",
    "    for elem in curr_visits_arr:\n",
    "        if elem == 1 - active_type:\n",
    "            if curr_visits > 0:\n",
    "                ans_arr.append(curr_visits)\n",
    "            curr_visits = 0\n",
    "        else:\n",
    "            curr_visits += 1\n",
    "    ans_arr.append(curr_visits)\n",
    "    return ans_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_probobas(curr_visits_arr: tp.List[int], \n",
    "                     num_of_non_visits: np.array, \n",
    "                     num_of_non_visits_after_30: np.array) -> None:\n",
    "    curr_zeros = 0\n",
    "    for elem in curr_visits_arr:\n",
    "        if elem == 0:\n",
    "            curr_zeros += 1\n",
    "        else:\n",
    "            num_of_non_visits[np.arange(1, 8) <= curr_zeros] += 1\n",
    "            num_of_non_visits_after_30[np.arange(1, 8) + 30 <= curr_zeros] +=1\n",
    "            curr_zeros = 0\n",
    "    num_of_non_visits[np.arange(1, 8) + 30 <= curr_zeros] += 1\n",
    "    num_of_non_visits_after_30[np.arange(1, 8) + 30 <= curr_zeros] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_active_probabilities(df: pd.DataFrame) -> np.array:\n",
    "    num_of_non_visits = np.zeros(7)\n",
    "    num_of_non_visits_after_30 = np.zeros(7)\n",
    "    for _, row in df.iterrows():\n",
    "        refresh_probobas(row[\"visits\"], num_of_non_visits, num_of_non_visits_after_30)\n",
    "    return num_of_non_visits_after_30 / num_of_non_visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cogort_type(consumption: float) -> int: # type\n",
    "    for threshold, ind in treshold_paid_log:\n",
    "        if threshold >= consumption:\n",
    "            return ind - 1\n",
    "    return treshold_paid_log[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cogorta_churn_probabilty_column(df_raw) -> tp.List[float]:\n",
    "    df = df_raw.copy()\n",
    "    df[\"cogort\"] = df[\"avg_consumption_in_active_day\"].apply(lambda x: get_cogort_type(x))\n",
    "    probas_matrix = np.zeros((len(treshold_paid_log), 7))\n",
    "    for _, cogort_type in treshold_paid_log:\n",
    "        cogort_df = df[(df[\"cogort\"] == cogort_type) &\n",
    "                               (df[\"total_active_days\"] >= 7)]\n",
    "        probas_matrix[cogort_type] = create_non_active_probabilities(cogort_df).tolist()\n",
    "    proba_column = []\n",
    "    for _, row in df.iterrows():\n",
    "        cogort = row[\"cogort\"]\n",
    "        non_visited_days = row[\"last_non_active_period\"]\n",
    "        if non_visited_days == 0:\n",
    "            proba_column.append(0)\n",
    "            continue\n",
    "        proba_column.append(probas_matrix[cogort, min(non_visited_days - 1, 6)])\n",
    "    return proba_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visits_df(date: str) -> pd.DataFrame:\n",
    "    req = create_visits_req(date)\n",
    "    visits_df = execute_query(req)\n",
    "    if visits_df.shape[0] == 0:\n",
    "        return visits_df\n",
    "    visits_df[\"total_consumption\"] = visits_df[\"total_consumption\"].astype(float)\n",
    "    visits_df[\"active_periods\"] = visits_df[\"visits\"].apply(lambda row: get_days_arr(row, 1))\n",
    "    visits_df[\"non_active_periods\"] = visits_df[\"visits\"].apply(lambda row: get_days_arr(row, 0))\n",
    "    visits_df[\"last_active_period\"] = visits_df[\"active_periods\"].apply(lambda x: x[-1])\n",
    "    visits_df[\"last_non_active_period\"] = visits_df[\"non_active_periods\"].apply(lambda x: x[-1])\n",
    "    visits_df[\"avg_non_active_period\"] = visits_df[\"non_active_periods\"].apply(lambda x: np.mean(x) if x[-1] != 0 \n",
    "                                                                               else np.mean(x[:-1]))\n",
    "    visits_df[\"avg_active_period\"] = visits_df[\"active_periods\"].apply(lambda x: np.mean(x) if x[-1] != 0\n",
    "                                                                       else np.mean(x[:-1]))\n",
    "    visits_df[\"summary_active_days_pct\"] = visits_df[[\"active_periods\", \"visits\"]].apply(\n",
    "        lambda row: np.sum(row[\"active_periods\"]) / len(row[\"visits\"]) * 100, axis = 1)\n",
    "    visits_df[\"total_active_days\"] = visits_df[\"active_periods\"].apply(lambda x: sum(x))\n",
    "    visits_df[\"avg_non_active_period_greater_30\"] = visits_df[\"avg_non_active_period\"].apply(lambda x: int(x > 30))\n",
    "    visits_df.replace(np.nan, 0, inplace = True)\n",
    "    visits_df[\"avg_consumption_in_active_day\"] = \\\n",
    "    np.log(visits_df[\"total_consumption\"] / visits_df[\"total_active_days\"])\n",
    "    churn_prob_column = create_cogorta_churn_probabilty_column(visits_df)\n",
    "    visits_df[\"churn_probabilty\"] = churn_prob_column\n",
    "    visits_df.drop(columns = [\"visits\", \n",
    "                              \"total_consumption\", \n",
    "                              \"active_periods\", \"non_active_periods\"], inplace=True)\n",
    "    return visits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Main information DataFrame (as model features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_infromation_req(date: str) -> str:\n",
    "    common_info_req = f\"\"\"\n",
    "SELECT\n",
    "    billing_account_id,\n",
    "    cloud_id,\n",
    "    age,\n",
    "    sex,\n",
    "    region,\n",
    "    payment_type,\n",
    "    person_type,\n",
    "    if (state in ('suspended', 'inactive', 'deleted'), \n",
    "    'inactive', state) as state,\n",
    "    multiIf (os like '%Mac%', 'Mac',\n",
    "             os like '%Linux%' or os like '%Ubuntu%', 'Linux',\n",
    "             os like '%Windows%', 'Windows', \n",
    "             'other') as os,\n",
    "    is_suspended_now,\n",
    "    from_desktop\n",
    "FROM (\n",
    "    SELECT\n",
    "    billing_account_id,\n",
    "    max(cloud_id) as cloud_id,\n",
    "    max(age) as age, -- leak, but not important\n",
    "    max(sex) as sex,-- leak, but not important\n",
    "    max(multiIf(city == 'Москва', 'Moscow',\n",
    "                city == 'Санкт-Петербург', 'Saint Petersburg',\n",
    "                country == 'Россия', 'Russia', \n",
    "                isNotNull(country), 'Other countries',\n",
    "                'undefined')) as region,  -- leak, but not important\n",
    "    max(ba_payment_cycle_type) as payment_type,  -- leak, but not important\n",
    "    max(if(ba_person_type like '%company%', \n",
    "                               'company',\n",
    "                               ba_person_type)) as person_type,  -- leak, but not important\n",
    "    argMax(ba_state, event_time) as state,\n",
    "    if(state in ('suspended', 'inactive', 'deleted'), 1, 0)\n",
    "    as is_suspended_now,\n",
    "    max(segment) as segment,  -- leak, but not important\n",
    "    max(If(device_type like '%desktop%', 1, 0)) as from_desktop,  -- leak, but not important\n",
    "    max(if (isNotNull(os), splitByChar(' ', assumeNotNull(os))[1], Null)) as os, -- leak, but not important\n",
    "    sum(if(\n",
    "            toDate(event_time) < toDate('{date}') AND\n",
    "            toDate(event_time) >= addDays(toDate('{date}'), -7) \n",
    "           AND sku_lazy == 0, real_consumption, 0)) as cons,\n",
    "    max(multiIf(\n",
    "    first_first_paid_consumption_datetime == '0000-00-00 00:00:00', '2070-01-01 00:00:00',\n",
    "    first_first_paid_consumption_datetime)) as first_first_paid_consumption_datetime\n",
    "FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "WHERE\n",
    "    toDate(event_time) < toDate('{date}')\n",
    "AND ba_usage_status != 'service'\n",
    "GROUP BY billing_account_id\n",
    "HAVING segment not in ('Enterprise', 'Large ISV', 'ISV ML', 'Medium', 'Yandex Projects', 'Yandex Staff')\n",
    "AND cons > 0\n",
    "AND toDate((first_first_paid_consumption_datetime)) < \n",
    "addDays(toDate('{date}'), -14)\n",
    ")\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\".encode('utf-8')\n",
    "    return common_info_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_infromation_df(date: str) -> pd.DataFrame:\n",
    "    main_info_df = execute_query(create_main_infromation_req(date))\n",
    "    main_info_df[\"is_suspended_now\"] = \\\n",
    "    main_info_df[\"is_suspended_now\"].astype(float)\n",
    "    main_info_df[\"from_desktop\"] = \\\n",
    "    main_info_df[\"from_desktop\"].astype(float)\n",
    "    main_info_df.replace(np.nan, \"undefined\", inplace=True)\n",
    "    return main_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Main information DataFrame (ONLY for mails info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mail_info_req() -> str:\n",
    "    passport_current_path = f\"//home/cloud_analytics/import/iam/cloud_owners_history\"\n",
    "    main_request = f\"\"\"\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        account_name,\n",
    "        email,\n",
    "        phone,\n",
    "        timezone,\n",
    "        person_type,\n",
    "        paid_consumption_per_last_month\n",
    "    FROM (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            account_name as account_name,\n",
    "            first_name as first_name,\n",
    "            last_name as last_name,\n",
    "            user_settings_email as email,\n",
    "            ba_person_type as person_type,\n",
    "            phone as phone,\n",
    "            timezone\n",
    "        FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\" as a\n",
    "        LEFT JOIN (\n",
    "            SELECT\n",
    "                max(timezone) as timezone, \n",
    "                passport_uid \n",
    "            FROM \"{passport_current_path}\"\n",
    "            GROUP BY passport_uid\n",
    "        ) as b\n",
    "        ON a.puid == b.passport_uid\n",
    "        WHERE \n",
    "            event == 'ba_created'\n",
    "        AND segment not in ('Enterprise', 'Large ISV', 'ISV ML', 'Medium', 'Yandex Projects')\n",
    "        ) as a\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            SUM(if(toDate(event_time) >= addDays(NOW(), -30), real_consumption, 0)) as paid_consumption_per_last_month\n",
    "        FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "        GROUP BY billing_account_id\n",
    "    ) as b\n",
    "    on a.billing_account_id == b.billing_account_id\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    return main_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mail_info_df() -> pd.DataFrame:\n",
    "    df = execute_query(create_mail_info_req())\n",
    "    \n",
    "    df['last_active_id'] = df[\"billing_account_id\"].apply(lambda x: builling_to_last_active_billing[x] \n",
    "                                                   if builling_to_last_active_billing.get(x) is not None \n",
    "                                                   else x)\n",
    "    consum_dict = df.groupby('last_active_id')['paid_consumption_per_last_month'].sum()\n",
    "    df['paid_consumption_per_last_month'] = df['last_active_id'].apply(lambda x: consum_dict[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Добавить ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_precision_billings_request(date: str) -> str:\n",
    "    req = f\"\"\"\n",
    "SELECT\n",
    "    DISTINCT billing_account_id\n",
    "FROM (\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        groupUniqArray(toDate(event_time)) as visited_days\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "    WHERE sku_lazy = 0\n",
    "    AND real_consumption > 0\n",
    "    AND toDate(event_time) >= addDays(toDate('{date}'), -33)\n",
    "    AND toDate(event_time) < toDate('{date}')\n",
    "    GROUP BY billing_account_id\n",
    ")\n",
    "WHERE length(visited_days) > 3\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_billings_request(date: str) -> str:\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT billing_account_id\n",
    "    FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "    WHERE sku_lazy = 0\n",
    "    AND real_consumption > 0\n",
    "    AND toDate(event_time) >= addDays(toDate('{date}'), -33)\n",
    "    AND toDate(event_time) < toDate('{date}')\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_billings_which_visits(date: str) -> pd.DataFrame:\n",
    "    funcs = [lambda x: execute_query(create_target_billings_request(x)), \n",
    "             lambda x: execute_query(create_evaluation_precision_billings_request(x))]\n",
    "    data = Parallel(n_jobs=-1)(delayed(func)(date) for func in funcs)\n",
    "    target_df, evaluation_precision_df = data[0], data[1]\n",
    "    string_date_to_add_answers =  date_to_string(parse(date) - timedelta(days=35))\n",
    "    target_df[\"date\"] = [string_date_to_add_answers] * target_df.shape[0]\n",
    "    evaluation_precision_df[\"date\"] = [string_date_to_add_answers] * evaluation_precision_df.shape[0]\n",
    "    return target_df, evaluation_precision_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Склеить таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_week_df(date: str) -> pd.DataFrame:\n",
    "    df_funcs = [create_vm_cube_info_df, \n",
    "                create_main_infromation_df, \n",
    "                create_visits_df, \n",
    "                create_paid_df,\n",
    "                create_services_cons_last_week_df,\n",
    "                create_mdb_on_vm_list,\n",
    "                create_df_of_billings_which_visits]\n",
    "    dfs = Parallel(n_jobs=-1)(delayed(func)(date) for func in df_funcs)\n",
    "    vm_cube_df = dfs[0]\n",
    "    main_info_df = dfs[1]\n",
    "    visits_df = dfs[2]\n",
    "    paid_df = dfs[3]\n",
    "    service_df = dfs[4]\n",
    "    mdb_on_vm_bids = dfs[5]\n",
    "    target_df, evaluation_precision_df = dfs[6]\n",
    "    \n",
    "    df = pd.merge(main_info_df, visits_df, on = \"billing_account_id\", how = \"inner\")\n",
    "    df = pd.merge(df, paid_df, on = \"billing_account_id\", how = \"inner\")\n",
    "    df = pd.merge(df, service_df, on = \"billing_account_id\", how = \"inner\")\n",
    "    df = pd.merge(df, vm_cube_df, on = \"cloud_id\", how = \"left\")\n",
    "    df[\"cogort\"] = df['cons_avg_last_week'].apply(lambda x: get_cogort_type(x))\n",
    "    df[\"using_mdb_on_vm\"] = (df[\"billing_account_id\"].isin(mdb_on_vm_bids)).astype(int)\n",
    "    df[\"date\"] = [date] * df.shape[0]\n",
    "    df.replace(np.nan, 0, inplace=True)\n",
    "    new_col_list = [\"date\"] + list(df.columns[:-1])\n",
    "    df = df[new_col_list]\n",
    "    df[\"core_fraction\"].replace(\"0\", \"undefined\", inplace=True)\n",
    "    df[\"core_fraction\"].replace(0, \"undefined\", inplace=True)\n",
    "    df = replace_column_in_df(df, \"date\")\n",
    "    return df, target_df, evaluation_precision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [\"compute\", \"managed_databases\", \"artificial_intelligence\", \n",
    "            \"marketplace\", \"storage\", \"network\", \"other\"]\n",
    "cube_products = [\"Ubuntu\", \"Windows\", \"CentOS\", \"Debian\"]\n",
    "databases = [\"clickhouse\", \"pg\", \"redis\", \"mongo\"]\n",
    "ais = [\"speech\", \"translate\", \"vision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10  финальная таблица (Для создания первого датасета с фичами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_target_column(date, target, answers_max_date):\n",
    "    if pd.isnull(target):\n",
    "        if date <= answers_max_date:\n",
    "            return 1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_table(start_date, end_date):\n",
    "    def loop_body(date):\n",
    "        string_date = date_to_string(date)\n",
    "        return create_week_df(string_date)\n",
    "    data = Parallel(n_jobs=-1)(delayed(loop_body)(date) for date in date_range(end_date, \n",
    "                                                                           start_date=start_date, \n",
    "                                                                           freq=\"7D\"))\n",
    "    evaluation_array = [] \n",
    "    weeks_arrays = []\n",
    "    target_array = []\n",
    "    weeks_arrays = []\n",
    "    for week_df, target_df, evaluation_precision_df in data:\n",
    "        evaluation_array.append(evaluation_precision_df)\n",
    "        weeks_arrays.append(week_df)\n",
    "        target_array.append(target_df)\n",
    "    \n",
    "    person_df = concatenate_tables(weeks_arrays)\n",
    "    target_df = concatenate_tables(target_array)\n",
    "    evaluation_precision_df = concatenate_tables(evaluation_array)\n",
    "    target_df[\"target\"] = [0] * target_df.shape[0]\n",
    "    evaluation_precision_df[\"evaluate\"] = [0] * evaluation_precision_df.shape[0]\n",
    "    res_df = pd.merge(person_df.replace(np.nan, 0), target_df, on = [\"date\", \"billing_account_id\"], how = \"left\")\n",
    "    res_df = pd.merge(res_df, evaluation_precision_df, on = [\"date\", \"billing_account_id\"], how = \"left\")\n",
    "    res_df[\"core_fraction\"].replace(\"0\", \"undefined\", inplace=True)\n",
    "    res_df[\"core_fraction\"].replace(0, \"undefined\", inplace=True)\n",
    "    max_date = max(res_df[\"date\"])\n",
    "    answers_max_date = date_to_string(parse(max_date) - timedelta(days=35))\n",
    "    res_df[\"target\"] = res_df[[\"date\", \"target\"]].apply(lambda row: \n",
    "                                                    clean_target_column(row[\"date\"], \n",
    "                                                                        row[\"target\"], \n",
    "                                                                        answers_max_date), axis =1)\n",
    "    res_df[\"precision_evaluate\"] = res_df[[\"date\", \"evaluate\"]].apply(lambda row: \n",
    "                                                    clean_target_column(row[\"date\"], \n",
    "                                                                        row[\"evaluate\"], \n",
    "                                                                        answers_max_date), axis =1)\n",
    "    res_df.drop(columns = [\"evaluate\"], inplace=True)\n",
    "    res_df = replace_column_in_df(res_df, \"date\")\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = '2020-01-06'\n",
    "# end_date = '2020-06-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# res_df = create_train_table(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_path = \"//home/cloud_analytics/churn_prediction\"\n",
    "# file_name = \"person_info\"\n",
    "# save_table(file_name, temp_path, res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df[\"precision_evaluate\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Создание скрипта обновления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Перекопирование всех файлов внутри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_all(from_path, to_path):\n",
    "    assert (\"churn_prediction\" in from_path and \"churn_prediction_copy/churn_prediction\" in to_path) or \\\n",
    "    (\"churn_prediction\" in to_path and \"churn_prediction_copy/churn_prediction\" in from_path)\n",
    "\n",
    "    yt.copy(from_path, to_path, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_person_df_without_types(path):\n",
    "    curr_path = path + \"/person_info\"\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \"{curr_path}\"\n",
    "    ORDER BY date\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = execute_query(req)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_answers(date, bid, target, last_answers_date, visiting_billings):\n",
    "    if date == last_answers_date:\n",
    "        if bid in visiting_billings:\n",
    "            return 0\n",
    "        return 1\n",
    "    if not pd.isnull(target):\n",
    "        return target\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_with_new_week(from_path, to_path):\n",
    "    copy_all(from_path, to_path)\n",
    "    df = read_person_df_without_types(from_path)\n",
    "    max_date = df[\"date\"].max()\n",
    "    next_date = date_to_string(parse(max_date) + timedelta(7))\n",
    "    week_df, target_df, evaluation_precision_df = create_week_df(next_date)\n",
    "    last_answers_date = target_df[\"date\"].iloc[0]\n",
    "    target_billings = set(target_df[\"billing_account_id\"])\n",
    "    evaluation_precision_billings = set(evaluation_precision_df[\"billing_account_id\"])\n",
    "    for column in evaluate_columns:\n",
    "        if column == \"precision_evaluate\": \n",
    "            billings = evaluation_precision_billings\n",
    "        else: \n",
    "            billings = target_billings\n",
    "        df[column] = df[[\"date\", \"billing_account_id\", column]].apply(lambda row: \n",
    "                                                                        update_answers(row[\"date\"], \n",
    "                                                                        row[\"billing_account_id\"], \n",
    "                                                                        row[column], \n",
    "                                                                        last_answers_date, billings), axis = 1)\n",
    "        week_df[column] = [np.nan] * week_df.shape[0]\n",
    "    df = concatenate_tables([df, week_df])\n",
    "    return df, last_answers_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_with_new_week(from_path, to_path):\n",
    "    copy_all(from_path, to_path)\n",
    "    df = read_person_df_without_types(from_path)\n",
    "    max_date = df[\"date\"].max()\n",
    "#     print(max_date)\n",
    "    next_date = date_to_string(parse(max_date) + timedelta(7))\n",
    "    print(next_date)\n",
    "    assert parse(next_date) <= parse(get_current_date_as_str())\n",
    "    week_df, target_df, evaluation_precision_df = create_week_df(next_date)\n",
    "    last_answers_date = target_df[\"date\"].iloc[0]\n",
    "    target_billings = set(target_df[\"billing_account_id\"])\n",
    "    evaluation_precision_billings = set(evaluation_precision_df[\"billing_account_id\"])\n",
    "    for column in evaluate_columns:\n",
    "        if column == \"precision_evaluate\":\n",
    "            billings = evaluation_precision_billings\n",
    "        else:\n",
    "            billings = target_billings\n",
    "        df[column] = df[[\"date\", \"billing_account_id\", column]].apply(lambda row:\n",
    "                                                                        update_answers(row[\"date\"],\n",
    "                                                                        row[\"billing_account_id\"],\n",
    "                                                                        row[column],\n",
    "                                                                        last_answers_date, billings), axis = 1)\n",
    "        week_df[column] = [np.nan] * week_df.shape[0]\n",
    "    df = concatenate_tables([df, week_df])\n",
    "    return df, last_answers_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, last_answers_date = update_table_with_new_week(from_path, to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_results_table = create_model_validate_and_predict(df, current_date, last_answers_date, from_path,\n",
    "#                                                        append=append)\n",
    "# create_answer_df_and_update_already_in_experiment_users(meta_results_table, \n",
    "#                                                         current_date,\n",
    "#                                                         from_path,\n",
    "#                                                         append=append)\n",
    "\n",
    "# file_name = \"person_info\"\n",
    "# save_table(file_name, from_path, df)\n",
    "# final_week_evaluation(df, last_answers_date, from_path, append=append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Очистка (ДЛЯ ДЕБАГА)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_request(path):\n",
    "    req = f\"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM \"{path}\"\n",
    "ORDER BY date\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "    full_df = execute_query(req)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_from_last_date(path):\n",
    "    df = read_person_df_without_types(path)\n",
    "    date = df[\"date\"].max()\n",
    "    print(date)\n",
    "    df = df[df[\"date\"] != date]\n",
    "    val_date = date_to_string(parse(date) - timedelta(35))\n",
    "    print(val_date)\n",
    "    for index in df[df[\"date\"] == val_date][\"target\"].index:\n",
    "        df[\"target\"].loc[index] = np.nan\n",
    "    print(df[df[\"date\"] == val_date][\"target\"].unique())\n",
    "    file_name = \"person_info\"\n",
    "    save_table(file_name, path, df)\n",
    "    val_table = full_request(path + \"/validation_table\")\n",
    "    val_table = val_table[val_table[\"date\"] != val_date]\n",
    "    save_table(\"validation_table\", path, val_table)\n",
    "    \n",
    "    eval_table = full_request(path + \"/evaluation_table\")\n",
    "    eval_table = eval_table[eval_table[\"date\"] != val_date]\n",
    "    save_table(\"evaluation_table\", path, eval_table)\n",
    "    \n",
    "    persons_in_experiment = full_request(path + \"/persons_in_experiment_already\")\n",
    "    persons_in_experiment = persons_in_experiment[persons_in_experiment[\"date\"] != date]\n",
    "    save_table(\"persons_in_experiment_already\", path, persons_in_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_path = \"//home/cloud_analytics/churn_prediction\"\n",
    "# to_path = \"//home/cloud_analytics/lunin-dv/churn_prediction_copy/churn_prediction_xgboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_all(to_path, from_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_from_last_date(from_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.changer = {}\n",
    "    \n",
    "    def __create_dummies(df, cat_column, *, changer=None):\n",
    "        dum = pd.get_dummies(df[cat_column])\n",
    "        dum.columns = [cat_column + \"_\"+ val for val in dum.columns]\n",
    "        if \"undefined\" in df[cat_column].unique():\n",
    "            dum.drop(columns = [cat_column + \"_undefined\"], inplace = True)\n",
    "        if \"Undefined\" in df[cat_column].unique():\n",
    "            dum.drop(columns = [cat_column + \"_undefined\"], inplace = True)\n",
    "        df = pd.concat([df, dum], axis = 1)\n",
    "        \n",
    "        if changer is None:\n",
    "            changer = df.groupby(cat_column)[\"y\"].mean()\n",
    "            changer = changer.to_dict()\n",
    "        \n",
    "        df[cat_column] = df[cat_column].replace(changer)\n",
    "        return df, changer\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        df = X.copy()\n",
    "        df[\"y\"] = y\n",
    "        for cat_column in X.columns:\n",
    "            if X[cat_column].dtype == object or X[cat_column].dtype == str:\n",
    "                _, self.changer[cat_column] = DummyTransformer.__create_dummies(df, cat_column, changer=None)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_real):\n",
    "        X = X_real.copy()\n",
    "        for cat_column in self.changer:\n",
    "            X, _ =  DummyTransformer.__create_dummies(X, cat_column, changer=self.changer[cat_column])\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 train часть "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results_for_meta_id_using_raw_billing_answers(answer_column_name, date, df_raw):\n",
    "    df = df_raw.copy()\n",
    "    df[\"meta_id\"] = df[\"billing_account_id\"].apply(lambda x: builling_to_last_active_billing[x] \n",
    "                                                   if builling_to_last_active_billing.get(x) is not None \n",
    "                                                   else x)\n",
    "    curr_meta_df = df[df[\"date\"] == date][[\"meta_id\", answer_column_name]]\n",
    "    y = curr_meta_df[answer_column_name]\n",
    "    curr_meta_df[answer_column_name] = 1 - np.array(y)\n",
    "    \n",
    "    meta_df = pd.DataFrame(curr_meta_df.groupby(\"meta_id\")[answer_column_name].sum())\n",
    "    meta_df.columns = [answer_column_name]\n",
    "    meta_df.columns = [answer_column_name]\n",
    "    meta_df[answer_column_name] = meta_df[answer_column_name].apply(lambda x: 0 if x > 0 else 1)\n",
    "    meta_df[\"meta_id\"] = meta_df.index\n",
    "    meta_df.index = np.arange(0, meta_df.shape[0])\n",
    "    meta_df[\"billing_account_id\"] = meta_df[\"meta_id\"]\n",
    "    meta_df.drop(columns = [\"meta_id\"], inplace=True)\n",
    "    meta_df[\"date\"] = [date] * meta_df.shape[0]\n",
    "    return meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_associated_predictions_and_answers(prediction_df_raw, df):\n",
    "    prediction_df = prediction_df_raw.copy()\n",
    "    answers_df = df.copy()\n",
    "    date = prediction_df[\"date\"].max()\n",
    "    prediction_df = prepare_results_for_meta_id_using_raw_billing_answers(\"prediction\", date, \n",
    "                                                                          prediction_df)\n",
    "    prediction = np.array(prediction_df[\"prediction\"])\n",
    "    answers_df = prepare_results_for_meta_id_using_raw_billing_answers(\"precision_evaluate\", date, answers_df)\n",
    "    resulted_df = pd.merge(prediction_df, answers_df, on = \"billing_account_id\", how = \"inner\")\n",
    "    precision = np.array(resulted_df[\"precision_evaluate\"])\n",
    "    \n",
    "    answers_df = df.copy()\n",
    "    answers_df = prepare_results_for_meta_id_using_raw_billing_answers(\"target\", date, answers_df)\n",
    "    resulted_df = pd.merge(prediction_df, answers_df, on = \"billing_account_id\", how = \"inner\")\n",
    "    recall = np.array(resulted_df[\"target\"])\n",
    "\n",
    "    return prediction, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, date, path, file_name=None, append=True, prediction_df = None, \n",
    "                   X_val=None, y_val_precision=None, y_val_recall=None,  model=None):\n",
    "    answers = []\n",
    "    if \"validation\" in file_name:\n",
    "        y_pred = model.predict(X_val)\n",
    "        answers.append((\"billing_account_id\", y_pred, y_val_precision, y_val_recall))\n",
    "        y_pred = np.array(y_pred)\n",
    "        billings = np.array(df[df[\"date\"] == date][\"billing_account_id\"])\n",
    "        \n",
    "        prediction_df = pd.DataFrame.from_dict({\"date\": [date] * y_pred.shape[0], \n",
    "                                                \"billing_account_id\": billings,\n",
    "                                                \"prediction\": y_pred})\n",
    "    y_meta_predictions, y_meta_precision, y_meta_recall =\\\n",
    "    get_associated_predictions_and_answers(prediction_df, df)\n",
    "    answers.append((\"meta_id\", y_meta_predictions, y_meta_precision, y_meta_recall))\n",
    "    print(\"date:\", date)\n",
    "    for elem in answers:\n",
    "        curr_id_type, prediction_y, precision_y, recall_and_fscore_y = elem\n",
    "        precision = precision_score(precision_y, prediction_y)\n",
    "        recall = recall_score(recall_and_fscore_y, prediction_y)\n",
    "        fscore = fbeta_score(recall_and_fscore_y, prediction_y, beta=0.5)\n",
    "        print(f\"id type: {curr_id_type}; fbeta(0.5): {round(fscore, 3)};\"\n",
    "              f\" precision: {round(precision, 3)}; recall: {round(recall, 3)};\")\n",
    "        curr_dict = {\"date\": date,\n",
    "                     \"id type\": curr_id_type,\n",
    "                     \"amount of user churn\": int(np.sum(recall_and_fscore_y)),\n",
    "                     \"number of predictions\": int(np.sum(prediction_y)),\n",
    "                     \"precision\": [precision], \n",
    "                     \"recall\": [recall], \n",
    "                     \"fbeta(0.5)\": [fscore]}  \n",
    "        res_df = pd.DataFrame.from_dict(curr_dict)\n",
    "        if file_name is not None:\n",
    "            save_table(file_name, path, res_df, append=append)\n",
    "        append=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 финальная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "append = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_validate_and_predict(df, date, last_answers_date, path, append=True):\n",
    "    train_table = df[df[\"date\"] < last_answers_date]\n",
    "    validation_table = df[df[\"date\"] == last_answers_date]\n",
    "    test_table = df[df[\"date\"] == date]\n",
    "    boost_pipeline = Pipeline([('dummy_transformer', DummyTransformer()),\n",
    "                                 ('StandartScaler', MinMaxScaler()),\n",
    "                                 ('CatBoost', CatBoostClassifier(verbose=False, random_seed=8))])\n",
    "    columns = [\"date\", \"billing_account_id\", \"cloud_id\"]\n",
    "    X = train_table.drop(columns = evaluate_columns + columns)\n",
    "    y = train_table[\"target\"]\n",
    "    X_val = validation_table.drop(columns = evaluate_columns + columns)\n",
    "    y_recall_val = validation_table[\"target\"]\n",
    "    y_precision_val = validation_table[\"precision_evaluate\"]\n",
    "    test_X = test_table.drop(columns = evaluate_columns + columns)\n",
    "    \n",
    "    #xgboost_pipeline, params = create_best_xgboost_model_with_grid_search(xgboost_pipeline, X, y)\n",
    "    boost_pipeline.fit(X, y)\n",
    "    \n",
    "    # validation\n",
    "    print(\"VALIDATION\")\n",
    "    evaluate_model(df, last_answers_date, path, file_name=\"validation_table\", \n",
    "                   append=append, X_val=X_val, \n",
    "                   y_val_precision=y_precision_val, \n",
    "                   y_val_recall=y_recall_val,\n",
    "                   model=boost_pipeline)\n",
    "    X = df[df[\"date\"] <= last_answers_date].drop(columns = evaluate_columns + columns)\n",
    "    y = df[df[\"date\"] <= last_answers_date][\"target\"]\n",
    "    \n",
    "    boost_pipeline.fit(X, y)\n",
    "    y_pred = boost_pipeline.predict(test_X)\n",
    "    billings = np.array(test_table[\"billing_account_id\"])\n",
    "    prediction_df = pd.DataFrame.from_dict({\"date\": [date] * y_pred.shape[0], \n",
    "                                            \"billing_account_id\": billings,\n",
    "                                            \"prediction\": y_pred})\n",
    "    meta_results_table = prepare_results_for_meta_id_using_raw_billing_answers(\"prediction\", date, prediction_df)\n",
    "    return meta_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_results_table = create_model_validate_and_predict(df, current_date, last_answers_date, from_path,\n",
    "#                                                        append=append)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_already_in_experiment_billings(path):\n",
    "    curr_path = path + \"/persons_in_experiment_already\"\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \"{curr_path}\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    bad_persons = execute_query(req)\n",
    "    return bad_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_upsell():\n",
    "    min_datetime = parse(get_current_date_as_str()) - timedelta(days=70)\n",
    "    path = \"//home/cloud_analytics/export/crm/upsale\"\n",
    "    dfs = []\n",
    "    tables = []\n",
    "    \n",
    "    for table in find_tables_in_hahn_folder(path):\n",
    "        dt = parse(table.split('/')[-1])\n",
    "        if dt > min_datetime:\n",
    "            tables.append(table)\n",
    "\n",
    "    def req(table):\n",
    "        req = f\"\"\"\n",
    "        SELECT\n",
    "            billing_account_id\n",
    "        FROM \"{table}\"\n",
    "        FORMAT TabSeparatedWithNames\n",
    "        \"\"\"\n",
    "        return execute_query(req)\n",
    "\n",
    "    data = Parallel(n_jobs=-1)(delayed(req)(table) for table in tables)\n",
    "    upsell_df = concatenate_tables(data)\n",
    "    return upsell_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsell = read_from_upsell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer_df_and_update_already_in_experiment_users(meta_results_table, \n",
    "                                                            current_date,\n",
    "                                                            path,\n",
    "                                                            append=True):\n",
    "    mail_df = create_mail_info_df()\n",
    "    meta_results_table = meta_results_table[[\"date\", \"billing_account_id\", \"prediction\"]]\n",
    "    meta_results_table[\"associated_billings\"] = \\\n",
    "    meta_results_table[\"billing_account_id\"].apply(lambda x: user.get_all_accosiated_billings(x))\n",
    "    \n",
    "    meta_results_table = pd.merge(meta_results_table, mail_df, how = \"inner\")\n",
    "    \n",
    "    testing_billings = meta_results_table[meta_results_table[\"prediction\"] == 1][[\"date\", \"billing_account_id\"]]\n",
    "    file_name = \"persons_in_experiment_already\"\n",
    "    save_table(file_name, path, testing_billings, append=append)\n",
    "    bad_persons = read_already_in_experiment_billings(path)\n",
    "\n",
    "    print(\"Upsell removed\")\n",
    "    upsell_df = read_from_upsell()\n",
    "    upsell_df[\"date\"] = [current_date] * upsell_df.shape[0]\n",
    "    bad_persons = concatenate_tables([bad_persons, upsell_df])\n",
    "\n",
    "    bad_persons = bad_persons[(bad_persons[\"date\"] >= experiment_start_date) &\n",
    "                              (bad_persons[\"date\"] < current_date)]\n",
    "    \n",
    "    print(\"number of users before removing users in experiment already:\", meta_results_table.shape[0])\n",
    "    meta_results_table = \\\n",
    "    meta_results_table[~meta_results_table[\"billing_account_id\"].isin(bad_persons[\"billing_account_id\"])]\n",
    "    print(\"number of users:\", meta_results_table.shape[0])\n",
    "    # ab testing\n",
    "    interest_group =\\\n",
    "    meta_results_table[meta_results_table['prediction'] == 1].sort_values(\n",
    "        by='paid_consumption_per_last_month', ascending=False)\n",
    "\n",
    "    # 30 for calls -> 60 as control/test group\n",
    "    indexes = list(interest_group.head(60).index)\n",
    "    random.shuffle(indexes)\n",
    "    test_indexes = indexes[::2]\n",
    "    control_indexes = indexes[1::2]\n",
    "    ####################################\n",
    "    meta_results_table = add_AB_testing_group(meta_results_table, {\"test\": test_indexes, \n",
    "                                                                   \"control\": control_indexes})\n",
    "    file_name = current_date\n",
    "    save_table(file_name, path + \"/week_logs\", meta_results_table, schema={\"associated_billings\" : \n",
    "                                                                           \"list:string\"})\n",
    "    test_group = meta_results_table[meta_results_table[\"Group\"] == 'test']\n",
    "    save_table(file_name, path + \"/churn_prediction_test_group\",\n",
    "               test_group, schema={\"associated_billings\" : \"list:string\"})\n",
    "\n",
    "    predicted_group = meta_results_table[meta_results_table['Group'] != '']\n",
    "    save_table(file_name, path + \"/full_prediction_group\",\n",
    "               predicted_group, schema={\"associated_billings\" : \"list:string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Оценка алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_info_from_week_log(path, date):\n",
    "    curr_path = path + '/week_logs/' + date\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        date,\n",
    "        billing_account_id,\n",
    "        prediction\n",
    "    FROM \"{curr_path}\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    evaluate_df = execute_query(req)\n",
    "    return evaluate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_week_evaluation(df, last_answers_date, path, append=True):\n",
    "    if yt.exists(path + '/week_logs/' + last_answers_date):\n",
    "        evaluate_df = take_info_from_week_log(path, last_answers_date)\n",
    "        print(\"TEST EVALUATION\")\n",
    "        evaluate_model(df, last_answers_date, path, file_name=\"evaluation_table\", \n",
    "               append=append, prediction_df=evaluate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Сборочка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_final_script_function(from_path, to_path, append=True):\n",
    "    try:\n",
    "        df, last_answers_date = update_table_with_new_week(from_path, to_path)\n",
    "        current_date = df[\"date\"].max()\n",
    "        meta_results_table = create_model_validate_and_predict(df, current_date, last_answers_date, from_path,\n",
    "                                                               append=append)\n",
    "        create_answer_df_and_update_already_in_experiment_users(meta_results_table, \n",
    "                                                                current_date,\n",
    "                                                                from_path,\n",
    "                                                                append=append)\n",
    "\n",
    "        file_name = \"person_info\"\n",
    "        save_table(file_name, from_path, df)\n",
    "        final_week_evaluation(df, last_answers_date, from_path, append=append)\n",
    "    except BaseException:\n",
    "        # revert\n",
    "        print(\"Ooops, Exception!\")\n",
    "        copy_all(to_path, from_path)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_columns = [\"target\", \"precision_evaluate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_path = \"//home/cloud_analytics/churn_prediction\"\n",
    "to_path = \"//home/cloud_analytics/lunin-dv/churn_prediction_copy/churn_prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-21\n",
      "VALIDATION\n",
      "date: 2020-08-17\n",
      "id type: billing_account_id; fbeta(0.5): 0.64; precision: 0.872; recall: 0.439;\n",
      "id type: meta_id; fbeta(0.5): 0.646; precision: 0.871; recall: 0.442;\n",
      "Upsell removed\n",
      "number of users before removing users in experiment already: 5043\n",
      "number of users: 3514\n",
      "TEST EVALUATION\n",
      "date: 2020-08-17\n",
      "id type: meta_id; fbeta(0.5): 0.659; precision: 0.885; recall: 0.453;\n",
      "CPU times: user 6min 45s, sys: 18.3 s, total: 7min 4s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_full_final_script_function(from_path, to_path, append=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
