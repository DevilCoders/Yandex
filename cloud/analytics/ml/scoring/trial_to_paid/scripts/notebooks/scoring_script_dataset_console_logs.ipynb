{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil.parser import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import typing as tp\n",
    "from sklearn.preprocessing import normalize\n",
    "import my_library as lib\n",
    "from datetime import datetime\n",
    "import re\n",
    "import gensim\n",
    "import gensim as gen\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_TO_OBSERVE = 14\n",
    "EMPTY_TYPE = 'undefined'\n",
    "column_types = set()\n",
    "TABLE_NAME = 'console_logs_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    column_types.add(\"number_of_days_site_visits:numeric\")\n",
    "    column_types.add(\"last_hit_first_hit_date_diff:numeric\")\n",
    "    column_types.add(\"last_hit_day_diff_with_scoring:numeric\")\n",
    "    column_types.add(\"site_hit_length:numeric\")\n",
    "    column_types.add(\"unique_site_hit_length:numeric\")\n",
    "    column_types.add(\"unique_to_all_hits_pct:numeric\")\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        arraySort((x, y) -> y, groupArray(event), groupArray(time)) as event_array,\n",
    "        COUNT(DISTINCT time) as number_of_days_site_visits,\n",
    "        dateDiff('day', MIN(time), MAX(time)) as last_hit_first_hit_date_diff,\n",
    "        dateDiff('day', MAX(time), scoring_date) as last_hit_day_diff_with_scoring,\n",
    "        length(groupArray(event)) as site_hit_length,\n",
    "        length(groupUniqArray(event)) as unique_site_hit_length,\n",
    "        unique_site_hit_length / site_hit_length * 100 as unique_to_all_hits_pct,\n",
    "        scoring_date\n",
    "    FROM (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            scoring_date,\n",
    "            event,\n",
    "            toDate(replaceRegexpOne(timestamp, '[.].*', '')) as time\n",
    "        FROM \"//home/cloud_analytics/import/console_logs/events\" as a\n",
    "        INNER JOIN (\n",
    "            SELECT\n",
    "                puid,\n",
    "                billing_account_id,\n",
    "                addDays(toDate(MIN(first_first_trial_consumption_datetime)),\n",
    "                {DAYS_TO_OBSERVE}) as scoring_date\n",
    "            FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "            WHERE event == 'ba_created'\n",
    "            AND billing_account_id != ''\n",
    "            AND puid != ''\n",
    "            AND first_first_trial_consumption_datetime != '0000-00-00 00:00:00'\n",
    "            GROUP BY puid, billing_account_id \n",
    "\n",
    "        ) as b\n",
    "        ON a.puid == b.puid\n",
    "        WHERE time < scoring_date\n",
    "        AND response >= '200'\n",
    "        AND response < '300'\n",
    "    )\n",
    "    GROUP BY billing_account_id, scoring_date\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = lib.execute_query(req)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_changer(x):\n",
    "    x = re.sub(r'.ru', r'.com', x)\n",
    "    x = re.sub(r'https:\\/\\/(.*)|\\/(.*)', \n",
    "               r'\\1\\2', x)\n",
    "    x = re.sub(r'\\/id|_\\/|\\/folders', \n",
    "               r'', x)\n",
    "    x = re.sub(r'([^\\/]*\\/[^\\/]*\\/[^\\/]*)\\/.*|(.*)', \n",
    "               r'\\1\\2', x)\n",
    "    x = re.sub(r'([^?]*)?.*', \n",
    "               r'\\1', x)\n",
    "    x = re.sub(r'([^&]*)&.*', \n",
    "               r'\\1', x)\n",
    "    parts = x.split(\"/\")\n",
    "    good_parts = []\n",
    "    for part in parts:\n",
    "        if not any(map(str.isdigit, part)):\n",
    "            good_parts.append(part)\n",
    "    x = \"/\".join(good_parts)\n",
    "    x = re.sub(r\"[.]|\\/\", r'_', x)\n",
    "    x = x.lower()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "interested_patterns = [\n",
    "    \"console.cloud.yandex.com\",\n",
    "    \"docs\",\n",
    "    \"docs/compute\",\n",
    "    'docs/vpc',\n",
    "    'docs/storage',\n",
    "    'docs/solutions',\n",
    "    'docs/resource-manager',\n",
    "    'docs/billing',\n",
    "    'docs/speechkit',\n",
    "    'prices',\n",
    "    'prices/compute',\n",
    "    'prices/storage',\n",
    "    'prices/speechkit',\n",
    "    'marketplace',\n",
    "    'services',\n",
    "    'services/compute',\n",
    "    'services/managed-clickhouse',\n",
    "    'services/managed-postgresql',\n",
    "    'services/speechkit',\n",
    "    'services/storage',\n",
    "    'services/vpc',\n",
    "    'billing',\n",
    "    'updateUserSettings',\n",
    "    'support',\n",
    "    'form',\n",
    "    'managed-postgresql',\n",
    "    'managed-clickhouse',\n",
    "    'compute',\n",
    "    'speechkit',\n",
    "    'storage',\n",
    "    'vpc',\n",
    "    'instance',\n",
    "    'blog',\n",
    "    'delete',\n",
    "    'remove',\n",
    "    '/add',\n",
    "    'create',\n",
    "    'update',\n",
    "    'start',\n",
    "    'stop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_patterns(patterns):\n",
    "    new_patterns = []\n",
    "    for x in patterns:\n",
    "        x = re.sub(r\"[.]|\\/\", r'_', x)\n",
    "        x = x.lower()\n",
    "        new_patterns.append(x)\n",
    "    return new_patterns\n",
    "interested_patterns = update_patterns(interested_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(array, pattern):\n",
    "    cnt = 0\n",
    "    for event in array:\n",
    "        if pattern in event:\n",
    "            cnt += 1\n",
    "    return cnt / len(array) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text_paths, model):\n",
    "    res = []\n",
    "    gc.collect()\n",
    "    for ind, session in enumerate(text_paths):\n",
    "        res.append(model.infer_vector(session))\n",
    "        if ind % 10000 == 0:\n",
    "            gc.collect()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_console_logs_scoring_table():\n",
    "    df = get_table()\n",
    "    df['events'] = df['event_array'].apply(\n",
    "        lambda array: [event_changer(x) for x in array])\n",
    "    \n",
    "    for pattern in interested_patterns:\n",
    "        df[pattern] = df['events'].apply(lambda x: find_pattern(x, pattern))\n",
    "        \n",
    "    df.drop(columns = ['event_array'], inplace=True)\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=100, epochs=100, workers=-1)\n",
    "    text_corpus = [TaggedDocument(doc, [i]) for i, doc in enumerate(df['events'])]\n",
    "    model.build_vocab(text_corpus)\n",
    "    model.train(text_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    text_paths = df['events'].to_list()\n",
    "    events_vec_rows = text2vec(text_paths, model)\n",
    "    vec_events_df = pd.DataFrame(events_vec_rows)\n",
    "    vec_events_df.columns = [f'event_vec_path_{i}' for i in range(vec_events_df.shape[1])]\n",
    "    \n",
    "    events = df['events'].apply(lambda x: \" \".join(x))\n",
    "    vectorizer = TfidfVectorizer(token_pattern=u'[^ ]+', min_df=30)\n",
    "    vectorizer.fit(events)\n",
    "    count_vec_transform = vectorizer.fit_transform(events)\n",
    "    tdf_df = pd.DataFrame(count_vec_transform.toarray(), \n",
    "                          columns =vectorizer.get_feature_names())\n",
    "    tdf_df.columns = ['tdf_' + col for col in tdf_df.columns]\n",
    "    \n",
    "    df = pd.concat([df, vec_events_df], axis=1)\n",
    "    df = pd.concat([df, tdf_df], axis=1)\n",
    "    df['events'] = df['events'].astype(str)\n",
    "\n",
    "    for i in range(vec_events_df.shape[1]):\n",
    "        column_types.add(f'event_vec_path_{i}:numeric')\n",
    "    for column in tdf_df.columns:\n",
    "        column_types.add(f'{column}:numeric')\n",
    "\n",
    "    column_types.add(f\"events:json_pct__200\")\n",
    "    for pattern in interested_patterns:\n",
    "        column_types.add(f\"{pattern}:numeric\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_types(column_types):\n",
    "    rows = []\n",
    "    for column_type in column_types:\n",
    "        column, current_type = column_type.split(':')\n",
    "        rows.append([column, current_type, TABLE_NAME])\n",
    "    type_df = pd.DataFrame(np.matrix(rows), columns=['column_name', 'type',\n",
    "                                                     'table_name'])\n",
    "    lib.save_table('type_table', \"//home/cloud_analytics/scoring_v2/data_tables\", \n",
    "                   type_df, append=True)\n",
    "\n",
    "\n",
    "def add_table_to_model_to_observe(table_name):\n",
    "    tables_df = pd.DataFrame([table_name], \n",
    "                             columns=['table_names'])\n",
    "    lib.save_table('table_names_for_scoring_model', \n",
    "               \"//home/cloud_analytics/scoring_v2/data_tables\", \n",
    "               tables_df, append=True)\n",
    "    \n",
    "\n",
    "def check_types_correspondence(df, column_types):\n",
    "    req = \"\"\"\n",
    "    SELECT\n",
    "        type,\n",
    "        checker_function\n",
    "    FROM \"//home/cloud_analytics/scoring_v2/data_tables/column_type_description\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    type_df = lib.execute_query(req)\n",
    "\n",
    "    checker_functions = {}\n",
    "    for func_str in type_df['checker_function']:\n",
    "        exec(func_str.replace(\"\\\\n\", '\\n'), checker_functions)\n",
    "    \n",
    "    assert len(df.columns) == len(column_types) + 2, \\\n",
    "    'difference in number of columns in dataframe and in column_types, '\n",
    "    f'{len(column_types) + 2 - len(df.columns)}'\n",
    "    \n",
    "    for column_type in column_types:\n",
    "        column, curr_type = column_type.split(\":\")\n",
    "        curr_function_name = curr_type.split(\"__\")[0]\n",
    "        if checker_functions.get(curr_function_name + \"_checker\") is None:\n",
    "            assert False, f\"no type {curr_function_name}\"\n",
    "        assert checker_functions[curr_function_name + \"_checker\"](df, column),\\\n",
    "        f'{curr_function_name} check failed for column {column}'\n",
    "        \n",
    "\n",
    "def save_all_results(df):\n",
    "    check_types_correspondence(df, column_types)\n",
    "    lib.save_table(TABLE_NAME, \n",
    "                   \"//home/cloud_analytics/scoring_v2/data_tables\", df)\n",
    "    save_types(column_types)\n",
    "    add_table_to_model_to_observe(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 58s, sys: 5.93 s, total: 28min 4s\n",
      "Wall time: 28min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "console_logs_df = make_console_logs_scoring_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_results(console_logs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
