{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy.stats as sps\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from dateutil.parser import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import typing as tp\n",
    "import my_library as lib\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_TO_OBSERVE = 14\n",
    "EMPTY_TYPE = 'undefined'\n",
    "column_types = set()\n",
    "TABLE_NAME = 'grant_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grants_preprod_maker():\n",
    "    req = f\"\"\"\n",
    "    SELECT\n",
    "        billing_account_id,\n",
    "        end_time,\n",
    "        start_time,\n",
    "        id as grant_id,\n",
    "        initial_amount,\n",
    "        scoring_date,\n",
    "        type\n",
    "    FROM (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            toDate(end_time) as end_time,\n",
    "            toDate(start_time) as start_time,\n",
    "            source as type,\n",
    "            id,\n",
    "            CAST(initial_amount as Double) as initial_amount\n",
    "        FROM \"//home/cloud/billing/exported-billing-tables/monetary_grants_prod\"\n",
    "        WHERE id != ''\n",
    "    ) as a\n",
    "    INNER JOIN (\n",
    "        SELECT\n",
    "            billing_account_id,\n",
    "            addDays(toDate(first_first_trial_consumption_datetime),\n",
    "            {DAYS_TO_OBSERVE}) as scoring_date,\n",
    "            first_first_trial_consumption_datetime\n",
    "        FROM \"//home/cloud_analytics/cubes/acquisition_cube/cube\"\n",
    "        WHERE event == 'ba_created'\n",
    "        AND billing_account_id != ''\n",
    "        AND cloud_id != ''\n",
    "        AND first_first_trial_consumption_datetime != '0000-00-00 00:00:00'\n",
    "        GROUP BY billing_account_id, first_first_trial_consumption_datetime\n",
    "    ) as b\n",
    "    ON a.billing_account_id == b.billing_account_id\n",
    "    WHERE toDate(start_time) < toDate(scoring_date)\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    df = lib.execute_query(req)\n",
    "    lib.save_table(\"grants_prepare_information\", \"//home/cloud_analytics/scoring_v2/data_tables\", df)\n",
    "    time.sleep(20)\n",
    "    return df\n",
    "\n",
    "grants_preprod = grants_preprod_maker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = f\"\"\"\n",
    "SELECT\n",
    "    billing_account_id,\n",
    "    scoring_date,\n",
    "    count(DISTINCT grant_id) as grants_number,\n",
    "    SUM(if(type == 'default', initial_amount, 0)) as default_grant_amount,\n",
    "    SUM(initial_amount) as all_grants_amount,\n",
    "    if (toDate(MAX(if(type == 'default', end_time, '2100-01-01'))) < addDays(toDate(NOW()), -{DAYS_TO_OBSERVE}) \n",
    "        AND toDate(scoring_date) < addDays(toDate(NOW()), -60), \n",
    "        1, 0) as is_training_group\n",
    "FROM \"//home/cloud_analytics/scoring_v2/data_tables/grants_prepare_information\"\n",
    "GROUP BY billing_account_id, scoring_date\n",
    "FORMAT TabSeparatedWithNames\n",
    "\"\"\"\n",
    "grant_df = lib.execute_query(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lib.save_table(TABLE_NAME, \"//home/cloud_analytics/scoring_v2/data_tables\", grant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types.add(\"grants_number:numeric\")\n",
    "column_types.add(\"default_grant_amount:numeric\")\n",
    "column_types.add(\"all_grants_amount:numeric\")\n",
    "column_types.add(\"is_training_group:binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_types(column_types):\n",
    "    rows = []\n",
    "    for column_type in column_types:\n",
    "        column, current_type = column_type.split(':')\n",
    "        rows.append([column, current_type, TABLE_NAME])\n",
    "    type_df = pd.DataFrame(np.matrix(rows), columns=['column_name', 'type',\n",
    "                                                     'table_name'])\n",
    "    lib.save_table('type_table', \"//home/cloud_analytics/scoring_v2/data_tables\", \n",
    "                   type_df, append=True)\n",
    "\n",
    "def add_table_to_model_to_observe():\n",
    "    tables_df = pd.DataFrame([TABLE_NAME], columns=['table_names'])\n",
    "    lib.save_table('table_names_for_scoring_model', \n",
    "               \"//home/cloud_analytics/scoring_v2/data_tables\", \n",
    "               tables_df, append=True)\n",
    "    \n",
    "\n",
    "def check_types_correspondence(df, column_types):\n",
    "    req = \"\"\"\n",
    "    SELECT\n",
    "        type,\n",
    "        checker_function\n",
    "    FROM \"//home/cloud_analytics/scoring_v2/data_tables/column_type_description\"\n",
    "    FORMAT TabSeparatedWithNames\n",
    "    \"\"\"\n",
    "    type_df = lib.execute_query(req)\n",
    "\n",
    "    checker_functions = {}\n",
    "    for func_str in type_df['checker_function']:\n",
    "        exec(func_str.replace(\"\\\\n\", '\\n'), checker_functions)\n",
    "    \n",
    "    assert len(df.columns) == len(column_types) + 2, \\\n",
    "    'difference in number of columns in dataframe and in column_types, '\\\n",
    "    f'{len(column_types) + 2 - len(df.columns)}'\n",
    "    \n",
    "    for column_type in column_types:\n",
    "        column, curr_type = column_type.split(\":\")\n",
    "        curr_function_name = curr_type.split(\"__\")[0]\n",
    "        if checker_functions.get(curr_function_name + \"_checker\") is None:\n",
    "            assert False, f\"no type {curr_function_name}\"\n",
    "        assert checker_functions[curr_function_name + \"_checker\"](df, column),\\\n",
    "        f'{curr_function_name} check failed for column {column}'\n",
    "        \n",
    "\n",
    "def save_all_results(df):\n",
    "    check_types_correspondence(df, column_types)\n",
    "    lib.save_table(TABLE_NAME, \"//home/cloud_analytics/scoring_v2/data_tables\", df)\n",
    "    save_types(column_types)\n",
    "    add_table_to_model_to_observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_results(grant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
