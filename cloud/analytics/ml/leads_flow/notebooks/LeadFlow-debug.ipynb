{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7842fe0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T17:42:51.846383Z",
     "start_time": "2022-04-12T17:42:47.878866Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from os.path import join as path_join\n",
    "from clan_tools.data_adapters.YTAdapter import YTAdapter\n",
    "from clan_tools.data_adapters.YQLAdapter import YQLAdapter\n",
    "import spyt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.session import SparkSession\n",
    "from clan_tools.utils.spark import SPARK_CONF_LARGE\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.column import Column as SparkColumn\n",
    "from pyspark.sql.dataframe import DataFrame as SparkDataFrame\n",
    "import re\n",
    "from os.path import join as path_join\n",
    "import pickle\n",
    "from clan_tools.data_adapters.crm.CRMHistoricalDataAdapter import CRMHistoricalDataAdapter\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "import logging.config\n",
    "from clan_tools.logging.logger import default_log_config\n",
    "\n",
    "logging.config.dictConfig(default_log_config)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 250)\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f8a667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T17:43:02.884129Z",
     "start_time": "2022-04-12T17:42:51.849204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12 20:43:02,750 - spyt.client: [INFO]: SPYT Cluster version: 1.36.0\n",
      "2022-04-12 20:43:02,754 - spyt.client: [INFO]: SPYT library version: 1.36.0\n",
      "2022-04-12 20:43:02,791 - spyt.client: [INFO]: SHS link: http://sas3-9051-node-hahn.sas.yp-c.yandex.net:27001/history/app-20220412204301-0029/jobs/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SPYT</b></p>\n",
       "                \n",
       "                <p><a href=\"sas0-1038-node-hahn.sas.yp-c.yandex.net:27002\">Master Web UI</a></p>\n",
       "                <p><a href=\"http://sas3-9051-node-hahn.sas.yp-c.yandex.net:27001/history/app-20220412204301-0029/jobs/\">Spark history server</a></p>\n",
       "                <dl>\n",
       "                  <dt>Yt Cluster</dt>\n",
       "                    <dd><code>hahn.yt.yandex.net</code></dd>\n",
       "                  <dt>SPYT Cluster version</dt>\n",
       "                    <dd><code>1.36.0</code></dd>\n",
       "                  <dt>SPYT Library version</dt>\n",
       "                    <dd><code>1.36.0</code></dd>\n",
       "                </dl>\n",
       "                \n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-cloud-pavelvasilev.sas.yp-c.yandex.net:27002\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://sas0-1038-node-hahn.sas.yp-c.yandex.net:27001</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark for robot-clan-pii-yt</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<spyt.client.SparkInfo at 0x7fe19e71b9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clan_tools.secrets.Vault import Vault\n",
    "Vault().get_secrets(secret_id='sec-01fm06fw1zsqp08cxtyd247tm5')  # robot-clan-pii-yt\n",
    "yt_adapter = YTAdapter()\n",
    "\n",
    "spark = spyt.connect(spark_conf_args=SPARK_CONF_LARGE)\n",
    "spyt.info(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f40f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T17:43:02.893484Z",
     "start_time": "2022-04-12T17:43:02.887587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-04-12'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_date = datetime.now().strftime('%Y-%m-%d')\n",
    "rep_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a167bc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-12T17:47:13.638277Z",
     "start_time": "2022-04-12T17:43:02.896487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-12 20:46:37,844 - __main__: [INFO]: TOTAL:     82\n",
      "2022-04-12 20:46:37,846 - __main__: [INFO]: ==============================\n",
      "2022-04-12 20:46:37,848 - __main__: [INFO]: \t> Leads:       1491\n",
      "2022-04-12 20:46:37,849 - __main__: [INFO]: \t> Leads mkt:      0\n",
      "2022-04-12 20:46:37,851 - __main__: [INFO]: \t> Age < 7d:     214\n",
      "2022-04-12 20:46:37,852 - __main__: [INFO]: \t> Fraud:        150\n",
      "2022-04-12 20:46:37,854 - __main__: [INFO]: \t> Paid:         439\n",
      "2022-04-12 20:46:37,855 - __main__: [INFO]: \t> Isv/Var:       15\n",
      "2022-04-12 20:46:37,857 - __main__: [INFO]: \t> Acc.owner:     87\n",
      "2022-04-12 20:46:37,858 - __main__: [INFO]: \t> State:        431\n",
      "2022-04-12 20:46:37,860 - __main__: [INFO]: \t> Other:         82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calc_date</th>\n",
       "      <th>billing_account_id</th>\n",
       "      <th>event_date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>step</th>\n",
       "      <th>date_created</th>\n",
       "      <th>segment</th>\n",
       "      <th>person_type</th>\n",
       "      <th>resident_type</th>\n",
       "      <th>state</th>\n",
       "      <th>status</th>\n",
       "      <th>is_var</th>\n",
       "      <th>is_isv</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>dn204st7hcbv5r0qjnfn</td>\n",
       "      <td>2022-01-18</td>\n",
       "      <td>BA created</td>\n",
       "      <td>ba is isv/var</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>Mass</td>\n",
       "      <td>company</td>\n",
       "      <td>resident</td>\n",
       "      <td>active</td>\n",
       "      <td>trial</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>dn204st7hcbv5r0qjnfn</td>\n",
       "      <td>2022-04-13</td>\n",
       "      <td>ba is isv/var</td>\n",
       "      <td>active billing-account</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-11</td>\n",
       "      <td>Mass</td>\n",
       "      <td>company</td>\n",
       "      <td>resident</td>\n",
       "      <td>active</td>\n",
       "      <td>trial</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>dn20nkn77hgeb786f7hs</td>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>BA created</td>\n",
       "      <td>ba is not active</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>Mass</td>\n",
       "      <td>company</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>payment_required</td>\n",
       "      <td>trial</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>dn20p36m1jt1rocnugvf</td>\n",
       "      <td>2022-03-28</td>\n",
       "      <td>BA created</td>\n",
       "      <td>paid billing-account</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>Mass</td>\n",
       "      <td>company</td>\n",
       "      <td>resident</td>\n",
       "      <td>active</td>\n",
       "      <td>paid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-12</td>\n",
       "      <td>dn20p36m1jt1rocnugvf</td>\n",
       "      <td>2022-04-01</td>\n",
       "      <td>paid billing-account</td>\n",
       "      <td>upsell - CSM</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-03-21</td>\n",
       "      <td>Mass</td>\n",
       "      <td>company</td>\n",
       "      <td>resident</td>\n",
       "      <td>active</td>\n",
       "      <td>paid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    calc_date    billing_account_id  event_date                  from  \\\n",
       "0  2022-04-12  dn204st7hcbv5r0qjnfn  2022-01-18            BA created   \n",
       "1  2022-04-12  dn204st7hcbv5r0qjnfn  2022-04-13         ba is isv/var   \n",
       "2  2022-04-12  dn20nkn77hgeb786f7hs  2022-03-22            BA created   \n",
       "3  2022-04-12  dn20p36m1jt1rocnugvf  2022-03-28            BA created   \n",
       "4  2022-04-12  dn20p36m1jt1rocnugvf  2022-04-01  paid billing-account   \n",
       "\n",
       "                        to  step date_created segment person_type  \\\n",
       "0            ba is isv/var     1   2022-01-11    Mass     company   \n",
       "1  active billing-account      2   2022-01-11    Mass     company   \n",
       "2         ba is not active     1   2022-03-15    Mass     company   \n",
       "3     paid billing-account     1   2022-03-21    Mass     company   \n",
       "4             upsell - CSM     2   2022-03-21    Mass     company   \n",
       "\n",
       "  resident_type             state status  is_var  is_isv  is_fraud  \n",
       "0      resident            active  trial       0       1         0  \n",
       "1      resident            active  trial       0       1         0  \n",
       "2   switzerland  payment_required  trial       0       0         0  \n",
       "3      resident            active   paid       0       0         0  \n",
       "4      resident            active   paid       0       0         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_by(colname_val: str, colname_by: str) -> SparkColumn:\n",
    "    return F.expr(f'Max_by(`{colname_val}`, `{colname_by}`)')\n",
    "\n",
    "\n",
    "def min_by(colname_val: str, colname_by: str) -> SparkColumn:\n",
    "    return F.expr(f'Min_by(`{colname_val}`, `{colname_by}`)')\n",
    "\n",
    "\n",
    "class LeadsFlow:\n",
    "    \"\"\" Used for tracking new billing accounts way throug CRM\n",
    "    \"\"\"\n",
    "    path_ba_crm_tags = '//home/cloud-dwh/data/prod/cdm/dm_ba_crm_tags'\n",
    "    path_actual_features = '//home/cloud_analytics/ml/scoring/consumption_predictor_v2/data/actual_features'\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, yt_adapter):\n",
    "        self.spark = spark\n",
    "        self.yt_adapter = yt_adapter\n",
    "        self.today = datetime.now().strftime('%Y-%m-%d')\n",
    "        self.from_date = '2022-01-01'\n",
    "        self.to_print = True\n",
    "    \n",
    "    def _get_base_info(self) -> SparkDataFrame:\n",
    "        spdf_base = (\n",
    "            self.spark.read.yt(self.path_ba_crm_tags)\n",
    "            .groupby('billing_account_id')\n",
    "            .agg(\n",
    "                F.min('date').alias('date_created'),\n",
    "                min_by('segment', 'date').alias('segment_start'),\n",
    "                max_by('segment_current', 'date').alias('segment_end'),\n",
    "                max_by('person_type_current', 'date').alias('raw_person_type'),\n",
    "                max_by('state_current', 'date').alias('state'),\n",
    "                max_by('usage_status_current', 'date').alias('status'),\n",
    "                max_by('is_var_current', 'date').astype('int').alias('is_var'),\n",
    "                max_by('is_isv_current', 'date').astype('int').alias('is_isv'),\n",
    "                max_by('is_suspended_by_antifraud_current', 'date').astype('int').alias('is_fraud')\n",
    "            )\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                'date_created',\n",
    "                'segment_start',\n",
    "                'segment_end',\n",
    "                self._person_type('raw_person_type').alias('person_type'),\n",
    "                self._resident('raw_person_type').alias('resident_type'),\n",
    "                self._state('state').alias('state'),\n",
    "                'status',\n",
    "                'is_var',\n",
    "                'is_isv',\n",
    "                'is_fraud'\n",
    "            )\n",
    "            .filter(col('segment_start')=='Mass')\n",
    "            .filter(col('person_type')=='company')\n",
    "            .filter(col('date_created') >= self.from_date)\n",
    "            .filter(col('date_created') < self.today)\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        return spdf_base\n",
    "    \n",
    "    def _person_type(self, colname: str) -> SparkColumn:\n",
    "        company = F.when(col(colname).rlike('company'), lit('company'))\n",
    "        individual = F.when(col(colname).rlike('individual'), lit('individual'))\n",
    "        internal = F.when(col(colname).rlike('internal'), lit('internal'))\n",
    "        return F.coalesce(company, individual, internal, lit('undefined'))\n",
    "\n",
    "\n",
    "    def _resident(self, colname: str) -> SparkColumn:\n",
    "        undefined = F.when(col(colname).isNull(), lit('undefined'))\n",
    "        fuw = F.split(col(colname), '_')[0]\n",
    "        is_resident = F.when(fuw.isin(['company', 'individual', 'internal']), lit('resident')).otherwise(fuw)\n",
    "        return F.coalesce(undefined, is_resident)\n",
    "\n",
    "\n",
    "    def _state(self, colname: str) -> SparkColumn:\n",
    "        p_req = ['payment_not_confirmed', 'payment_required']\n",
    "        p_del = ['deleted', 'inactive']\n",
    "        res_col = F.when(col(colname).isin(p_req), lit('payment_required')).otherwise(\n",
    "            F.when(col(colname).isin(p_del), lit('inactive')).otherwise(col(colname)))\n",
    "        return res_col\n",
    "    \n",
    "    def _get_reasons(self) -> SparkDataFrame:\n",
    "        spdf_reasons = (\n",
    "            self.spark.read.yt(self.path_actual_features)\n",
    "            .filter(col('billing_record_msk_date') >= self.from_date)\n",
    "            .filter(col('days_from_created') == 7)\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                'billing_record_msk_date',\n",
    "                'billing_record_cost_rub',\n",
    "                'billing_record_credit_rub',\n",
    "                'billing_record_total_rub',\n",
    "                'days_from_created',\n",
    "                'billing_account_usage_status',\n",
    "                'billing_account_person_type',\n",
    "                'billing_account_currency',\n",
    "                'billing_account_state',\n",
    "                'billing_account_is_fraud',\n",
    "                'billing_account_is_suspended_by_antifraud',\n",
    "                'billing_account_is_isv',\n",
    "                'billing_account_is_var',\n",
    "                'billing_account_is_crm_account',\n",
    "                'crm_partner_manager',\n",
    "                'crm_segment',\n",
    "                'prev_7d_cons'\n",
    "            )\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        return spdf_reasons\n",
    "    \n",
    "    def _get_crm_data(self) -> SparkDataFrame:\n",
    "        historical_data_adapter = CRMHistoricalDataAdapter(self.yt_adapter, self.spark)\n",
    "\n",
    "        def make_crm_event(ls_name: str, lsd_name: str):\n",
    "            # initialize columns\n",
    "            ls = F.coalesce(col(ls_name), lit('unknown'))\n",
    "            lsd = F.coalesce(col(lsd_name), lit('unknown'))\n",
    "\n",
    "            ls = F.when(lsd == 'new4upsell', lit('upsell')).otherwise(\n",
    "                    F.when(ls.isin(['upsell', 'trial']), ls).otherwise(lit('other')))\n",
    "\n",
    "            onb_list = ['Client is Company', 'Client is Individual', 'TrialCompanies',\n",
    "                        'TrialCompaniesNonresidents', 'NoTrialCompaniesWithBA',\n",
    "                        'Client is Kazakhstan_individual']\n",
    "            upsell_list = ['advanced onboarding', 'contact more then 70 days', 'upsell', 'new4upsell']\n",
    "            lsd = F.when(ls == 'other', lit('Other')).otherwise(\n",
    "                F.when(ls == 'mkt', lit('Website request')).otherwise(\n",
    "                    F.when((ls == 'trial') & (lsd.isin(onb_list)), lit('Onboarding')).otherwise(\n",
    "                        F.when(ls == 'trial', lit('Other')).otherwise(\n",
    "                            F.when((ls == 'upsell') & lsd.isin(upsell_list), lit('Upsell')).otherwise(\n",
    "                                F.when(F.lower(lsd).rlike('potential candidate'), lit('CSM')).otherwise(\n",
    "                                    F.when(F.lower(lsd).rlike('consumed more'), lit('CSM')).otherwise(\n",
    "                                        lit('Other'))))))))\n",
    "\n",
    "            return F.concat(ls, lit(' - '), lsd)\n",
    "\n",
    "        spdf_crm = (\n",
    "            historical_data_adapter.\n",
    "            historical_preds()\n",
    "            .filter(col('billing_account_id').isNotNull())\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                F.to_date(F.to_timestamp(col('date_entered').astype(T.LongType())/1e6)).alias('event_date'),\n",
    "                col('lead_source_crm').alias('lead_source'),\n",
    "                col('lead_source').alias('lead_source_description'),\n",
    "                make_crm_event('lead_source_crm', 'lead_source').alias('event')\n",
    "            )\n",
    "            .distinct()\n",
    "            .cache()\n",
    "        )\n",
    "        \n",
    "        return spdf_crm\n",
    "    \n",
    "    def get_sankey_start(self) -> SparkDataFrame:\n",
    "        sankey_start = (\n",
    "            self._get_base_info()\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                col('date_created').alias('event_date'),\n",
    "                lit('BA created').alias('event')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return sankey_start\n",
    "    \n",
    "    def get_sankey_on_7_days(self) -> SparkDataFrame:\n",
    "        filtered_crm = (\n",
    "            self._get_crm_data()\n",
    "            .filter(col('event').isin(['trial - Onboarding', 'mkt - Website request']))\n",
    "        )\n",
    "        \n",
    "        df_7_days = (\n",
    "            self._get_base_info()\n",
    "            .join(self._get_reasons(), on='billing_account_id', how='left')\n",
    "            .join(filtered_crm, on='billing_account_id', how='left')\n",
    "            .toPandas()\n",
    "        )\n",
    "\n",
    "        curr_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        df_7_days['curr_age'] = (curr_date - pd.to_datetime(df_7_days['date_created'])).dt.days\n",
    "\n",
    "        # aggregated events\n",
    "        bind_all = df_7_days['billing_account_id'].notna()\n",
    "        events_cols = pd.Series(['']*sum(bind_all), index=df_7_days.index, name='event')\n",
    "        bind_other = bind_all\n",
    "\n",
    "        bind_leads = df_7_days['event'] == 'trial - Onboarding'\n",
    "        events_cols.loc[bind_leads] = 'trial - Onboarding'\n",
    "        bind_other &= ~bind_leads\n",
    "\n",
    "        bind_leads_mkt = df_7_days['event'] == 'mkt - Website request'\n",
    "        events_cols.loc[bind_leads_mkt] = 'mkt - Website request'\n",
    "        bind_other &= ~bind_leads_mkt\n",
    "\n",
    "        bind_less = bind_other & (df_7_days['curr_age'] <= 7)\n",
    "        events_cols.loc[bind_less] = 'lifetime < 7 days'\n",
    "        bind_other &= ~bind_less\n",
    "\n",
    "        bind_frod = bind_other & df_7_days['is_fraud']\n",
    "        events_cols.loc[bind_frod] = 'fraud billing-account'\n",
    "        bind_other &= ~bind_frod\n",
    "\n",
    "        bind_paid = bind_other & ((df_7_days['prev_7d_cons']>0.) | (df_7_days['billing_account_usage_status']=='paid'))\n",
    "        events_cols.loc[bind_paid] = 'paid billing-account'\n",
    "        bind_other &= ~bind_paid\n",
    "\n",
    "        bind_isv_var = bind_other & (df_7_days['billing_account_is_var'] | df_7_days['billing_account_is_isv'])\n",
    "        events_cols.loc[bind_isv_var] = 'ba is isv/var'\n",
    "        bind_other &= ~bind_isv_var\n",
    "\n",
    "        bind_acc_owner = bind_other & df_7_days['billing_account_is_crm_account']\n",
    "        events_cols.loc[bind_acc_owner] = 'ba has account-owner'\n",
    "        bind_other &= ~bind_acc_owner\n",
    "\n",
    "        bind_state = bind_other & (df_7_days['billing_account_state'] != 'active')\n",
    "        events_cols.loc[bind_state] = 'ba is not active'\n",
    "        bind_other &= ~bind_state\n",
    "\n",
    "        events_cols.loc[bind_other] = 'unknown reason'\n",
    "        if self.to_print:\n",
    "            logger.info('TOTAL: %6.d' % sum(bind_all))\n",
    "            logger.info('='*30)\n",
    "            logger.info('\\t> Leads:     %6.d' % sum(bind_leads))\n",
    "            logger.info('\\t> Leads mkt: %6.d' % sum(bind_leads_mkt))\n",
    "            logger.info('\\t> Age < 7d:  %6.d' % sum(bind_less))\n",
    "            logger.info('\\t> Fraud:     %6.d' % sum(bind_frod))\n",
    "            logger.info('\\t> Paid:      %6.d' % sum(bind_paid))\n",
    "            logger.info('\\t> Isv/Var:   %6.d' % sum(bind_isv_var))\n",
    "            logger.info('\\t> Acc.owner: %6.d' % sum(bind_acc_owner))\n",
    "            logger.info('\\t> State:     %6.d' % sum(bind_state))\n",
    "            logger.info('\\t> Other:     %6.d' % sum(bind_other))\n",
    "\n",
    "        tdf_7_days = df_7_days[['billing_account_id']].copy()\n",
    "        tdf_7_days['event_date'] = np.minimum((pd.to_datetime(df_7_days['date_created']) + timedelta(days=7)).dt.strftime('%Y-%m-%d'), self.today)\n",
    "        tdf_7_days['event'] = events_cols\n",
    "        self.to_print = False\n",
    "        sankey_7_days = spark.createDataFrame(tdf_7_days)\n",
    "        \n",
    "        return sankey_7_days\n",
    "    \n",
    "    def get_sankey_after_7_days(self) -> SparkDataFrame:\n",
    "        go_on_events = ['paid billing-account', 'trial - Onboarding', 'ba has account-owner',\n",
    "                         'ba is isv/var', 'mkt - Website request','unknown reason']\n",
    "        events_filter = (\n",
    "            self.get_sankey_on_7_days()\n",
    "            .filter(col('event').isin(go_on_events))\n",
    "            .select('billing_account_id', col('event_date').alias('date_from'))\n",
    "        )\n",
    "\n",
    "        sankey_events = (\n",
    "            self._get_crm_data()\n",
    "            .filter(col('event').isin(['upsell - CSM', 'upsell - Upsell']))\n",
    "            .join(events_filter, on='billing_account_id', how='inner')\n",
    "            .filter(col('event_date') > col('date_from'))\n",
    "            .filter(col('event_date') < self.today)\n",
    "            .groupby('billing_account_id', 'event')\n",
    "            .agg(F.min('event_date').alias('event_date'))\n",
    "            .select('billing_account_id', 'event_date', 'event')\n",
    "        )\n",
    "\n",
    "        win = Window.partitionBy('billing_account_id').orderBy(col('event_date').asc())\n",
    "\n",
    "        sankey_finish = (\n",
    "            self._get_base_info()\n",
    "            .join(events_filter, on='billing_account_id', how='inner')\n",
    "            .filter(col('date_from') < self.today)\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                lit((datetime.now()+timedelta(days=1)).strftime('%Y-%m-%d')).alias('event_date'),\n",
    "                F.coalesce(\n",
    "                    F.when(col('state') != 'active', lit('ba is not active ')),\n",
    "                    F.when(col('is_fraud') == 1, lit('fraud billing-account ')),\n",
    "                    F.when(col('status') == 'service', lit('service biling_account ')),\n",
    "                    F.when(col('segment_end') != 'Mass', lit('other segment ')),\n",
    "                    F.when(col('state') == 'active', lit('active billing-account '))\n",
    "                ).alias('event')\n",
    "            )\n",
    "            .select('billing_account_id', 'event_date', 'event')\n",
    "        )\n",
    "        \n",
    "        sankey_after_7_days = sankey_events.union(sankey_finish)\n",
    "        \n",
    "        return sankey_after_7_days\n",
    "    \n",
    "    def get_sankey_graph(self) -> SparkDataFrame:\n",
    "        win = Window.partitionBy('billing_account_id').orderBy(col('event_date').asc(), col('event'))\n",
    "\n",
    "        sankey_res = (\n",
    "            self.get_sankey_start()\n",
    "            .union(self.get_sankey_on_7_days())\n",
    "            .union(self.get_sankey_after_7_days())\n",
    "            .select(\n",
    "                'billing_account_id',\n",
    "                'event_date',\n",
    "                F.lag('event').over(win).alias('from'),\n",
    "                col('event').alias('to'),\n",
    "                (F.row_number().over(win) - 1).alias('step')\n",
    "            )\n",
    "            .filter(col('from').isNotNull())\n",
    "            .filter(col('from') != col('to'))\n",
    "        )\n",
    "        \n",
    "        spdf_graph_sankey = (\n",
    "            sankey_res\n",
    "            .join(self._get_base_info(), on='billing_account_id', how='left')\n",
    "            .select(\n",
    "                lit(self.today).alias('calc_date'),\n",
    "                'billing_account_id',\n",
    "                'event_date',\n",
    "                'from',\n",
    "                'to',\n",
    "                'step',\n",
    "                'date_created',\n",
    "                col('segment_end').alias('segment'),\n",
    "                'person_type',\n",
    "                'resident_type',\n",
    "                'state',\n",
    "                'status',\n",
    "                'is_var',\n",
    "                'is_isv',\n",
    "                'is_fraud'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return spdf_graph_sankey\n",
    "\n",
    "LF = LeadsFlow(spark, yt_adapter)\n",
    "\n",
    "spdf_graph_sankey = LF.get_sankey_graph()\n",
    "# spdf_graph_sankey.coalesce(1).write.yt('//home/cloud_analytics/ml/leads_flow/graph_sankey', mode='overwrite')\n",
    "\n",
    "tt = spdf_graph_sankey.toPandas()\n",
    "tt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55273636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
