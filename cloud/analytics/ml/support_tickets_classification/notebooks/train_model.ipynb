{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import string\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/jdk-11\"\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "import spyt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import col, lit, broadcast\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "from clan_tools.data_adapters.YTAdapter import YTAdapter \n",
    "import pyspark.sql.dataframe as spd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:00:41.839123: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-28 17:00:41.839173: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T13:58:06.738879Z",
     "start_time": "2021-09-28T13:58:03.165213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spark = spyt.connect(spark_conf_args ={\n",
    "      \"spark.executor.memory\": \"6G\",\n",
    "      \"spark.executor.cores\": 2,\n",
    "      \"spark.sql.session.timeZone\": \"UTC\",\n",
    "      \"spark.dynamicAllocation.maxExecutors\": 6,\n",
    "      \"spark.dynamicAllocation.enabled\":True,\n",
    "      \"spark.sql.autoBroadcastJoinThreshold\":-1,\n",
    "      \"spark.cores.min\":16,\n",
    "      \"spark.driver.memory\": \"4G\",\n",
    "      \"spark.executor.instances\":6,\n",
    "      \"spark.jars\":'yt:///home/sashbel/graphframes-assembly-0.8.2-SNAPSHOT-spark3.0.jar',\n",
    "})\n",
    "# spyt.info(spark)\n",
    "# Enable Arrow-based columnar data \n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:00:47,812 - WARNING - spyt.conf - Your SPYT library has version 1.16.0 which is older than your cluster version 3.0.1-1.13.2+yandex. Some new features may not work as expected. Please update your cluster with spark-launch-yt utility\n",
      "21/09/28 17:00:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/28 17:01:07 WARN Utils: Service 'sparkDriver' could not bind on port 27001. Attempting port 27002.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'sparkDriver' could not bind on port 27002. Attempting port 27003.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'sparkDriver' could not bind on port 27003. Attempting port 27004.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'SparkUI' could not bind on port 27001. Attempting port 27002.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'SparkUI' could not bind on port 27002. Attempting port 27003.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'SparkUI' could not bind on port 27003. Attempting port 27004.\n",
      "21/09/28 17:01:07 WARN Utils: Service 'SparkUI' could not bind on port 27004. Attempting port 27005.\n",
      "21/09/28 17:01:10 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 27001. Attempting port 27002.\n",
      "21/09/28 17:01:10 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 27002. Attempting port 27003.\n",
      "21/09/28 17:01:10 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 27003. Attempting port 27004.\n",
      "21/09/28 17:01:10 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 27004. Attempting port 27005.\n",
      "21/09/28 17:01:10 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 27005. Attempting port 27006.\n",
      "2021-09-28 17:01:11,902 - INFO - spyt.client - SPYT Cluster version: 3.0.1-1.13.2+yandex\n",
      "2021-09-28 17:01:11,904 - INFO - spyt.client - SPYT library version: 1.16.0\n",
      "2021-09-28 17:01:11,979 - INFO - spyt.client - SHS link: http://sas4-1256-node-hahn.sas.yp-c.yandex.net:27001/history/app-20210928170110-0022/jobs/\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T13:58:06.741274Z",
     "start_time": "2021-09-28T13:58:06.741253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "\n",
    "ma = pymorphy2.MorphAnalyzer()\n",
    "maxlen=100\n",
    "\n",
    "\n",
    "def num_digits(s):\n",
    "    return sum(c.isdigit() for c in s)\n",
    "\n",
    "\n",
    "def clean_text(text, words_count=maxlen):\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', ' ', text)\n",
    "    text = re.sub(\n",
    "        '[.,:;_%©?*,!@#$%^&(){{}}]|[+=]|[«»]|[<>]|[\\']|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    text = ' '.join(word for word in text.split() if len(word) > 2)\n",
    "    text = ' '.join(word for word in text.split() if not word.isnumeric())\n",
    "    text = ' '.join(word for word in text.split() if num_digits(word)<=2)\n",
    "    text = \" \".join(ma.parse(word)[0].normal_form for word in text.split())\n",
    "    words = text.split()[:words_count]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "    \n",
    "\n",
    "clean_text_udf = F.udf(clean_text, returnType=T.StringType())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:01:45,406 - INFO - pymorphy2.opencorpora_dict.wrapper - Loading dictionaries from /home/albina-volk/miniconda3/envs/py37/lib/python3.7/site-packages/pymorphy2_dicts_ru/data\n",
      "2021-09-28 17:01:45,469 - INFO - pymorphy2.opencorpora_dict.wrapper - format: 2.4, revision: 417127, updated: 2020-10-11T15:05:51.070345\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T13:58:06.742844Z",
     "start_time": "2021-09-28T13:58:06.742808Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "START_DATE = \"01-09-2019\"\n",
    "END_DATE = \"01-09-2021\"\n",
    "PATH_TO_RESULTING_MODEL_DATA = '../src/support_tickets_classification/data/model'"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T13:58:06.744543Z",
     "start_time": "2021-09-28T13:58:06.744525Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data collecting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "support_issues_path=\"//home/startrek/tables/prod/yandex-team/queue/CLOUDSUPPORT/issues\"\n",
    "tickets_prod_path=\"//home/cloud/billing/exported-support-tables/tickets_prod\"\n",
    "components_path=\"//home/startrek/tables/prod/yandex-team/queue/CLOUDSUPPORT/components\"\n",
    "components_white_list_path = \"//home/cloud_analytics/ml/support_tickets_classification/components_white_list\""
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:51:24.634577Z",
     "start_time": "2021-09-24T07:51:24.592485Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "issues = (\n",
    "    spark.read\n",
    "    .schema_hint({'components': T.ArrayType(T.StringType())})\n",
    "    .yt(support_issues_path)\n",
    "    .select('key', F.explode('components').alias('components'))\n",
    ")\n",
    "\n",
    "tickets_prod = (\n",
    "    spark.read\n",
    "    .yt(tickets_prod_path)\n",
    "    .select('description', 'summary', 'st_key', 'iam_user_id', 'created_at')\n",
    ")\n",
    "\n",
    "components = (\n",
    "    spark.read.yt(components_path)\n",
    "    .select('id',\n",
    "            col('name').alias('component_name'),\n",
    "            col('shortId').alias('component_short_id'))\n",
    ")\n",
    "\n",
    "tickets_flat = (\n",
    "    tickets_prod\n",
    "    .join(issues, on=tickets_prod.st_key == issues.key)\n",
    "    .join(components, on=issues.components == components.id)\n",
    ")\n",
    "\n",
    "tickets_with_components = (\n",
    "    tickets_flat\n",
    "    .groupBy('key', 'created_at')\n",
    "    .agg(\n",
    "        F.first('iam_user_id').alias('iam_user_id'),\n",
    "        F.first('summary').alias('summary'),\n",
    "        F.first('description').alias('description'),\n",
    "        F.collect_set('component_name').alias('component_names')\n",
    "    )\n",
    "    .withColumn('sum_description', F.concat(col('summary'), lit('. '), col('description')))\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:51:30.036441Z",
     "start_time": "2021-09-24T07:51:24.637034Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "cleaned_tickets = (\n",
    "    tickets_with_components\n",
    "    .filter(~F.isnull('created_at'))\n",
    "    .withColumn('clean_text', clean_text_udf(col('sum_description').cast('string')))\n",
    "    .withColumn('clean_summary', clean_text_udf(col('summary').cast('string')))\n",
    "    .withColumn('creation_date', F.from_unixtime(col(\"created_at\").cast(T.LongType())))\n",
    "    .filter(col('creation_date') < datetime.strptime(END_DATE, \"%d-%m-%Y\"))\n",
    "    .filter(col('creation_date') > datetime.strptime(START_DATE, \"%d-%m-%Y\"))\n",
    "    .select('key', 'iam_user_id', 'creation_date', 'summary', 'description', 'clean_text', 'clean_summary', 'component_names')\n",
    "    .orderBy('creation_date', ascending=False)\n",
    "    .cache()\n",
    "#     .limit(40000)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:51:32.281848Z",
     "start_time": "2021-09-24T07:51:30.039068Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data = cleaned_tickets.toPandas()\n",
    "print(len(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/28 17:03:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "63380\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:37.423048Z",
     "start_time": "2021-09-24T07:51:32.284636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data.clean_text = data.clean_text.apply(str)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:37.453730Z",
     "start_time": "2021-09-24T07:55:37.430697Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
    "tokenizer.fit_on_texts(data['clean_text'].values)\n",
    "sequences = tokenizer.texts_to_sequences(data['clean_text'].values)\n",
    "X = pad_sequences(sequences, maxlen=maxlen)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:45.177321Z",
     "start_time": "2021-09-24T07:55:37.456789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open(PATH_TO_RESULTING_MODEL_DATA + '/tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:45.284929Z",
     "start_time": "2021-09-24T07:55:45.180620Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "components_white_list = spark.read.yt(components_white_list_path).toPandas()[['component_names']]\n",
    "components_white_list.to_csv(PATH_TO_RESULTING_MODEL_DATA + '/components_white_list.csv')\n",
    "components_white_list = list(components_white_list['component_names'])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:45.295500Z",
     "start_time": "2021-09-24T07:55:45.287235Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "components = dict(zip(components_white_list, range(len(components_white_list))))\n",
    "cleaned_components = []\n",
    "for i in range(len(data)):\n",
    "    cleaned_components.append(list(set(data.component_names.iloc[i]) & set(components_white_list)))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:47.832652Z",
     "start_time": "2021-09-24T07:55:45.297442Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def encode_components(component_list):\n",
    "    res = np.array([components[x] if x in components else np.nan for x in component_list])\n",
    "    res = res[~np.isnan(res)]\n",
    "    return res"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:47.839989Z",
     "start_time": "2021-09-24T07:55:47.835025Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "data['labels'] = list(map(encode_components, cleaned_components))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:48.457084Z",
     "start_time": "2021-09-24T07:55:47.841980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "temp = list(data['labels'])\n",
    "temp.append(list(components.values()))\n",
    "y = mlb.fit_transform(temp)[:-1, :]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:48.669944Z",
     "start_time": "2021-09-24T07:55:48.459589Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPool1D, Dense, LSTM\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:48.677410Z",
     "start_time": "2021-09-24T07:55:48.672639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 64, input_length=maxlen))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:08:53.685186: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-28 17:08:53.687528: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-28 17:08:53.687551: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-28 17:08:53.687588: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-cloud-albina-volk.sas.yp-c.yandex.net): /proc/driver/nvidia/version does not exist\n",
      "2021-09-28 17:08:53.690935: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:55:48.794280Z",
     "start_time": "2021-09-24T07:55:48.679237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "history = model.fit(\n",
    "    X, y,\n",
    "    batch_size=128,\n",
    "    epochs=10\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:08:55.809820: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 25352000 exceeds 10% of free system memory.\n",
      "2021-09-28 17:08:55.835000: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 43605440 exceeds 10% of free system memory.\n",
      "2021-09-28 17:08:55.947639: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-28 17:08:55.955965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2599995000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "496/496 [==============================] - 6s 10ms/step - loss: 0.1006 - auc: 0.8307\n",
      "Epoch 2/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0318 - auc: 0.9632\n",
      "Epoch 3/10\n",
      "496/496 [==============================] - 5s 11ms/step - loss: 0.0283 - auc: 0.9710\n",
      "Epoch 4/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0267 - auc: 0.9751\n",
      "Epoch 5/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0260 - auc: 0.9767\n",
      "Epoch 6/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0248 - auc: 0.9793\n",
      "Epoch 7/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0244 - auc: 0.9800\n",
      "Epoch 8/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0238 - auc: 0.9810\n",
      "Epoch 9/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0233 - auc: 0.9820\n",
      "Epoch 10/10\n",
      "496/496 [==============================] - 5s 10ms/step - loss: 0.0229 - auc: 0.9827\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:56:39.008631Z",
     "start_time": "2021-09-24T07:55:48.796427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model.save(PATH_TO_RESULTING_MODEL_DATA + '/model')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:09:48.990197: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ../src/support_tickets_classification/data/model/model/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-28 17:09:49,596 - INFO - tensorflow - Assets written to: ../src/support_tickets_classification/data/model/model/assets\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T07:56:39.720974Z",
     "start_time": "2021-09-24T07:56:39.010819Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('py37': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "22412354d7a2904903aa118c00315b920c5f1fd46c7247d3cb41b3207328b676"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}