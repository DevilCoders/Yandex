#!/usr/bin/python3

import typing

import nirvana.job_context as nv
import ujson as json

from cloud.ai.lib.python.datasource.yt.ops import Table
from cloud.ai.lib.python.datetime import now
from cloud.ai.speechkit.stt.lib.data.model.dao import (
    MetricEvalRecordData,
    MetricEvalRecord,
    MetricEvalTagData,
    MetricEvalTag,
    RecognitionEndpoint,
    RecognitionPlainTranscript,
    ClusterReferencesInfo,
    TextComparisonStopWordsArcadiaSource,
)
from cloud.ai.speechkit.stt.lib.data.ops.yt import (
    table_metrics_eval_records_meta,
    table_metrics_eval_tags_meta,
)
from cloud.ai.speechkit.stt.lib.text.slice.generation import table_slices_meta, SliceDescriptor


def main():
    op_ctx = nv.context()

    inputs = op_ctx.inputs
    params = op_ctx.parameters

    enabled: bool = params.get('enabled')
    if not enabled:
        print('Storing is disabled')
        return

    with open(inputs.get('records_metrics.json')) as f:
        records_metrics_data_list = [MetricEvalRecordData.from_yson(m) for m in json.load(f)]
    with open(inputs.get('tags_metrics.json')) as f:
        tags_metrics_data_list = [MetricEvalTagData.from_yson(m) for m in json.load(f)]
    with open(inputs.get('params_of_recognition.json')) as f:
        recognition_params = json.load(f)
    with open(inputs.get('cluster_references_info.json')) as f:
        cluster_references_info = ClusterReferencesInfo.from_yson(json.load(f))
    with open(inputs.get('text_comparison_stop_words_info.json')) as f:
        text_comparison_stop_words_info = TextComparisonStopWordsArcadiaSource.from_yson(json.load(f))

    recognition_endpoint = RecognitionEndpoint.from_yson(recognition_params)
    model = recognition_endpoint.get_model()

    received_at = now()
    records_metrics = []
    tags_metrics = []

    def get_calc_data(metric_name: str) -> typing.Optional[dict]:
        if metric_name == 'WER':
            return {
                'cr': cluster_references_info.to_yson(),
                'stop_words': text_comparison_stop_words_info.to_yson(),
            }
        return None

    table = Table(meta=table_slices_meta, name='table')
    stored_slices_names = {SliceDescriptor.from_yson(row).name for row in table.read_rows()}

    def check_slice(slice_name: typing.Optional[str]):
        # At this point we don't know, is this slice generated by custom predicate or by predicate
        # of slice from YT. We hope that one will not store metrics with slices from custom predicates.
        if slice_name is not None:
            assert slice_name in stored_slices_names, f'slice {slice_name} is not registered in YT'

    for metric in records_metrics_data_list:
        for s in metric.slices:
            check_slice(s)
        records_metrics.append(
            MetricEvalRecord(
                record_id=metric.record_id,
                channel=metric.channel,
                slices=metric.slices,
                metric_name=metric.metric_name,
                text_transformations=metric.text_transformations,
                api=recognition_endpoint.api,
                model=model,
                method=recognition_endpoint.method.name,
                eval_id=recognition_params['eval_id'],
                metric_value=metric.metric_value,
                metric_data=metric.metric_data,
                calc_data=get_calc_data(metric.metric_name),
                hypothesis=RecognitionPlainTranscript(text=metric.hypothesis),
                reference=RecognitionPlainTranscript(text=metric.reference),
                recognition_endpoint=recognition_endpoint,
                received_at=received_at,
                other=None,
            )
        )
    for metric in tags_metrics_data_list:
        check_slice(metric.slice)
        tags_metrics.append(
            MetricEvalTag(
                tag=metric.tag,
                slice=metric.slice or 'none',
                metric_name=metric.metric_name,
                aggregation=metric.aggregation,
                text_transformations=metric.text_transformations,
                api=recognition_endpoint.api,
                model=model,
                method=recognition_endpoint.method.name,
                eval_id=recognition_params['eval_id'],
                metric_value=metric.metric_value,
                calc_data=get_calc_data(metric.metric_name),
                recognition_endpoint=recognition_endpoint,
                received_at=received_at,
                other=None,
            )
        )

    table_metrics_eval_records = Table(meta=table_metrics_eval_records_meta, name=Table.get_name(received_at))
    table_metrics_eval_records.append_objects(records_metrics)

    table_metrics_eval_tags = Table(meta=table_metrics_eval_tags_meta, name=Table.get_name(received_at))
    table_metrics_eval_tags.append_objects(tags_metrics)
